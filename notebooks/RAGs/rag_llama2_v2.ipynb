{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you will find:\n",
    "\n",
    "- chunk process of the data\n",
    "- RAG with elasticsearch and llama2\n",
    "- Retrieval evaluation\n",
    "- RAG Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/data.csv')\n",
    "documents = df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chapter': 'CHAPTER 1',\n",
       " 'title': 'Machine Learning Roles and the Interview Process',\n",
       " 'section': 'Overview of This Book',\n",
       " 'text': 'In the first part of this chapter, I’ll walk through the structure of this book. Then, I’ll discuss the various job titles and roles that use ML skills in industry. 1 I’ll also clarify the responsibilities of various job titles, such as data scientist, machine learning engineer, and so on, as this is a common point of confusion for job seekers. These will be illustrated with an ML skills matrix and ML lifecycle that will be referenced throughout the book. The second part of this chapter walks through the interview process, from beginning to end. I’ve mentored candidates who appreciated this overview since online resources often focus on specific pieces of the interview but not how they all connect together and result in an offer. Especially for new graduates 2 and readers coming from different industries, this chapter helps get everyone on the same page as well as clarifies the process. The interconnecting pieces of interviews are complex, with many types of combina‐ tions depending on the ML role you’re aiming for. This overview will help set the stage, so you’ll know what to focus your time on. For example, some online resources focus on knowledge specific to “product data scientists,” but will title the course or article “data scientist interview tips” without differentiating. For a newcomer, it’s hard to tell if that is relevant to your own career interests. After this chapter, you’ll be able to tell what skills are required for each job title, and in Chapter 2 , you’ll be able to parse out that information yourself from job postings and make your resume as relevant to the job title and job posting as possible. This chapter focuses on helping you differentiate among various ML roles, and walks through the entire interview process, as illustrated in Figure 1-1 : • Job applications and resume ( Chapter 2 ) • Technical interviews — Machine learning (Chapters 3 , 4 , and 6 ) — Coding/programming ( Chapter 5 ) • Behavioral interviews ( Chapter 7 ) • Your interview roadmap ( Chapter 8 ) • Post-interview and follow-up ( Chapter 9 ) Figure 1-1. Overview of the chapters and how they tie into the ML interview process. Depending on where you are in your ML interview journey, I encourage you to focus on the chapters and sections that seem relevant to you. I’ve also planned the book to be referenced as you go along; for example, you might iterate on your resume multi‐ ple times and then flip back to Chapter 2 when needed. The same applies to the other chapters. With that overview, let’s continue. The companion site to this book, https://susanshu.substack.com , features bonus content, helper resources, and more.',\n",
       " 'id': '86fd49a66d'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the number of tokens \n",
    "\n",
    "llama2 from Ollama has an embedding length of 4096. This means that the maximum number of tokens that can be introduced will be 4096 tokens. Let's check if the pre-chunking using the natural structure of the book is enough or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'chapter': 'CHAPTER 3', 'title': 'Technical Interview: Machine Learning Algorithms', 'section': 'Statistical and Foundational Techniques', 'num_tokens': 4151}, {'chapter': 'CHAPTER 6', 'title': 'Technical Interview: Model Deployment and End-to-End ML', 'section': 'Model Deployment', 'num_tokens': 6350}]\n"
     ]
    }
   ],
   "source": [
    "token_limit = 4096\n",
    "results = []\n",
    "big_docs = []\n",
    "\n",
    "for i, doc in enumerate(documents):\n",
    "    spacy_doc = nlp(doc['text'])\n",
    "    num_tokens = len(spacy_doc)\n",
    "\n",
    "    result = {\n",
    "        \"chapter\": doc[\"chapter\"],\n",
    "        \"title\": doc[\"title\"],\n",
    "        \"section\": doc[\"section\"],\n",
    "        \"num_tokens\": num_tokens\n",
    "    }\n",
    "    \n",
    "    results.append(result)\n",
    "    \n",
    "for res in results:\n",
    "    if res[\"num_tokens\"] > token_limit: \n",
    "        big_docs.append(res)\n",
    "        \n",
    "print(big_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking the problematic parts of the book\n",
    "\n",
    "I'm going to use the structure of the book to chunk both chapters into two parts. \n",
    "\n",
    "Since the initial parsing didn't consider subsections, I will chunk them into 1 chunk of 4000 + the following words until the end of the sentence and another chunk with the second part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_limit = 4000\n",
    "updated_documents = []\n",
    "\n",
    "for doc in documents:\n",
    "    spacy_doc = nlp(doc['text'])\n",
    "    num_tokens = len(spacy_doc)\n",
    "\n",
    "    if (doc['chapter'] in [\"CHAPTER 3\", \"CHAPTER 6\"]) and (doc['section'] in [\"Statistical and Foundational Techniques\", \"Model Deployment\"]):\n",
    "        text = doc['text']\n",
    "        tokens = nlp(text)  \n",
    "        current_chunk = []\n",
    "        current_tokens = 0\n",
    "        \n",
    "        for sentence in tokens.sents:\n",
    "            sentence_tokens = len(sentence)\n",
    "            \n",
    "            if current_tokens + sentence_tokens > token_limit:\n",
    "                updated_documents.append({\n",
    "                    \"chapter\": doc[\"chapter\"],\n",
    "                    \"title\": doc[\"title\"],\n",
    "                    \"section\": doc[\"section\"],\n",
    "                    \"text\": \" \".join([token.text for token in current_chunk]), \n",
    "                    \"num_tokens\": current_tokens,\n",
    "                    \"id\": f\"{doc['id']}_chunk_{len(updated_documents) + 1}\"  \n",
    "                })\n",
    "                current_chunk = [sentence] \n",
    "                current_tokens = sentence_tokens  \n",
    "            else:\n",
    "                current_chunk.append(sentence)\n",
    "                current_tokens += sentence_tokens\n",
    "\n",
    "        if current_chunk:\n",
    "            updated_documents.append({\n",
    "                \"chapter\": doc[\"chapter\"],\n",
    "                \"title\": doc[\"title\"],\n",
    "                \"section\": doc[\"section\"],\n",
    "                \"text\": \" \".join([token.text for token in current_chunk]), \n",
    "                \"num_tokens\": current_tokens,\n",
    "                \"id\": f\"{doc['id']}_chunk_{len(updated_documents) + 1}\"  \n",
    "            })\n",
    "    else:\n",
    "        updated_documents.append(doc)\n",
    "\n",
    "# for updated_doc in updated_documents:\n",
    "#     print(updated_doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Elasticsearch connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO -  docker config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run on the console (linux)\n",
    "\n",
    "sudo docker run -it \\\n",
    "    --rm \\\n",
    "    --name elasticsearch \\\n",
    "    -m 4GB \\\n",
    "    -p 9200:9200 \\\n",
    "    -p 9300:9300 \\\n",
    "    -e \"discovery.type=single-node\" \\\n",
    "    -e \"xpack.security.enabled=false\" \\\n",
    "    docker.elastic.co/elasticsearch/elasticsearch:8.4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create mappings and Index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'name': '10e9358c75eb', 'cluster_name': 'docker-cluster', 'cluster_uuid': '9cbyo7m3TIqww90LbpPAcQ', 'version': {'number': '8.4.3', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': '42f05b9372a9a4a470db3b52817899b99a76ee73', 'build_date': '2022-10-04T07:17:24.662462378Z', 'build_snapshot': False, 'lucene_version': '9.3.0', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es_client = Elasticsearch('http://localhost:9200') \n",
    "\n",
    "es_client.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"standard_analyzer\": {\n",
    "                \"type\": \"standard\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "    \"properties\": {        \n",
    "        \"chapter\": {\n",
    "            \"type\": \"text\",\n",
    "        },\n",
    "        \"title\": {\n",
    "            \"type\": \"text\",\n",
    "        },\n",
    "        \"section\": {\n",
    "            \"type\": \"text\",\n",
    "        },\n",
    "        \"text\": {\n",
    "            \"type\": \"text\",\n",
    "            \"analyzer\": \"standard_analyzer\"  \n",
    "        },\n",
    "        \"id\":{\n",
    "            \"type\": \"keyword\",\n",
    "        },\n",
    "        \n",
    "    }\n",
    "}\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'ds-interview-questions'})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_name = \"ds-interview-questions\"\n",
    "\n",
    "# it is better to delete the index every time when experimenting\n",
    "es_client.indices.delete(index=index_name, ignore_unavailable=True) \n",
    "es_client.indices.create(index=index_name, body=index_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add documents to the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 191.60it/s]\n"
     ]
    }
   ],
   "source": [
    "for doc in tqdm(updated_documents):\n",
    "    try:\n",
    "        es_client.index(index=index_name, document=doc)\n",
    "    except Exception as e:\n",
    "        print(f\"Error when indexing the document: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'what is the scope of a data scientist?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create search function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_search(query, index=index_name):\n",
    "    \"\"\"\n",
    "    Execute a search query on the specified index.\n",
    "\n",
    "    Parameters:\n",
    "        query (dict): The search query to execute.\n",
    "        index (str): The name of the index to search.\n",
    "\n",
    "    Returns:\n",
    "        None: Prints the search results.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = es_client.search(index=index, body=query)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error during search: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_text_search(query):\n",
    "    full_text_query = {\n",
    "        \"size\": 15,\n",
    "        \"query\": {\n",
    "            \"multi_match\": {\n",
    "                \"query\": query,\n",
    "                \"fields\": [\"text^3\", \"section\", \"title\"],\n",
    "                \"type\": \"best_fields\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    full_text_results = execute_search(full_text_query)\n",
    "\n",
    "    return full_text_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "client = ollama.Client()\n",
    "\n",
    "# to initiate ollama on console for the first time\n",
    "# ollama serve\n",
    "# ollama pull llama2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "<<SYS>>\n",
    "You are an assistant preparing a candidate for a data science job interview. \n",
    "Based on the provided context, please provide a concise and accurate answer to the following question in plain text format without any additional formatting. \n",
    "<</SYS>>\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "[INST] The answer has to be plain text\n",
    " [/INST]\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what is the scope of a data scientist?'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, full_text_results):\n",
    "    context = \"\"\n",
    "\n",
    "    for hit in full_text_results['hits']['hits']: \n",
    "        text = hit['_source']['text']  \n",
    "        context += f\"Text: {text}\\n\\n\"\n",
    "\n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# prompt = build_prompt(query, search_results)\n",
    "# print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(query, full_text_results):\n",
    "    message_content = build_prompt(query, full_text_results)\n",
    "    \n",
    "    response = client.chat(model=\"llama2\", messages=[{\"role\": \"user\", \"content\": message_content}])\n",
    "    \n",
    "    if 'message' in response and 'content' in response['message']:\n",
    "        content = response['message']['content']\n",
    "        \n",
    "        return content.strip()  \n",
    "    return \"\"  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = generate_answer(query, search_results)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query):\n",
    "    full_text_results = full_text_search(query)\n",
    "    response = generate_answer(query, full_text_results)\n",
    "    return response\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's the answer in plain text format:\n",
      "\n",
      "Data scientists require a range of skills, including:\n",
      "\n",
      "1. Programming skills (e.g., Python, R, SQL)\n",
      "2. Statistical knowledge (e.g., hypothesis testing, regression analysis)\n",
      "3. Data visualization and communication\n",
      "4. Machine learning and deep learning techniques\n",
      "5. Data wrangling and preprocessing\n",
      "6. Understanding of data mining and big data technologies\n",
      "7. Familiarity with cloud computing platforms (e.g., AWS, Azure)\n",
      "8. Experience with data management and storage solutions (e.g., relational databases, NoSQL)\n",
      "9. Ability to work with large datasets and perform complex analysis\n",
      "10. Strong analytical and problem-solving skills.\n"
     ]
    }
   ],
   "source": [
    "query = 'Which skills are important for a data scientist?'\n",
    "print(rag(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>text_id</th>\n",
       "      <th>chapter</th>\n",
       "      <th>title</th>\n",
       "      <th>section</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How do you approach data preprocessing for mac...</td>\n",
       "      <td>86fd49a66d</td>\n",
       "      <td>CHAPTER 1</td>\n",
       "      <td>Machine Learning Roles and the Interview Process</td>\n",
       "      <td>Overview of This Book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can you explain the difference between supervi...</td>\n",
       "      <td>86fd49a66d</td>\n",
       "      <td>CHAPTER 1</td>\n",
       "      <td>Machine Learning Roles and the Interview Process</td>\n",
       "      <td>Overview of This Book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How do you evaluate the performance of a machi...</td>\n",
       "      <td>86fd49a66d</td>\n",
       "      <td>CHAPTER 1</td>\n",
       "      <td>Machine Learning Roles and the Interview Process</td>\n",
       "      <td>Overview of This Book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are some common pitfalls to avoid when wo...</td>\n",
       "      <td>86fd49a66d</td>\n",
       "      <td>CHAPTER 1</td>\n",
       "      <td>Machine Learning Roles and the Interview Process</td>\n",
       "      <td>Overview of This Book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How do you handle missing values in a dataset ...</td>\n",
       "      <td>86fd49a66d</td>\n",
       "      <td>CHAPTER 1</td>\n",
       "      <td>Machine Learning Roles and the Interview Process</td>\n",
       "      <td>Overview of This Book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>In what ways do you tailor your resume for dif...</td>\n",
       "      <td>1026686599</td>\n",
       "      <td>CHAPTER 9</td>\n",
       "      <td>Post-Interview and Follow-up</td>\n",
       "      <td>What to Do Between Interviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>Can you share an instance when you received an...</td>\n",
       "      <td>1026686599</td>\n",
       "      <td>CHAPTER 9</td>\n",
       "      <td>Post-Interview and Follow-up</td>\n",
       "      <td>What to Do Between Interviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>How do you handle rejection in the job search ...</td>\n",
       "      <td>1026686599</td>\n",
       "      <td>CHAPTER 9</td>\n",
       "      <td>Post-Interview and Follow-up</td>\n",
       "      <td>What to Do Between Interviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>How long should you wait before following up w...</td>\n",
       "      <td>22eb7b9b30</td>\n",
       "      <td>CHAPTER 9</td>\n",
       "      <td>Post-Interview and Follow-up</td>\n",
       "      <td>Post-Interview Steps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>What advice would you give to someone starting...</td>\n",
       "      <td>2ca59d8bf2</td>\n",
       "      <td>CHAPTER 9</td>\n",
       "      <td>Post-Interview and Follow-up</td>\n",
       "      <td>First 30/60/90 Days of Your New ML Job</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question     text_id    chapter  \\\n",
       "0    How do you approach data preprocessing for mac...  86fd49a66d  CHAPTER 1   \n",
       "1    Can you explain the difference between supervi...  86fd49a66d  CHAPTER 1   \n",
       "2    How do you evaluate the performance of a machi...  86fd49a66d  CHAPTER 1   \n",
       "3    What are some common pitfalls to avoid when wo...  86fd49a66d  CHAPTER 1   \n",
       "4    How do you handle missing values in a dataset ...  86fd49a66d  CHAPTER 1   \n",
       "..                                                 ...         ...        ...   \n",
       "235  In what ways do you tailor your resume for dif...  1026686599  CHAPTER 9   \n",
       "236  Can you share an instance when you received an...  1026686599  CHAPTER 9   \n",
       "237  How do you handle rejection in the job search ...  1026686599  CHAPTER 9   \n",
       "238  How long should you wait before following up w...  22eb7b9b30  CHAPTER 9   \n",
       "239  What advice would you give to someone starting...  2ca59d8bf2  CHAPTER 9   \n",
       "\n",
       "                                                title  \\\n",
       "0    Machine Learning Roles and the Interview Process   \n",
       "1    Machine Learning Roles and the Interview Process   \n",
       "2    Machine Learning Roles and the Interview Process   \n",
       "3    Machine Learning Roles and the Interview Process   \n",
       "4    Machine Learning Roles and the Interview Process   \n",
       "..                                                ...   \n",
       "235                      Post-Interview and Follow-up   \n",
       "236                      Post-Interview and Follow-up   \n",
       "237                      Post-Interview and Follow-up   \n",
       "238                      Post-Interview and Follow-up   \n",
       "239                      Post-Interview and Follow-up   \n",
       "\n",
       "                                    section  \n",
       "0                     Overview of This Book  \n",
       "1                     Overview of This Book  \n",
       "2                     Overview of This Book  \n",
       "3                     Overview of This Book  \n",
       "4                     Overview of This Book  \n",
       "..                                      ...  \n",
       "235           What to Do Between Interviews  \n",
       "236           What to Do Between Interviews  \n",
       "237           What to Do Between Interviews  \n",
       "238                    Post-Interview Steps  \n",
       "239  First 30/60/90 Days of Your New ML Job  \n",
       "\n",
       "[240 rows x 5 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../../data/ground_truth_data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions = df[['question', 'text_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'How do you approach data preprocessing for machine learning models?',\n",
       " 'text_id': '86fd49a66d'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth = df_questions.to_dict(orient = 'records')\n",
    "ground_truth[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_rate(relevance_total):\n",
    "    cnt = 0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        if isinstance(line, (list, tuple)) and True in line:\n",
    "            cnt += 1\n",
    "\n",
    "    return cnt / len(relevance_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mrr(relevance_total):\n",
    "    total_score = 0.0\n",
    "    num_queries = len(relevance_total)\n",
    "\n",
    "    for line in relevance_total:\n",
    "        query_score = 0.0\n",
    "        for rank in range(len(line)):\n",
    "            if line[rank] == True:\n",
    "                query_score = 1 / (rank + 1)\n",
    "                break  \n",
    "\n",
    "        total_score += query_score\n",
    "\n",
    "    return total_score / num_queries if num_queries > 0 else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240/240 [00:01<00:00, 163.43it/s]\n"
     ]
    }
   ],
   "source": [
    "relevance_total = []\n",
    "\n",
    "for q in tqdm(ground_truth):\n",
    "    doc_id = q['text_id']\n",
    "    text_results = full_text_search(q['question'])\n",
    "    hits = text_results.get('hits', {}).get('hits', [])\n",
    "    relevance = [doc['_source']['id'] == doc_id for doc in hits]\n",
    "    relevance_total.append(relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.775, 0.5180606893106892)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hit_rate(relevance_total), mrr(relevance_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparams Optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "import numpy as np\n",
    "from sklearn.model_selection import ParameterGrid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = df_questions[:100]\n",
    "df_test = df_questions[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'temperature': hp.uniform('temperature', 0.1, 1.0),\n",
    "    'top_p': hp.uniform('top_p', 0.5, 1.0),\n",
    "    'top_k': hp.quniform('top_k', 10, 100, 1),\n",
    "    'frequency_penalty': hp.uniform('frequency_penalty', 0.0, 1.0),\n",
    "    'presence_penalty': hp.uniform('presence_penalty', 0.0, 1.0),\n",
    "    'max_tokens': hp.quniform('max_tokens', 50, 200, 1),\n",
    "    'repetition_penalty': hp.uniform('repetition_penalty', 1.0, 1.5)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'space' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 24\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: loss, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m: STATUS_OK}\n\u001b[1;32m     20\u001b[0m trials \u001b[38;5;241m=\u001b[39m Trials()\n\u001b[1;32m     22\u001b[0m best \u001b[38;5;241m=\u001b[39m fmin(\n\u001b[1;32m     23\u001b[0m     fn\u001b[38;5;241m=\u001b[39mobjective,\n\u001b[0;32m---> 24\u001b[0m     space\u001b[38;5;241m=\u001b[39m\u001b[43mspace\u001b[49m,\n\u001b[1;32m     25\u001b[0m     algo\u001b[38;5;241m=\u001b[39mtpe\u001b[38;5;241m.\u001b[39msuggest,\n\u001b[1;32m     26\u001b[0m     max_evals\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,  \n\u001b[1;32m     27\u001b[0m     trials\u001b[38;5;241m=\u001b[39mtrials\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest hyperparameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'space' is not defined"
     ]
    }
   ],
   "source": [
    "def objective(params):\n",
    "    # Extrae los parámetros del diccionario `params`\n",
    "    temperature = params['temperature']\n",
    "    top_p = params['top_p']\n",
    "    top_k = params['top_k']\n",
    "    frequency_penalty = params['frequency_penalty']\n",
    "    presence_penalty = params['presence_penalty']\n",
    "    max_tokens = params['max_tokens']\n",
    "    repetition_penalty = params['repetition_penalty']\n",
    "    \n",
    "    response = rag(query)\n",
    "\n",
    "    hit_rate_value = hit_rate(response)  \n",
    "    mrr_value = mrr(response)\n",
    "    \n",
    "    loss = - (hit_rate_value + mrr_value)\n",
    "    \n",
    "    return {'loss': loss, 'status': STATUS_OK}\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "best = fmin(\n",
    "    fn=objective,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=10,  \n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "print(f\"Best hyperparameters: {best}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparams, best_score = hyperparameter_optimization(df_questions)\n",
    "\n",
    "# Output the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", best_hyperparams)\n",
    "print(\"Best Score:\", best_score)\n",
    "\n",
    "\n",
    "for _, row in df_val.iterrows():\n",
    "    query = row['question']\n",
    "    val_results = full_text_search(query, best_hyperparams)\n",
    "    print(\"Validation Results:\", val_results)\n",
    "\n",
    "# Example of applying the best parameters on test set\n",
    "for _, row in df_test.iterrows():\n",
    "    query = row['question']\n",
    "    test_results = full_text_search(query, best_hyperparams)\n",
    "    print(\"Test Results:\", test_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = []\n",
    "for _, row in df_test.iterrows():\n",
    "    query = row['question']  # Assuming your DataFrame has a 'question' column\n",
    "    relevant_ids = row['relevant_ids']  # Replace with your actual relevant ID retrieval logic\n",
    "    results = full_text_search(query, best_params[\"boost_title\"], best_params[\"boost_content\"], best_params[\"min_score\"])\n",
    "    score = evaluate_results(results, relevant_ids)\n",
    "    test_scores.append(score)\n",
    "\n",
    "print(\"Test set scores:\", test_scores)\n",
    "print(\"Average test score:\", np.mean(test_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../../data/best_hyperparameters_elasticsearch.json\", \"w\") as json_file:\n",
    "#     json.dump(best_params, json_file)\n",
    "\n",
    "# print(\"Best hyperparameters saved to 'best_hyperparameters.json':\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_val = df_val.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minsearch_search_optimized(query, boost):\n",
    "    # boost = {'text': 3.0, 'section': 0.5}\n",
    "    \n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict = {},\n",
    "        boost_dict=boost,\n",
    "        num_results=5)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost = {'text': best['boost']}\n",
    "         \n",
    "evaluate(gt_val, lambda q: minsearch_search_optimized(q['question'], boost))\n",
    "# para mirar cuanto da con los mejores hyperparam "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little bit better :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1_template = \"\"\"\n",
    "You are an expert evaluator for a RAG system.\n",
    "Your task is to analyze the relevance of the generated answer compared to the original answer provided.\n",
    "Based on the relevance and similarity of the generated answer to the original answer, you will classify\n",
    "it as \"NON_RELEVANT\", \"PARTLY_RELEVANT\", or \"RELEVANT\".\n",
    "\n",
    "Here is the data for evaluation:\n",
    "\n",
    "Original Answer: {answer_orig}\n",
    "Generated Question: {question}\n",
    "Generated Answer: {answer_llm}\n",
    "\n",
    "Please analyze the content and context of the generated answer in relation to the original\n",
    "answer and provide your evaluation in parsable JSON without using code blocks:\n",
    "\n",
    "{{\n",
    "  \"Relevance\": \"NON_RELEVANT\" | \"PARTLY_RELEVANT\" | \"RELEVANT\",\n",
    "  \"Explanation\": \"[Provide a brief explanation for your evaluation]\"\n",
    "}}\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt2_template = \"\"\"\n",
    "You are an expert evaluator for a Retrieval-Augmented Generation (RAG) system.\n",
    "Your task is to analyze the relevance of the generated answer to the given question.\n",
    "Based on the relevance of the generated answer, you will classify it\n",
    "as \"NON_RELEVANT\", \"PARTLY_RELEVANT\", or \"RELEVANT\".\n",
    "\n",
    "Here is the data for evaluation:\n",
    "\n",
    "Question: {question}\n",
    "Generated Answer: {answer_llm}\n",
    "\n",
    "Please analyze the content and context of the generated answer in relation to the question\n",
    "and provide your evaluation in parsable JSON without using code blocks:\n",
    "\n",
    "{{\n",
    "  \"Relevance\": \"NON_RELEVANT\" | \"PARTLY_RELEVANT\" | \"RELEVANT\",\n",
    "  \"Explanation\": \"[Provide a brief explanation for your evaluation]\"\n",
    "}}\n",
    "\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ground_truth) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = ground_truth[0]\n",
    "question = record['question']\n",
    "answer_llm = rag(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt2_template.format(question = question , answer_llm = answer_llm)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = minsearch_search_optimized(query, boost)\n",
    "relevance = generate_answer(prompt, search_results)\n",
    "\n",
    "print(relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in tqdm(ground_truth):\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations = []\n",
    "\n",
    "for record in tqdm(ground_truth):\n",
    "    question = record['question']\n",
    "    answer_llm = rag(question)\n",
    "    \n",
    "    prompt = prompt2_template.format(question = question , answer_llm = answer_llm)\n",
    "    search_results = minsearch_search_optimized(query, boost)\n",
    "    relevance = generate_answer(prompt, search_results)\n",
    "    evaluations.append((record['question'], answer_llm, relevance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = pd.DataFrame(evaluations, columns=['Question', 'Response', 'Evaluation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def categorize_evaluation(text):\n",
    "    if re.search(r'\"NON_RELEVANT\"', text):\n",
    "        return \"NON_RELEVANT\"\n",
    "    elif re.search(r'\"PARTLY_RELEVANT\"', text):\n",
    "        return \"PARTLY_RELEVANT\"\n",
    "    elif re.search(r'\"RELEVANT\"', text):\n",
    "        return \"RELEVANT\"\n",
    "    else:\n",
    "        return \"UNKNOWN\"\n",
    "\n",
    "df_eval['Category'] = df_eval['Evaluation'].apply(categorize_evaluation)\n",
    "\n",
    "category_counts = df_eval['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_counts = df_eval['Category'].value_counts(normalize= True)\n",
    "normalized_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interview_preparation_bot-ZQDkHgpI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
