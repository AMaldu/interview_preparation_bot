{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you will find:\n",
    "\n",
    "- chunk process of the data\n",
    "- RAG with elasticsearch and llama2\n",
    "- Retrieval evaluation\n",
    "- RAG Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/data.csv')\n",
    "documents = df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chapter': 'CHAPTER 1',\n",
       " 'title': 'Machine Learning Roles and the Interview Process',\n",
       " 'section': 'Overview of This Book',\n",
       " 'text': 'In the first part of this chapter, I’ll walk through the structure of this book. Then, I’ll discuss the various job titles and roles that use ML skills in industry. 1 I’ll also clarify the responsibilities of various job titles, such as data scientist, machine learning engineer, and so on, as this is a common point of confusion for job seekers. These will be illustrated with an ML skills matrix and ML lifecycle that will be referenced throughout the book. The second part of this chapter walks through the interview process, from beginning to end. I’ve mentored candidates who appreciated this overview since online resources often focus on specific pieces of the interview but not how they all connect together and result in an offer. Especially for new graduates 2 and readers coming from different industries, this chapter helps get everyone on the same page as well as clarifies the process. The interconnecting pieces of interviews are complex, with many types of combina‐ tions depending on the ML role you’re aiming for. This overview will help set the stage, so you’ll know what to focus your time on. For example, some online resources focus on knowledge specific to “product data scientists,” but will title the course or article “data scientist interview tips” without differentiating. For a newcomer, it’s hard to tell if that is relevant to your own career interests. After this chapter, you’ll be able to tell what skills are required for each job title, and in Chapter 2 , you’ll be able to parse out that information yourself from job postings and make your resume as relevant to the job title and job posting as possible. This chapter focuses on helping you differentiate among various ML roles, and walks through the entire interview process, as illustrated in Figure 1-1 : • Job applications and resume ( Chapter 2 ) • Technical interviews — Machine learning (Chapters 3 , 4 , and 6 ) — Coding/programming ( Chapter 5 ) • Behavioral interviews ( Chapter 7 ) • Your interview roadmap ( Chapter 8 ) • Post-interview and follow-up ( Chapter 9 ) Figure 1-1. Overview of the chapters and how they tie into the ML interview process. Depending on where you are in your ML interview journey, I encourage you to focus on the chapters and sections that seem relevant to you. I’ve also planned the book to be referenced as you go along; for example, you might iterate on your resume multi‐ ple times and then flip back to Chapter 2 when needed. The same applies to the other chapters. With that overview, let’s continue. The companion site to this book, https://susanshu.substack.com , features bonus content, helper resources, and more.',\n",
       " 'id': '86fd49a66d'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the number of tokens \n",
    "\n",
    "llama2 from Ollama has an embedding length of 4096. This means that the maximum number of tokens that can be introduced will be 4096 tokens. Let's check if the pre-chunking using the natural structure of the book is enough or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'chapter': 'CHAPTER 3', 'title': 'Technical Interview: Machine Learning Algorithms', 'section': 'Statistical and Foundational Techniques', 'num_tokens': 4151}, {'chapter': 'CHAPTER 6', 'title': 'Technical Interview: Model Deployment and End-to-End ML', 'section': 'Model Deployment', 'num_tokens': 6350}]\n"
     ]
    }
   ],
   "source": [
    "token_limit = 4096\n",
    "results = []\n",
    "big_docs = []\n",
    "\n",
    "for i, doc in enumerate(documents):\n",
    "    spacy_doc = nlp(doc['text'])\n",
    "    num_tokens = len(spacy_doc)\n",
    "\n",
    "    result = {\n",
    "        \"chapter\": doc[\"chapter\"],\n",
    "        \"title\": doc[\"title\"],\n",
    "        \"section\": doc[\"section\"],\n",
    "        \"num_tokens\": num_tokens\n",
    "    }\n",
    "    \n",
    "    results.append(result)\n",
    "    \n",
    "for res in results:\n",
    "    if res[\"num_tokens\"] > token_limit: \n",
    "        big_docs.append(res)\n",
    "        \n",
    "print(big_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking the problematic parts of the book\n",
    "\n",
    "I'm going to use the structure of the book to chunk both chapters into two parts. \n",
    "\n",
    "Since the initial parsing didn't consider subsections, I will chunk them into 1 chunk of 4000 + the following words until the end of the sentence and another chunk with the second part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_limit = 4000\n",
    "updated_documents = []\n",
    "\n",
    "for doc in documents:\n",
    "    spacy_doc = nlp(doc['text'])\n",
    "    num_tokens = len(spacy_doc)\n",
    "\n",
    "    if (doc['chapter'] in [\"CHAPTER 3\", \"CHAPTER 6\"]) and (doc['section'] in [\"Statistical and Foundational Techniques\", \"Model Deployment\"]):\n",
    "        text = doc['text']\n",
    "        tokens = nlp(text)  \n",
    "        current_chunk = []\n",
    "        current_tokens = 0\n",
    "        \n",
    "        for sentence in tokens.sents:\n",
    "            sentence_tokens = len(sentence)\n",
    "            \n",
    "            if current_tokens + sentence_tokens > token_limit:\n",
    "                updated_documents.append({\n",
    "                    \"chapter\": doc[\"chapter\"],\n",
    "                    \"title\": doc[\"title\"],\n",
    "                    \"section\": doc[\"section\"],\n",
    "                    \"text\": \" \".join([token.text for token in current_chunk]), \n",
    "                    \"num_tokens\": current_tokens,\n",
    "                    \"id\": f\"{doc['id']}_chunk_{len(updated_documents) + 1}\"  \n",
    "                })\n",
    "                current_chunk = [sentence] \n",
    "                current_tokens = sentence_tokens  \n",
    "            else:\n",
    "                current_chunk.append(sentence)\n",
    "                current_tokens += sentence_tokens\n",
    "\n",
    "        if current_chunk:\n",
    "            updated_documents.append({\n",
    "                \"chapter\": doc[\"chapter\"],\n",
    "                \"title\": doc[\"title\"],\n",
    "                \"section\": doc[\"section\"],\n",
    "                \"text\": \" \".join([token.text for token in current_chunk]), \n",
    "                \"num_tokens\": current_tokens,\n",
    "                \"id\": f\"{doc['id']}_chunk_{len(updated_documents) + 1}\"  \n",
    "            })\n",
    "    else:\n",
    "        updated_documents.append(doc)\n",
    "\n",
    "# for updated_doc in updated_documents:\n",
    "#     print(updated_doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Elasticsearch connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO -  docker config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run on the console (linux)\n",
    "\n",
    "sudo docker run -it \\\n",
    "    --rm \\\n",
    "    --name elasticsearch \\\n",
    "    -m 4GB \\\n",
    "    -p 9200:9200 \\\n",
    "    -p 9300:9300 \\\n",
    "    -e \"discovery.type=single-node\" \\\n",
    "    -e \"xpack.security.enabled=false\" \\\n",
    "    docker.elastic.co/elasticsearch/elasticsearch:8.4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create mappings and Index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'name': 'd6cc5366abe6', 'cluster_name': 'docker-cluster', 'cluster_uuid': 'v41YPv22T0Cz5Ot3oDqStA', 'version': {'number': '8.4.3', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': '42f05b9372a9a4a470db3b52817899b99a76ee73', 'build_date': '2022-10-04T07:17:24.662462378Z', 'build_snapshot': False, 'lucene_version': '9.3.0', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es_client = Elasticsearch('http://localhost:9200') \n",
    "\n",
    "es_client.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"standard_analyzer\": {\n",
    "                \"type\": \"standard\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "    \"properties\": {        \n",
    "        \"chapter\": {\n",
    "            \"type\": \"text\",\n",
    "        },\n",
    "        \"title\": {\n",
    "            \"type\": \"text\",\n",
    "        },\n",
    "        \"section\": {\n",
    "            \"type\": \"text\",\n",
    "        },\n",
    "        \"text\": {\n",
    "            \"type\": \"text\",\n",
    "            \"analyzer\": \"standard_analyzer\"  \n",
    "        },\n",
    "        \"id\":{\n",
    "            \"type\": \"keyword\",\n",
    "        },\n",
    "        \n",
    "    }\n",
    "}\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'ds-interview-questions'})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_name = \"ds-interview-questions\"\n",
    "\n",
    "# it is better to delete the index every time when experimenting\n",
    "es_client.indices.delete(index=index_name, ignore_unavailable=True) \n",
    "es_client.indices.create(index=index_name, body=index_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add documents to the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 143.43it/s]\n"
     ]
    }
   ],
   "source": [
    "for doc in tqdm(updated_documents):\n",
    "    try:\n",
    "        es_client.index(index=index_name, document=doc)\n",
    "    except Exception as e:\n",
    "        print(f\"Error when indexing the document: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'what is the scope of a data scientist?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create search function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_search(query, index=index_name):\n",
    "    \"\"\"\n",
    "    Execute a search query on the specified index.\n",
    "\n",
    "    Parameters:\n",
    "        query (dict): The search query to execute.\n",
    "        index (str): The name of the index to search.\n",
    "\n",
    "    Returns:\n",
    "        None: Prints the search results.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = es_client.search(index=index, body=query)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error during search: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_text_search(query):\n",
    "    full_text_query = {\n",
    "        \"size\": 15,\n",
    "        \"query\": {\n",
    "            \"multi_match\": {\n",
    "                \"query\": query,\n",
    "                \"fields\": [\"text^3\", \"section\", \"title\"],\n",
    "                \"type\": \"best_fields\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    full_text_results = execute_search(full_text_query)\n",
    "\n",
    "    return full_text_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "client = ollama.Client()\n",
    "\n",
    "# to initiate ollama on console for the first time\n",
    "# ollama serve\n",
    "# ollama pull llama2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "<<SYS>>\n",
    "You are an assistant preparing a candidate for a data science job interview. \n",
    "Based on the provided context, please provide a concise and accurate answer to the following question in plain text format without any additional formatting. \n",
    "<</SYS>>\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "[INST] The answer has to be plain text\n",
    " [/INST]\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what is the scope of a data scientist?'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, full_text_results):\n",
    "    context = \"\"\n",
    "\n",
    "    for hit in full_text_results['hits']['hits']: \n",
    "        text = hit['_source']['text']  \n",
    "        context += f\"Text: {text}\\n\\n\"\n",
    "\n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# prompt = build_prompt(query, search_results)\n",
    "# print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(query, full_text_results):\n",
    "    message_content = build_prompt(query, full_text_results)\n",
    "    \n",
    "    response = client.chat(model=\"llama2\", messages=[{\"role\": \"user\", \"content\": message_content}])\n",
    "    \n",
    "    if 'message' in response and 'content' in response['message']:\n",
    "        content = response['message']['content']\n",
    "        \n",
    "        return content.strip()  \n",
    "    return \"\"  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = generate_answer(query, search_results)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query):\n",
    "    full_text_results = full_text_search(query)\n",
    "    response = generate_answer(query, full_text_results)\n",
    "    return response\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some of the key skills that are important for a data scientist include:\n",
      "\n",
      "1. Programming skills: Proficiency in programming languages such as Python, R, or Julia is essential for data science tasks such as data cleaning, manipulation, and analysis.\n",
      "2. Statistical knowledge: A strong understanding of statistical concepts and methods, including hypothesis testing, regression analysis, and time series analysis, is critical for interpreting and analyzing data.\n",
      "3. Data visualization: The ability to effectively communicate insights and findings through data visualizations is crucial for presenting results to stakeholders.\n",
      "4. Machine learning: Knowledge of machine learning algorithms and techniques, such as supervised and unsupervised learning, is important for developing predictive models and classifying data.\n",
      "5. Data wrangling: The ability to clean, transform, and restructure data into a format suitable for analysis is essential for working with large datasets.\n",
      "6. Communication skills: Data scientists must be able to effectively communicate complex technical concepts to non-technical stakeholders, such as business leaders or end-users.\n",
      "7. Business acumen: A data scientist should have a good understanding of the business context and be able to identify opportunities for data-driven decision making.\n",
      "8. Data management: Knowledge of data management practices and tools, such as databases, data warehousing, and ETL processes, is important for working with large datasets.\n",
      "9. Algorithmic thinking: The ability to develop and implement algorithms to solve complex problems is a critical skill for data scientists.\n",
      "10. Ethical considerations: Data scientists must be aware of ethical considerations related to data privacy and security, and ensure that they are working in compliance with relevant regulations and standards.\n"
     ]
    }
   ],
   "source": [
    "query = 'Which skills are important for a data scietist?'\n",
    "print(rag(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>text_id</th>\n",
       "      <th>chapter</th>\n",
       "      <th>title</th>\n",
       "      <th>section</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How do you approach data preprocessing for mac...</td>\n",
       "      <td>86fd49a66d</td>\n",
       "      <td>CHAPTER 1</td>\n",
       "      <td>Machine Learning Roles and the Interview Process</td>\n",
       "      <td>Overview of This Book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can you explain the difference between supervi...</td>\n",
       "      <td>86fd49a66d</td>\n",
       "      <td>CHAPTER 1</td>\n",
       "      <td>Machine Learning Roles and the Interview Process</td>\n",
       "      <td>Overview of This Book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How do you evaluate the performance of a machi...</td>\n",
       "      <td>86fd49a66d</td>\n",
       "      <td>CHAPTER 1</td>\n",
       "      <td>Machine Learning Roles and the Interview Process</td>\n",
       "      <td>Overview of This Book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are some common pitfalls to avoid when wo...</td>\n",
       "      <td>86fd49a66d</td>\n",
       "      <td>CHAPTER 1</td>\n",
       "      <td>Machine Learning Roles and the Interview Process</td>\n",
       "      <td>Overview of This Book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How do you handle missing values in a dataset ...</td>\n",
       "      <td>86fd49a66d</td>\n",
       "      <td>CHAPTER 1</td>\n",
       "      <td>Machine Learning Roles and the Interview Process</td>\n",
       "      <td>Overview of This Book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>In what ways do you tailor your resume for dif...</td>\n",
       "      <td>1026686599</td>\n",
       "      <td>CHAPTER 9</td>\n",
       "      <td>Post-Interview and Follow-up</td>\n",
       "      <td>What to Do Between Interviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>Can you share an instance when you received an...</td>\n",
       "      <td>1026686599</td>\n",
       "      <td>CHAPTER 9</td>\n",
       "      <td>Post-Interview and Follow-up</td>\n",
       "      <td>What to Do Between Interviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>How do you handle rejection in the job search ...</td>\n",
       "      <td>1026686599</td>\n",
       "      <td>CHAPTER 9</td>\n",
       "      <td>Post-Interview and Follow-up</td>\n",
       "      <td>What to Do Between Interviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>How long should you wait before following up w...</td>\n",
       "      <td>22eb7b9b30</td>\n",
       "      <td>CHAPTER 9</td>\n",
       "      <td>Post-Interview and Follow-up</td>\n",
       "      <td>Post-Interview Steps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>What advice would you give to someone starting...</td>\n",
       "      <td>2ca59d8bf2</td>\n",
       "      <td>CHAPTER 9</td>\n",
       "      <td>Post-Interview and Follow-up</td>\n",
       "      <td>First 30/60/90 Days of Your New ML Job</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question     text_id    chapter  \\\n",
       "0    How do you approach data preprocessing for mac...  86fd49a66d  CHAPTER 1   \n",
       "1    Can you explain the difference between supervi...  86fd49a66d  CHAPTER 1   \n",
       "2    How do you evaluate the performance of a machi...  86fd49a66d  CHAPTER 1   \n",
       "3    What are some common pitfalls to avoid when wo...  86fd49a66d  CHAPTER 1   \n",
       "4    How do you handle missing values in a dataset ...  86fd49a66d  CHAPTER 1   \n",
       "..                                                 ...         ...        ...   \n",
       "235  In what ways do you tailor your resume for dif...  1026686599  CHAPTER 9   \n",
       "236  Can you share an instance when you received an...  1026686599  CHAPTER 9   \n",
       "237  How do you handle rejection in the job search ...  1026686599  CHAPTER 9   \n",
       "238  How long should you wait before following up w...  22eb7b9b30  CHAPTER 9   \n",
       "239  What advice would you give to someone starting...  2ca59d8bf2  CHAPTER 9   \n",
       "\n",
       "                                                title  \\\n",
       "0    Machine Learning Roles and the Interview Process   \n",
       "1    Machine Learning Roles and the Interview Process   \n",
       "2    Machine Learning Roles and the Interview Process   \n",
       "3    Machine Learning Roles and the Interview Process   \n",
       "4    Machine Learning Roles and the Interview Process   \n",
       "..                                                ...   \n",
       "235                      Post-Interview and Follow-up   \n",
       "236                      Post-Interview and Follow-up   \n",
       "237                      Post-Interview and Follow-up   \n",
       "238                      Post-Interview and Follow-up   \n",
       "239                      Post-Interview and Follow-up   \n",
       "\n",
       "                                    section  \n",
       "0                     Overview of This Book  \n",
       "1                     Overview of This Book  \n",
       "2                     Overview of This Book  \n",
       "3                     Overview of This Book  \n",
       "4                     Overview of This Book  \n",
       "..                                      ...  \n",
       "235           What to Do Between Interviews  \n",
       "236           What to Do Between Interviews  \n",
       "237           What to Do Between Interviews  \n",
       "238                    Post-Interview Steps  \n",
       "239  First 30/60/90 Days of Your New ML Job  \n",
       "\n",
       "[240 rows x 5 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../../data/ground_truth_data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions = df[['question', 'text_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'How do you approach data preprocessing for machine learning models?',\n",
       " 'text_id': '86fd49a66d'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth = df_questions.to_dict(orient = 'records')\n",
    "ground_truth[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_rate(relevance_total):\n",
    "    cnt = 0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        if isinstance(line, (list, tuple)) and True in line:\n",
    "            cnt += 1\n",
    "\n",
    "    return cnt / len(relevance_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mrr(relevance_total):\n",
    "    total_score = 0.0\n",
    "    num_queries = len(relevance_total)\n",
    "\n",
    "    for line in relevance_total:\n",
    "        query_score = 0.0\n",
    "        for rank in range(len(line)):\n",
    "            if line[rank] == True:\n",
    "                query_score = 1 / (rank + 1)\n",
    "                break  \n",
    "\n",
    "        total_score += query_score\n",
    "\n",
    "    return total_score / num_queries if num_queries > 0 else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240/240 [00:01<00:00, 148.43it/s]\n"
     ]
    }
   ],
   "source": [
    "relevance_total = []\n",
    "\n",
    "for q in tqdm(ground_truth):\n",
    "    doc_id = q['text_id']\n",
    "    text_results = full_text_search(q['question'])\n",
    "    hits = text_results.get('hits', {}).get('hits', [])\n",
    "    relevance = [doc['_source']['id'] == doc_id for doc in hits]\n",
    "    relevance_total.append(relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.775, 0.5180606893106892)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hit_rate(relevance_total), mrr(relevance_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparams Optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "from sklearn.model_selection import ParameterGrid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = df_questions[:100]\n",
    "df_test = df_questions[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    'title_boost': hp.uniform('title_boost', 1.0, 5.0),\n",
    "    'content_boost': hp.uniform('content_boost', 1.0, 3.0),\n",
    "    'minimum_should_match': hp.choice('minimum_should_match', [\"50%\", \"75%\", \"90%\"])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_results(results, query):\n",
    "    doc_ids = [hit[\"_id\"] for hit in results[\"hits\"][\"hits\"]]\n",
    "    # For uniform relevance, we can calculate hit rate as a measure\n",
    "    # Assuming we want to check if any document was returned\n",
    "    return int(len(doc_ids) > 0)  # Returns 1 if there are results, else 0\n",
    "\n",
    "# Hyperparameter search function\n",
    "def hyperparameter_optimization(df_questions):\n",
    "    # Define hyperparameters to optimize\n",
    "    param_grid = {\n",
    "        'fuzziness': ['AUTO', '1', '2'],\n",
    "        'operator': ['AND', 'OR'],\n",
    "        'size': [5, 10, 20]\n",
    "    }\n",
    "\n",
    "    best_score = 0\n",
    "    best_params = {}\n",
    "\n",
    "    for params in ParameterGrid(param_grid):\n",
    "        scores = []\n",
    "\n",
    "        for _, row in df_questions.iterrows():\n",
    "            query = row['question']  # Assuming df_questions has a 'question' column\n",
    "            results = full_text_search(query, params)\n",
    "            score = evaluate_results(results, query)\n",
    "            scores.append(score)\n",
    "\n",
    "        avg_score = np.mean(scores)\n",
    "\n",
    "        if avg_score > best_score:\n",
    "            best_score = avg_score\n",
    "            best_params = params\n",
    "\n",
    "    return best_params, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "full_text_search() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m best_hyperparams, best_score \u001b[38;5;241m=\u001b[39m \u001b[43mhyperparameter_optimization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_questions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Output the best hyperparameters\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Hyperparameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, best_hyperparams)\n",
      "Cell \u001b[0;32mIn[34], line 24\u001b[0m, in \u001b[0;36mhyperparameter_optimization\u001b[0;34m(df_questions)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m df_questions\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m     23\u001b[0m     query \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Assuming df_questions has a 'question' column\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mfull_text_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     score \u001b[38;5;241m=\u001b[39m evaluate_results(results, query)\n\u001b[1;32m     26\u001b[0m     scores\u001b[38;5;241m.\u001b[39mappend(score)\n",
      "\u001b[0;31mTypeError\u001b[0m: full_text_search() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "best_hyperparams, best_score = hyperparameter_optimization(df_questions)\n",
    "\n",
    "# Output the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", best_hyperparams)\n",
    "print(\"Best Score:\", best_score)\n",
    "\n",
    "\n",
    "for _, row in df_val.iterrows():\n",
    "    query = row['question']\n",
    "    val_results = full_text_search(query, best_hyperparams)\n",
    "    print(\"Validation Results:\", val_results)\n",
    "\n",
    "# Example of applying the best parameters on test set\n",
    "for _, row in df_test.iterrows():\n",
    "    query = row['question']\n",
    "    test_results = full_text_search(query, best_hyperparams)\n",
    "    print(\"Test Results:\", test_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'relevant_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/share/virtualenvs/interview_preparation_bot-ZQDkHgpI/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'relevant_ids'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m df_test\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m      3\u001b[0m     query \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Assuming your DataFrame has a 'question' column\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     relevant_ids \u001b[38;5;241m=\u001b[39m \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelevant_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Replace with your actual relevant ID retrieval logic\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     results \u001b[38;5;241m=\u001b[39m full_text_search(query, best_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboost_title\u001b[39m\u001b[38;5;124m\"\u001b[39m], best_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboost_content\u001b[39m\u001b[38;5;124m\"\u001b[39m], best_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_score\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      6\u001b[0m     score \u001b[38;5;241m=\u001b[39m evaluate_results(results, relevant_ids)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/interview_preparation_bot-ZQDkHgpI/lib/python3.10/site-packages/pandas/core/series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/interview_preparation_bot-ZQDkHgpI/lib/python3.10/site-packages/pandas/core/series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/interview_preparation_bot-ZQDkHgpI/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'relevant_ids'"
     ]
    }
   ],
   "source": [
    "test_scores = []\n",
    "for _, row in df_test.iterrows():\n",
    "    query = row['question']  # Assuming your DataFrame has a 'question' column\n",
    "    relevant_ids = row['relevant_ids']  # Replace with your actual relevant ID retrieval logic\n",
    "    results = full_text_search(query, best_params[\"boost_title\"], best_params[\"boost_content\"], best_params[\"min_score\"])\n",
    "    score = evaluate_results(results, relevant_ids)\n",
    "    test_scores.append(score)\n",
    "\n",
    "print(\"Test set scores:\", test_scores)\n",
    "print(\"Average test score:\", np.mean(test_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../../data/best_hyperparameters_elasticsearch.json\", \"w\") as json_file:\n",
    "#     json.dump(best_params, json_file)\n",
    "\n",
    "# print(\"Best hyperparameters saved to 'best_hyperparameters.json':\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_val = df_val.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minsearch_search_optimized(query, boost):\n",
    "    # boost = {'text': 3.0, 'section': 0.5}\n",
    "    \n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict = {},\n",
    "        boost_dict=boost,\n",
    "        num_results=5)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost = {'text': best['boost']}\n",
    "         \n",
    "evaluate(gt_val, lambda q: minsearch_search_optimized(q['question'], boost))\n",
    "# para mirar cuanto da con los mejores hyperparam "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little bit better :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1_template = \"\"\"\n",
    "You are an expert evaluator for a RAG system.\n",
    "Your task is to analyze the relevance of the generated answer compared to the original answer provided.\n",
    "Based on the relevance and similarity of the generated answer to the original answer, you will classify\n",
    "it as \"NON_RELEVANT\", \"PARTLY_RELEVANT\", or \"RELEVANT\".\n",
    "\n",
    "Here is the data for evaluation:\n",
    "\n",
    "Original Answer: {answer_orig}\n",
    "Generated Question: {question}\n",
    "Generated Answer: {answer_llm}\n",
    "\n",
    "Please analyze the content and context of the generated answer in relation to the original\n",
    "answer and provide your evaluation in parsable JSON without using code blocks:\n",
    "\n",
    "{{\n",
    "  \"Relevance\": \"NON_RELEVANT\" | \"PARTLY_RELEVANT\" | \"RELEVANT\",\n",
    "  \"Explanation\": \"[Provide a brief explanation for your evaluation]\"\n",
    "}}\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt2_template = \"\"\"\n",
    "You are an expert evaluator for a Retrieval-Augmented Generation (RAG) system.\n",
    "Your task is to analyze the relevance of the generated answer to the given question.\n",
    "Based on the relevance of the generated answer, you will classify it\n",
    "as \"NON_RELEVANT\", \"PARTLY_RELEVANT\", or \"RELEVANT\".\n",
    "\n",
    "Here is the data for evaluation:\n",
    "\n",
    "Question: {question}\n",
    "Generated Answer: {answer_llm}\n",
    "\n",
    "Please analyze the content and context of the generated answer in relation to the question\n",
    "and provide your evaluation in parsable JSON without using code blocks:\n",
    "\n",
    "{{\n",
    "  \"Relevance\": \"NON_RELEVANT\" | \"PARTLY_RELEVANT\" | \"RELEVANT\",\n",
    "  \"Explanation\": \"[Provide a brief explanation for your evaluation]\"\n",
    "}}\n",
    "\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ground_truth) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = ground_truth[0]\n",
    "question = record['question']\n",
    "answer_llm = rag(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt2_template.format(question = question , answer_llm = answer_llm)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = minsearch_search_optimized(query, boost)\n",
    "relevance = generate_answer(prompt, search_results)\n",
    "\n",
    "print(relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in tqdm(ground_truth):\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations = []\n",
    "\n",
    "for record in tqdm(ground_truth):\n",
    "    question = record['question']\n",
    "    answer_llm = rag(question)\n",
    "    \n",
    "    prompt = prompt2_template.format(question = question , answer_llm = answer_llm)\n",
    "    search_results = minsearch_search_optimized(query, boost)\n",
    "    relevance = generate_answer(prompt, search_results)\n",
    "    evaluations.append((record['question'], answer_llm, relevance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = pd.DataFrame(evaluations, columns=['Question', 'Response', 'Evaluation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def categorize_evaluation(text):\n",
    "    if re.search(r'\"NON_RELEVANT\"', text):\n",
    "        return \"NON_RELEVANT\"\n",
    "    elif re.search(r'\"PARTLY_RELEVANT\"', text):\n",
    "        return \"PARTLY_RELEVANT\"\n",
    "    elif re.search(r'\"RELEVANT\"', text):\n",
    "        return \"RELEVANT\"\n",
    "    else:\n",
    "        return \"UNKNOWN\"\n",
    "\n",
    "df_eval['Category'] = df_eval['Evaluation'].apply(categorize_evaluation)\n",
    "\n",
    "category_counts = df_eval['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_counts = df_eval['Category'].value_counts(normalize= True)\n",
    "normalized_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interview_preparation_bot-ZQDkHgpI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
