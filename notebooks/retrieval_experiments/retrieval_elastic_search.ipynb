{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval approach with Elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='tqdm')\n",
    "\n",
    "import json\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained Model used for creation of embeddings\n",
    "\n",
    "The model used to create the embeddings can be found in this website\n",
    "https://www.sbert.net/docs/sentence_transformer/pretrained_models.html#semantic-search-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maldu/.local/share/virtualenvs/interview_preparation_bot-ZQDkHgpI/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output of the model has 768 dimensional embeddings\n"
     ]
    }
   ],
   "source": [
    "print(f'The output of the model has {len(model.encode(\"How many features or dimensions the model uses to represent the input text?\"))} dimensional embeddings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flattening the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/parsed_book.json', 'rt') as f_in:\n",
    "    book_raw = json.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chapter': 'CHAPTER 1',\n",
       " 'title': 'Machine Learning Roles and the Interview Process',\n",
       " 'content': [{'section': 'Overview of This Book',\n",
       "   'text': 'In the first part of this chapter, I’ll walk through the structure of this book. Then, I’ll discuss the various job titles and roles that use ML skills in industry. 1 I’ll also clarify the responsibilities of various job titles, such as data scientist, machine learning engineer, and so on, as this is a common point of confusion for job seekers. These will be illustrated with an ML skills matrix and ML lifecycle that will be referenced throughout the book. The second part of this chapter walks through the interview process, from beginning to end. I’ve mentored candidates who appreciated this overview since online resources often focus on specific pieces of the interview but not how they all connect together and result in an offer. Especially for new graduates 2 and readers coming from different industries, this chapter helps get everyone on the same page as well as clarifies the process. The interconnecting pieces of interviews are complex, with many types of combina‐ tions depending on the ML role you’re aiming for. This overview will help set the stage, so you’ll know what to focus your time on. For example, some online resources focus on knowledge specific to “product data scientists,” but will title the course or article “data scientist interview tips” without differentiating. For a newcomer, it’s hard to tell if that is relevant to your own career interests. After this chapter, you’ll be able to tell what skills are required for each job title, and in Chapter 2 , you’ll be able to parse out that information yourself from job postings and make your resume as relevant to the job title and job posting as possible. This chapter focuses on helping you differentiate among various ML roles, and walks through the entire interview process, as illustrated in Figure 1-1 : • Job applications and resume ( Chapter 2 ) • Technical interviews — Machine learning (Chapters 3 , 4 , and 6 ) — Coding/programming ( Chapter 5 ) • Behavioral interviews ( Chapter 7 ) • Your interview roadmap ( Chapter 8 ) • Post-interview and follow-up ( Chapter 9 ) Figure 1-1. Overview of the chapters and how they tie into the ML interview process. Depending on where you are in your ML interview journey, I encourage you to focus on the chapters and sections that seem relevant to you. I’ve also planned the book to be referenced as you go along; for example, you might iterate on your resume multi‐ ple times and then flip back to Chapter 2 when needed. The same applies to the other chapters. With that overview, let’s continue. The companion site to this book, https://susanshu.substack.com , features bonus content, helper resources, and more.'},\n",
       "  {'section': 'A Brief History of Machine Learning and Data Science Job Titles',\n",
       "   'text': 'First, let’s walk through a brief history of job titles. I decided to start with this section to dispel some myths about the “data scientist” job title and shed some light on why there are so many ML-related job titles. After understanding this history, you should be more aware of what job titles to aim for yourself. If you’ve ever been confused about the litany of titles such as machine learning engineer (MLE), product data sci‐ entist, MLOps engineer, and more, this section is for you. ML techniques aren’t a new thing; in 1985, David Ackley, Geoffrey E. Hinton, and Terrence J. Sejnowski popularized the Boltzmann Machine algorithm. 3 Even before that, regression techniques 4 had early developments in the 1800s. There have long been jobs and roles that use modeling techniques to forecast and predict. Econome‐ tricians, statisticians, financial modelers, physics modelers, and biochemical modelers have existed as professions for decades. The main difference is that there were much smaller datasets compared to the modern day (barring simulations). It was only in recent years, just before the 21st century, when compute power started to increase exponentially. In addition, advances in distributed and parallel computing created a cycle in which “big data” became more readily available. This allowed practitioners to apply that advanced compute power to millions or billions of data points. Larger datasets started being accumulated and distributed for ML research, such as WordNet, 5 and, subsequently, ImageNet, 6 a project led by Fei-Fei Li. These collective efforts laid the foundation for even more ML breakthroughs. AlexNet 7 was released in 2012, achieving high accuracy in the ImageNet challenge, 8 which demonstrated that deep learning can be adept at humanlike tasks at a scale that had not been seen before. Many ML practitioners see this as a time when machine learning, deep learning, and related topics increased by leaps and bounds in terms of recognition from the broader population, not just the AI community. The recent popularity of generative AI (such as ChatGPT) in 2022 and 2023 didn’t come out of nowhere, nor did the deepfakes, self-driving cars, chess bots, and more that came before it; these applica‐ tions were the results of many advances over recent years. “Data scientist” as a job title began as an umbrella term, when the ML and data fields were less mature. The term “data scientist” on Google Trends , which measures the popularity of search terms, surged in 2012. That was the year when that article was published by Harvard Business Review: “Data Scientist: The Sexiest Job of the 21st Century.” 9 By April 2013, the search popularity of “data scientist” was already tied with “statistician” and subsequently surpassed it by magnitudes, as shown in Figure 1-2 . Back in those days, there wasn’t a narrow divide between infrastructure jobs and model training, though. For example, Kubernetes was first released in 2014, but companies have taken some time to adopt it for orchestrating ML jobs. So now there are more specific job titles for ML infrastructure that didn’t exist before. Figure 1-2. Search popularity for the terms “data scientist,” “machine learning engi‐ neer,” and “statistician” on Google Trends (retrieved August 9, 2023). As social media, web recommender systems, and other modern use cases increased, companies started gathering much more granular data, such as clickstream data , which is data collected as a user browses a website or app. Another recent advance‐ ment is an average corporation being able to store the sheer amount of telemetry from machines and Internet of Things (IoT) devices. Previously, data scientists may have worked with data that was updated weekly or daily. Now, as many applications update more frequently or in real time, more infrastructure is needed to serve ML functionality in web products and apps, so more jobs have been created around those functions as well. In short: as the machine learning lifecycle grew more complex, more job titles were created to describe the new skills that a full ML team now requires. I’ll elaborate more on the job titles and ML lifecycle later in this chapter. All of this happened within the last decade, and companies don’t always change their job titles to reflect how the roles have become more specialized. Regardless, as a can‐ didate, knowing this history can help reduce confusion and frustration from applying for a job and finding the role is different from another company’s job with the exact same title. See Table 1-1 for previous trends in ML-related job titles and Table 1-2 for current trends in ML job titles. Table 1-1. Previous trends of ML and data job titles Table 1-2. Current trends of ML and data job titles With that history to explain why you will encounter different job titles, I’ll elaborate on each of these job titles and their responsibilities.'},\n",
       "  {'section': 'Job Titles Requiring ML Experience',\n",
       "   'text': 'Here is a nonexhaustive list of job titles for ML (or closely related) roles: • Data scientist • Machine learning engineer • Applied scientist • Software engineer, machine learning • MLOps engineer • Product data scientist • Data analyst • Decision scientist • Research scientist As I discussed “A Brief History of Machine Learning and Data Science Job Titles” on page 3 , each role is responsible for a different part of the ML lifecycle. A job title alone does not convey what the job entails. As a job seeker, be warned: in different companies, completely different titles might end up doing similar jobs! As illustrated in Figure 1-3 , your ML job title will depend on the company, the team, and which part(s) of the ML lifecycle your role is responsible for. To give specific examples of how job titles can depend on the company or organiza‐ tion that is hiring for the job—based on real people I’ve spoken to, job descriptions, and job interviews—the person responsible for training ML models but not for build‐ ing the underlying platform might be called the following: • Software engineer (ML) or data scientist (Google) • Applied scientist (Amazon) • Machine learning engineer (Meta, Pinterest) • Data scientist (Elastic, the team where I work) • Data scientist (Unity) Figure 1-3. What’s in a machine learning job title? By the time this book is published, any of the job titles within these companies and teams may have changed. Regardless, it demon‐ strates the point that ML titles may vary between companies and even between different teams in the same company. The job title also depends on the organization, the department, and so on. Some departments in Google have the data scientist 12 job title, and some don’t. At the com‐ panies where I’ve worked, my teams had data scientists train ML models while MLEs built the infrastructure (working all day in tools such as Kubernetes, Terraform, Jen‐ kins, and so on). In some other companies, MLEs are the ones who train ML models. As a personal example, my work experience has heavily involved ML model training, so I apply for jobs that have the title “machine learning engineer” or “data scientist.” I’ll provide more examples of skills and roles that could be a good fit for your inter‐ ests and skills in the following sections. As mentioned in the Preface , this book focuses more on the industry applications of ML rather than research roles. Here’s a brief overview of research roles: Requirements Most often PhD. Responsibilities Similar to academic roles/academia, such as conducting research and coming up with novel algorithms and improvements, authoring papers, presenting at aca‐ demic conferences, and so on. If you’re in a research role in industry , such as a researcher at Google DeepMind, the main difference is that there isn’t a require‐ ment for teaching (that I know of).'},\n",
       "  {'section': 'Machine Learning Lifecycle',\n",
       "   'text': 'In industry, it is an expectation for applied ML projects to eventually improve the customer experience—for example, a better recommender system that shows the user more relevant videos, news, and social media posts. In industry, “customer” can also mean internal customers: people within the same company or organization. For example, your team builds ML models that predict demand, which helps your com‐ pany’s logistics department better plan its shipment schedules. Regardless of whether the user is external or internal, many components are involved in building a full the user is external or internal, many components are involved in building a fullfledged, end-to-end ML product. I’ll walk through a simplified example. First, there needs to be data, as most ML is trained and tested with large amounts of data. Someone needs to make sure the raw data is brought in (ingested) so that it’s easily accessible later on for data analysis, ML, reporting and monitoring, and so on. This is illustrated by step A (data) in Figure 1-4 . Next, with the data in place, someone with knowledge of ML algorithms and tools will use the data to start ML development. This is illustrated by step B (machine learning development) in Figure 1-4 . This involves feature engineering, model train‐ ing, and evaluation. If the results aren’t great, there is a lot of iteration in step B, and this person might enhance their feature engineering or model training, or even go back to step A and request that more data be ingested. Once there are somewhat satisfactory results, they’ll move on to step C (machine learning deployment), which connects the ML models to customers. Depending on the type of ML project, it could be deployed to a website, app, internal dashboard, and so on. Of course, they’d like to make sure the ML is working properly, so any good team will have a way to monitor the results. In ML there are two main types of potential issues. The first is that something in the software layer doesn’t work, such as bugs in the code. The second is a data or ML model issue—for example, in the model-development phase, the model outputs normal results, but after deployment/ release, there is data imbalance, so then the model results become undesirable. From step C onward, there can be more iteration back to step B to improve the models and run more experiments in step C again. Figure 1-4. Machine learning lifecycle (the graph is simplified for understanding). In the machine learning lifecycle I just walked through, a lot of skills are required. Data pipelines, model training, maintaining continuous integration and continuous deployment (CI/CD): as a job candidate, what should you be learning to prepare for the interview? Thankfully, as I mentioned in “A Brief History of Machine Learning and Data Science Job Titles” on page 3 companies nowadays might hire people who have a subset of these skills. For example, they need some people specialized in step A (data engineering), some specialized in step B (ML development), some in step C (ML deployment), and so forth. I emphasize the might since it still differs depending on the company or team; I will walk through some scenarios. Startup roles will usually wear more hats, meaning they will need to do the jobs in multiple steps in the machine learning lifecycle as illustrated in Figure 1-4 . Here’s an example: Usually, startup companies have the goal of shipping 13 an end-to-end product, but because they have fewer customers they might care less about the scale and stability (at an early stage). Hence, it’s more likely that the person developing and training ML models is the same person doing data analysis and presenting to stakeholders, or even the same person building the platform infrastructure. An ML team in a startup might simply have fewer people. For example, the startup might have 30 software engineers and data people in total, whereas larger corporations could have a team of data ana‐ lysts alone numbering 30 people to disperse the workload. If the company and/or team has grown enough, it is more likely the ML roles have become more specialized. Generally, the larger the team, the more specialized the role. If the “machine learning engineer” at a larger company trains models, then it’s likely they don’t wear two or three hats at once, as they might at a startup. Instead, the big company hires more people to fill those roles. That isn’t to say the work is simpler at a larger company. In fact, there’s often more data, more scale, and more downsides if the ML functionality goes down, so each MLE’s time could be com‐ pletely tied up wearing only one hat. Larger company size often corresponds to larger ML teams, but it depends. For example, a large company in a traditionally nontech industry, might have its first ML team hires operate in more of a startup-like environment while they figure out how ML best works for the company. Let’s go one level deeper and add more details of ML or data responsibilities. Figure 1-5 expands on the machine learning lifecycle from Figure 1-4 to reflect teams or companies with more fine-grained roles. (It’s worth repeating that even if this list is a useful and common enough heuristic, it’s still a bit simplified for illustration pur‐ poses since there will always be exceptions and outliers.) Figure 1-5. Machine learning lifecycle with more fine-grained roles (extended version of Figure 1-4 ). Here’s an example of what your role might be responsible for within these more fine Here’s an example of what your role might be responsible for within these more finegrained roles, as illustrated in Figure 1-5 : • You build the data pipelines for analytics and ML (step A). • You train ML models (step B). • You build the infrastructure for ML models to be deployed (step C.1). • You design and conduct hypothesis testing, often A/B tests, for new ML product features (step C.2). • You do data analysis, build reports and dashboards, and present to stakeholders (step D). Figure 1-5 is often referred to in later chapters, so save or book‐ mark it!'},\n",
       "  {'section': 'The Three Pillars of Machine Learning Roles',\n",
       "   'text': 'To set the stage for the rest of the book, I’ll go over what I call the three pillars of ML and data science roles: • Machine learning algorithms and data intuition • Programming and software engineering skills • Execution and communication skills These are the broad categories of skills that you will be evaluated on during ML job interviews. This book focuses a lot on helping you understand these skills and bridge any gaps between your current experiences and skills and those under these three pillars (see Figure 1-6 ). All these skills will be expanded on in the following chapters. Figure 1-6. Three pillars of machine learning jobs. You’re able to understand the underlying workings of ML algorithms and statistics theory and their respective trade-offs—which is essential when you’re faced with an open-ended question in a real-world ML project at work. You’re not just following steps as you would for a school assignment. Having data intuition means that when you’re faced with a new problem, you know how to use data to solve it; and when you encounter new data or data sources, you know how to dive in to evaluate them. You ask yourself, is this data suitable for ML? What types of ML models might it be suitable for? Are there any issues with the data before you can use it for ML? You know what to ask and how to find the answers. In the ML job-interview process, various types of interviews and interview questions are aimed at assessing a candidate’s knowledge and readiness in this pillar, which I’ll cover in Chapters 3 and 4 . While working on a project, you have the programming skills required to deliver, such as manipulating data with Python or using an internal deploy process so that another team can use the results from the ML model. Even if you know the theory well, without the programming or software engineer‐ ing 14 sense, you can’t make ML materialize out of thin air. You need to use code to connect the data with ML algorithms, which are also implemented with code—that is, you must convert theory to practice. Other programming skills in high demand for ML roles are the (software) engineer’s ability to transition from prototype to production—that is, the ML is integrated and released. Some roles are responsible for end-to-end ML: from researching and train‐ ing models to deployment and production. Some ML roles, such as MLOps engi‐ neers, are responsible for building software infrastructure that can handle the demands of processing large amounts of data to send ML responses to users in sec‐ onds or even milliseconds. In the ML job-interview process, various types of interviews and interview questions assess a candidate’s skills in this pillar, which I’ll walk through in Chapters 5 and 6 . You’re able to work with people who aren’t in the same role as you. In ML, we work with software engineers, data engineers, product managers, and many other collea‐ gues. The ability to get things done in a team encompasses a few soft skills such as communication and some project management skills. For example, being unable to communicate with team members is a real blocker 15 for projects and could cause your ML projects to languish or even be deprioritized. Even in cases where you work with only one person (say, your boss), you still need to be able to report on your projects, which requires communication skills. Consequently, in the ML field a highly in-demand skill is being able to communicate technical con‐ cepts with nontechnical stakeholders. You’ll also need some project management skills to keep your tasks on track. We all learn to how manage our to-do lists and calendars during the process of education or self-learning, but it’s more chaotic since now your project calendar depends on oth‐ ers’ calendars and priorities. Even if you have a project and/or product manager to keep the team on track, you still need to manage yourself to some extent. Without soft skills, things don’t get done, full stop. Don’t be that candidate who focu‐ ses only on technical skills but neglects building and demonstrating their soft skills in interviews. I’ll delve into the details of how ML interviews evaluate candidates on this pillar in Chapter 7 . Growing your skills in all three ML pillars is a tall order, and for entry-level roles you’re usually only expected to have a minimum (such as a 3/10) for each pillar, as illustrated in Figure 1-7 . For example, a job candidate who has some exposure to pro‐ gramming, even if they aren’t skilled or experienced, can be taught to improve. Ideally, you’d be stronger on at least one pillar (such as 5/10 for programming) that is most related to the particular ML role in order to stand out from other job candidates. Figure 1-7. Minimum required skill levels for ML jobs (example). For senior roles, the bare minimum requirements are higher, but a similar rule of thumb applies: clear the minimum skill requirements. From then on, you’ll be com‐ pared with other candidates on the skills that you are great in, depending on the role. Data scientists who only train ML models but don’t deploy them might not need to develop their programming skills as much as their ML theory and communication skills. For entry level roles, I’d argue that the communication pillar has a lower requirement (but not 0/10, please!) because it takes the hard-earned experience of working with a larger group of people, including nontechnical teammates, to raise it higher. This also gives some candidates an edge in this pillar: for those with a nontraditional back‐ ground, such as candidates who are self-taught or switching from software engineer roles or another field, the ability to adeptly tell a story and showcase a portfolio can set them apart from other candidates. Now that you’ve had an overview of the three pillars, you can use this mental model to stand out.'},\n",
       "  {'section': 'Machine Learning Skills Matrix',\n",
       "   'text': 'Congratulations! You’ve made it to the end of a pretty dense section! Now that you’ve gone through the overview of the machine learning lifecycle and three pillars of ML skills, it’s time for you to map your interests and skills to job titles. Table 1-3 will give you a rough idea of what skills you will need to learn in order to succeed in specific roles. On a scale from one to three stars, one star represents a skill of lower importance, and three stars represents a highly important skill. Table 1-3. Machine learning and data skills matrix Table 1-3 is often referred to in later chapters, so save or bookmark it! Taking a look at these skills, you can roughly map them to the three pillars of ML skills in the previous section, as shown in Table 1-4 . Table 1-4. Machine learning and data skills mapped to the three pillars of ML jobs It’s OK if you aren’t completely sure what each type of skill might entail just yet. In Chapter 2 , we will revisit this matrix, and there will be details and a checklist for self assessment. You don’t need to worry about every single skill; companies are aware that they are responsible for training up new grads. But you can stand out from other job candi‐ dates by showing that you are more easily trainable and can learn fast. An easy way to demonstrate this earlier in your ML career is to gain high-level (not necessarily deep) exposure to topics that you don’t have experience with yet. For example, even if you haven’t worked much with version control, it’s a bonus to be familiar with. You can achieve this by watching some videos (30 minutes) and installing/testing it out on a project (one hour). Now, let’s tie all this together. We’ve looked at the machine learning lifecycle ( Figure 1-5 ) and machine learning skills matrix ( Table 1-3 ). What’s left is to see what jobs are best for you to apply to now or to gain the skills for! To do so, let’s connect everything to the current trend of ML and data job titles ( Table 1-2 ). This is illustra‐ ted in Figure 1-8 . Figure 1-8. Common ML job titles and how they correspond to the ML lifecycle. The alphabetical annotations in Figure 1-8 can be mapped to those in Figure 1-5 , lis‐ ted here for convenience: • (A) Data • (B) Machine learning development • (C.1) ML/software infrastructure • (C.2) ML hypothesis testing/monitoring • (D) Reports and dashboards Figure 1-8 is often referred to in later chapters, so save or book‐ mark it! When you see a job title and check the details of the job posting, you can map it to what that job is likely responsible for in the day to day. In addition, based on what part of the ML lifecycle you’re interested in, you can better prepare and target your job applications, so you don’t accidentally bark up the wrong tree. Go to a job board of your choice, such as LinkedIn, Indeed, or others listed in Chap‐ ter 2 . Search “machine learning,” “data scientist,” “data,” “AI,” “generative AI,” and so on. What are you seeing? Do you see different types of jobs being advertised that all use ML?'},\n",
       "  {'section': 'Introduction to ML Job Interviews',\n",
       "   'text': 'Now that I’ve introduced many job titles that might be of interest to you, it’s time to go through all the steps and types of interviews you will encounter during the pro‐ cess! This book is called Machine Learning Interviews , but interviews are so much more than just interview questions. There are job applications and your resume, which are how you get interviews in the first place. If you don’t increase your chances of getting more interviews, then you won’t even get the chance to answer any inter‐ view questions! I’ll be covering the process from beginning to end, including how to follow up after the interview ( Chapter 9 ). To quickly set the stage, here are some common terms used in this book. When I use the term “interviewee,” I am referring to the person currently seeking employment, while the “interviewer” is currently employed at the company that the interviewee is interviewing to join. The interviewee is also referred to as a “candidate” or “job candidate,” since they are a candidate to be the successfully hired person (see Table 1-5 ). Table 1-5. Synonyms of common terms used in this book “Big tech” refers to the major, large tech companies. Because of constant changes in the industry—for example, Facebook rebranding to Meta as the parent company and Google doing something similar with Alphabet—the popular FAANG. 16 (Facebook, Apple, Amazon, Netflix, Google) acronym has already become outdated. To keep things simple, I will use the umbrella term “big tech.”'},\n",
       "  {'section': 'Machine Learning Job-Interview Process',\n",
       "   'text': 'Now let’s get into the entire job-interview process. You’ll start by applying to jobs, then interviewing, and then, after some rounds of interviewing, finally receiving offers. This process is detailed in Figure 1-9 . Figure 1-9 is often referred to in later chapters, so save or book‐ mark it! Figure 1-9. ML interview process. Let’s imagine that you’re just starting out and applying for an ML role at a company with an established HR 17 and hiring process. You can begin your application in a few ways: by cold applying through the company website or job board (discussed in Chapter 2 ) or through a referral from someone within the team or company. You can also get interviews through cold messaging on LinkedIn or by emailing recruiters. Usually, at companies that have an HR-tracking software system, even if someone refers you, you’ll still need to upload a standard application into the online portal, which means you’ll need to prepare an updated resume and fill in your information. You may also choose to supplement your job-search efforts by working with a third-party recruiter, which is differentfrom an in from an inhouse recruiter who works or contracts specifically for the hiring company. Third-party recruiters often work with multiple compa‐ nies at once. Professional peers I know recommend working only with specific trusted third-party recruiters but warned me to beware those who make too many unrealistic promises or aren’t reputable. You can read more about third-party recruiters in this Forbes article . Using the first method—cold applying through company websites or third-party job boards—you’ve been browsing job boards like Indeed 18 as well as going directly to the career pages of companies you’re interested in working for. In this scenario, you don’t have someone referring you to the team or company (I’ll cover that in “Apply‐ ing via a Referral” on page 22 ). You’ve seen some ML-related jobs that seem relevant to you, and you clicked the links to apply. After you submit your application and the company has your information and resume, an HR member, recruiter, or whoever is in charge of resume screening, will proceed with the next step. The reality is that jobs have many applicants, and you should assume the first batch of applicants will be filtered before the hiring manager sees them. The hiring manager is the manager you’ll work with and report to if you join the team. So you can usually assume that a generalized HR partner or internal or external recruiters will be read‐ ing your resume first. These recruiters may be somewhat familiar with the roles they are screening resumes for, but they are still predominantly generalists, not as special‐ ized as the engineers and ML professionals you’ll actually be working with. This part of the screening process leads to several hidden criteria for your resume, which is why it might be baffling when your resume doesn’t clear this step even if you have a relevant background. It’s important to remember that these generalists will likely pass along your resume to the hiring manager if they: • See key technologies or experiences on your resume based on the job posting • See years of experience in key technologies or experiences or, in the case of entry • See years of experience in key technologies or experiences or, in the case of entrylevel or new-grad jobs, sufficient evidence that you can be easily trained • Understand that your skills and accomplishments are relevant, in plain language To determine whether your resume meets the criteria, the recruiter will likely be searching for keywords and comparing your resume to the job posting. They will not automatically “translate” skills on your resume for you. For example, if the job description says “Python” and your resume says “C++,” at this step they will likely not consider that, since both programming languages are object oriented, you could probably learn Python quickly if you put in the effort. There has been some debate at this stage on ATS , which is an acronym for applicant tracking system . While companies do use systems such as Workday to manage appli‐ cations, there hasn’t been concrete proof that companies are using them to program‐ matically filter out resumes at this step for each job posting ( Figure 1-10 ). Figure 1-10. Gergely Orosz (founder of The Pragmatic Engineer publication and former manager at Uber) on ATS (screenshot via Twitter). In practice, recruiters use them at most to filter resumes into a selection to fulfill existing criteria on the job posting, such as those mentioned on the previous page. I also haven’t seen ATS automatically filter out qualified candidates during full-time work experience, and once I was responsible for manually reading a PDF containing more than 50 resumes. However, I don’t want to claim that automated ATS rejections are fully untrue. To be safe, in this book I assume both standpoints have some truth to it. So even if ATS is an issue, the steps in this book will help you, since I’ll teach you the principles for how resume selection is conducted. (You can read more on ATS on thetechresume.com .) If you’re able to describe your experiences at a level that HR recruiters can under‐ stand is relevant to the job posting, you will increase your chances at the resume stand is relevant to the job posting, you will increase your chances at the resumescreening step. HR and recruiters, by nature of the role, are aware of higher-level technologies and what’s popular with the roles they’re hiring for but not the details, so it’s important for your resume to be optimized well. (Read more on how to opti‐ mize your resume in Chapter 2 .) Now that I’ve walked through cold applications directly via job boards or websites without any referral, I’ll provide some examples of how referrals can help you fast without any referral, I’ll provide some examples of how referrals can help you fasttrack the process. Let’s say you’re interested in an ML job at ARI Corporation. 19 You know an alum of your university who works on the ML team. You catch up with them and express your interest in the job. During the chat, you show the alum some of your personal ML projects, which are relevant to the ML job you’re interested in. The school alum agrees to refer you and gives you instructions for how to be referred, something that depends on the way the HR system of the company is set up. Since this alum knows you and is willing to vouch for your skills after seeing your personal projects, you get your resume to the “top of the pile.” Depending on the strength of the referral/recommendation, you may skip the resume screening alto‐ gether and get a highly guaranteed callback from a recruiter or even bypass the recruiter directly and get to the rest of the interview rounds. This is illustrated in Figure 1-11 . Note that I say “highly” guaranteed here since it still depends on various factors such as timing. As an example: maybe you got referred, but the job posting has coincidentally just been filled. Thus, you didn’t get to the rest of the interview. I will cover more on referrals and how to get them via professional networking in Chapter 2 . Figure 1-11. The interview process can be shortcut with a strong referral. You’ve been invited to an interview! How do you perform your best? Maybe time is limited; what do you do to ensure that you can maximize your outcome? My personal tactic is to first narrow down the types of questions that might be asked. For example, in the first round of an Amazon interview, the recruiter has outlined the format and it will focus on statistical theory questions. I will read online resources, skim over my notes, and see what topics I’m the weakest on. I will focus less on the questions that I know I can answer confidently and more on those that seem more likely to be asked but that I don’t know well. As to how I “guess” what will likely be asked, that is based mostly on conversations with the recruiter and my follow-up questions to the recruiter or hiring manager. I’m not super accurate at guessing, and this is similar to trying to guess what will come up in a university exam—it could work well, or it could backfire! Either way, there’s the trade-off between knowing a subset of questions well or know‐ ing all questions roughly but not as well (depth vs. breadth). When reviewing my preparation notes, I personally go for breadth, but your results may vary depending on how well you know the material already. Depending on your location and your interviewers’ location, there may be time zone differences. I try to find the time when I have the most energy possible. Sometimes the available interview time slots aren’t ideal, so I choose the lesser of the evils (for example, interviewing from GMT+8 and talking to someone in GMT-4 while travel‐ ing abroad). To make it easy to figure out time zones for candidates invited to an interview, it’s common for HR-scheduling software to have a calendar feature where you can input your preferred times and it will account for your local time zones. However, sometimes the time will be set via back-and-forth emails, and tools such as Cal‐ endly or Cal.com can help. As both an interviewer and interviewee, I am wary of scheduling right at the begin‐ ning of a workday. This is so that I have more time to prepare after I wake up. But of course, if no other time slots are available, then I will select the early time. As an interviewer, I’ve seen countless candidates’ interviews start late because of con‐ nection issues or using a new web-conferencing software—for example, not being able to set up Zoom on time because they hadn’t used it before. As a candidate, I’ve been tripped up and wasted time when needing to use Microsoft Teams because on my personal computers I only had Zoom and Google Meet. In the end, I used the browser version, but there was an issue with my login since my Microsoft student account had expired. We finally got it sorted out, a few minutes later. This could have been avoided if I had tried to sign in a bit earlier or on the day before the interview. Here are some tips to help your interview go more smoothly: Try your best to be in a quiet environment. Some software, such as Zoom, has pretty good built-in noise canceling, as do some wireless headphones. Check your audio and video beforehand. Video-wise, make sure the lighting is good and your camera lens is clean. Sound Video-wise, make sure the lighting is good and your camera lens is clean. Soundwise, make sure your mic sounds clear. On Windows and Mac, there are built-in camera and voice recording apps that I use. You could also start a new Zoom, Google Meet, or Teams session and run a test. Keep a mental list of backup options. Did the internet at your home suddenly go down before the interview? Is there a nearby cafe with (preferably secure) internet that you could go to? Can you use your phone data? Are there dial-in options via phone on the calendar invite? Knowing these things beforehand can help you a lot. I’ve had to dial in once to an interview, and thankfully, I knew that I had the option to. Congrats, your resume has made it past the resume screening! Now let’s go through an example to illustrate what might happen next. Let’s imagine that there were 200 applicants for the role. The recruiter has gone through them and removed 170 that either lacked relevant experience or for some reason didn’t seem to fit the role. Recall that this is based on the impression your resume gave the recruiter; it’s possible that with the same job title and same recruiter team, an improved resume would have passed. If you had a good referral, your resume might have already moved forward. Now that there are 30 applicants, the recruiter will call each of them; this is usually a shorter interview, 15 to 30 minutes long. We refer to this as the “recruiter screening” or “recruiter call.” Generally, the recruiter wants to see what you’re like as a person and if you’re easy to work with. If someone blatantly claims to have experience that they don’t, the call could reveal fabricated work or school experiences. There are other logistical issues to screen for, such as location, salary expectations, and legal status. The recruiter screening is more of a “smell test” instead of an in The recruiter screening is more of a “smell test” instead of an indepth test of your technical skills and experience. My tip for success is to optimize for one thing: that the recruiter understands that you are a good candidate, that your experience is relevant (or you can learn fast), and that you can fit well into the team and role they are hiring for. This is different from con‐ vincing a hiring manager of the same things, or an interview panel of senior MLEs. Instead, you will succeed here if you make the additional effort to connect your resume to the job description on this call. Here’s an example of some bullet points in a job description: • “The candidate has experience with recommender systems.” • “Experience with data processing such as Spark, Snowflake, or Hadoop.” • “The candidate has experience with Python.” A bad example of explaining your experience on the recruiter call for this job is: “For that past project, I used the ALS algorithm, which was implemented with PySpark.” A better example of explaining your experience on the recruiter call for this job is: “For that past project, I used the alternating least squares (ALS) algorithm, which is a recommender systems algorithm based off of matrix factorization, and I used PySpark, which is Spark that’s wrapped with a Python API.” Note that the italicized phrases also appear in the job description. The better example allows a recruiter to match up more of your skills to the job description, whereas the bad example doesn’t match up to the posted skills in an obvious manner. When you’re writing your resume, you have limited space; the real obvious manner. When you’re writing your resume, you have limited space; the realtime conversation of an interview is a chance for you to fill in gaps that the recruiter may not have noticed. It’s also important to expand on acronyms. This is true for interviews conducted with technical people too. I’m relatively specialized in recommender systems and rein‐ forcement learning, but I don’t work with computer vision tasks in my day-to-day work. I appreciated it when a candidate I interviewed was talking about computer vision projects and generally explained the more niche techniques. You can (and should) do this in a way that’s not condescending to your interviewer, whether they are a recruiter or part of your future team. The recruiter call is a good time for you as the candidate to assess the job as well. You can ask questions that you care about, to see if you should continue to interview. For example, I might ask about the team size and if this job focuses more on ML or data analyst responsibilities. You can also prepare some questions about the company and their products. For example, is the team’s current project focused on improving the click-through rate or long-term engagement? If you’re a user of the product, you might have a lot of ideas and questions to discuss. This is also a chance to show your enthusiasm and knowledge of the company. On to the next step. Good news: the recruiter cleared you! You explained your previ‐ ous experience well, and the recruiter was able to understand your past work and how it connects to the job description they have on hand. But it’s not over yet. You’re among 15 other candidates who succeeded at the first recruiter screening. The recruiter informs you of upcoming technical interviews which include ML theory, programming, and a case study interview. There are also behavioral interviews sprinkled throughout. If you pass those, you’ll make it to the on-site interview, which is often the final round. These days, there are also virtual on on-site interview, which is often the final round. These days, there are also virtual onsites/final rounds. If you pass the final round, you’ll be extended an offer. Let’s break down the various types of interviews that take place after the recruiter screening, the first being technical interviews. Technical interviews are typically con‐ ducted with technical individual contributors (ICs), such as an MLE or a data scientist. There may be multiple rounds of technical interviews; there could be one that is a data-focused coding round or one in which the interviewer presents some fictitious example data and asks you to use SQL or Python pandas/NumPy (sometimes there are multiple questions, and you use various programming tools throughout the inter‐ view). I’ll expand more on this type of interview structure and interview questions in Chapter 5 . Apart from ML and data-focused programming interviews, you might be asked brainteaser-type questions. For this type of interview, you might use an interview platform such as CoderPad or HackerRank, where the interviewer presents you with a question and you code in the online integrated development environment (IDE) that both you and your interviewer can see in real time. Sometimes you’ll get other formats, such as technical deep dives, systems design, take-home exercises in a pri‐ vate repo or Google Colab, and so on. I’ll elaborate on how to prepare for these types of interviews in Chapters 5 and 6 . These subsequent interview rounds could further reduce the number of candidates before the final round. In our example, fifteen candidates passed the recruiter screen, and eight passed the first round of technical interviews. After the second round of technical interviews, we’re left with three candidates who will proceed to the on-site interview. Interspersed during the interview process are questions meant to assess how you react in certain situations. The intent often is to use past experience to predict future performance and understand how you react to high-stress or difficult situations. In addition, these questions assess your soft skills, such as communication and team‐ work skills. You’ll want to prepare a few past experiences and relay them in a story‐ telling fashion. For example, during your first recruiter call, the recruiter might ask about a time when you dealt with a difficult timeline on a project. Once you’ve responded you won’t be out of the woods yet. During the on-site, an hour is often dedicated to behavioral questions. And in some technical interviews, you might be asked a couple of questions that are a mix between a purely technical question and a behavioral question. I’ll help you succeed with behavioral interviews in Chapter 7 , which also has tips on company-specific preparation, such as Amazon’s Leadership Principles . For many companies there is an “on-site” final round or the virtual equivalent. These are usually back-to-back interviews. For example, starting in the morning, you might meet with a technical director for a case study interview and then a senior data scien‐ tist for a programming interview. After a lunch break, you might meet with two data scientists who ask about ML theory, and then the hiring manager asks more behavio‐ ral questions and probes about your past experiences. In addition to technical inter‐ viewers, you may speak with a stakeholder (e.g., a product manager on an adjacent team that the team you’re interviewing for works closely with). In several final-round interviews I’ve been through, there was a product manager interviewer or someone from another department that the ML team worked closely with, such as marketing or advertising. Some companies will have an additional mini round after this, such as a quick chat with a skip level (your manager’s manager).In this chapter, you’ve learned about various ML roles, the ML lifecycle, and the dif‐ ferent responsibilities that map onto the ML lifecycle. You’ve also seen how you make your way from the beginning of the process to the final round of interviews. There’s a lot to prepare for and to learn about, but now you have an overview and hopefully some thoughts on how you can target your preparations. Now that this chapter has set the foundation, I’ll walk through a detailed job applica‐ tion guide, including a resume guide, to help you greatly increase your chances of getting interviews.'}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_raw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "for chapter in book_raw:\n",
    "    chapter_name = chapter['chapter']\n",
    "    title = chapter['title']\n",
    "\n",
    "    for doc in chapter['content']:\n",
    "        new_doc = {\n",
    "            'chapter': chapter_name,\n",
    "            'title': title,\n",
    "            'section': doc['section'],\n",
    "            'text': doc['text']\n",
    "        }\n",
    "        documents.append(new_doc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chapter': 'CHAPTER 1',\n",
       " 'title': 'Machine Learning Roles and the Interview Process',\n",
       " 'section': 'Overview of This Book',\n",
       " 'text': 'In the first part of this chapter, I’ll walk through the structure of this book. Then, I’ll discuss the various job titles and roles that use ML skills in industry. 1 I’ll also clarify the responsibilities of various job titles, such as data scientist, machine learning engineer, and so on, as this is a common point of confusion for job seekers. These will be illustrated with an ML skills matrix and ML lifecycle that will be referenced throughout the book. The second part of this chapter walks through the interview process, from beginning to end. I’ve mentored candidates who appreciated this overview since online resources often focus on specific pieces of the interview but not how they all connect together and result in an offer. Especially for new graduates 2 and readers coming from different industries, this chapter helps get everyone on the same page as well as clarifies the process. The interconnecting pieces of interviews are complex, with many types of combina‐ tions depending on the ML role you’re aiming for. This overview will help set the stage, so you’ll know what to focus your time on. For example, some online resources focus on knowledge specific to “product data scientists,” but will title the course or article “data scientist interview tips” without differentiating. For a newcomer, it’s hard to tell if that is relevant to your own career interests. After this chapter, you’ll be able to tell what skills are required for each job title, and in Chapter 2 , you’ll be able to parse out that information yourself from job postings and make your resume as relevant to the job title and job posting as possible. This chapter focuses on helping you differentiate among various ML roles, and walks through the entire interview process, as illustrated in Figure 1-1 : • Job applications and resume ( Chapter 2 ) • Technical interviews — Machine learning (Chapters 3 , 4 , and 6 ) — Coding/programming ( Chapter 5 ) • Behavioral interviews ( Chapter 7 ) • Your interview roadmap ( Chapter 8 ) • Post-interview and follow-up ( Chapter 9 ) Figure 1-1. Overview of the chapters and how they tie into the ML interview process. Depending on where you are in your ML interview journey, I encourage you to focus on the chapters and sections that seem relevant to you. I’ve also planned the book to be referenced as you go along; for example, you might iterate on your resume multi‐ ple times and then flip back to Chapter 2 when needed. The same applies to the other chapters. With that overview, let’s continue. The companion site to this book, https://susanshu.substack.com , features bonus content, helper resources, and more.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Elasticsearch connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run on the console\n",
    "\n",
    "sudo docker run -it \\\n",
    "    --rm \\\n",
    "    --name elasticsearch \\\n",
    "    -m 4GB \\\n",
    "    -p 9200:9200 \\\n",
    "    -p 9300:9300 \\\n",
    "    -e \"discovery.type=single-node\" \\\n",
    "    -e \"xpack.security.enabled=false\" \\\n",
    "    docker.elastic.co/elasticsearch/elasticsearch:8.4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'name': 'c61a76b457df', 'cluster_name': 'docker-cluster', 'cluster_uuid': '7a89_kRqR-GfIP4u79eGKQ', 'version': {'number': '8.4.3', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': '42f05b9372a9a4a470db3b52817899b99a76ee73', 'build_date': '2022-10-04T07:17:24.662462378Z', 'build_snapshot': False, 'lucene_version': '9.3.0', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es_client = Elasticsearch('http://localhost:9200') \n",
    "\n",
    "es_client.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create mappings and Index\n",
    "\n",
    "Imagine that you need to create a schema. what do you need? I would say the column names, the table name, the type of data you are going to introduce...\n",
    "\n",
    "The mapping will set this metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"standard_analyzer\": {\n",
    "                \"type\": \"standard\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "    \"properties\": {\n",
    "        \"text\": {\n",
    "            \"type\": \"text\",\n",
    "            \"analyzer\": \"standard_analyzer\"  \n",
    "        },\n",
    "        \"section\": {\n",
    "            \"type\": \"keyword\",\n",
    "        },\n",
    "        \"chapter\": {\n",
    "            \"type\": \"keyword\",\n",
    "        },\n",
    "        \"title\": {\n",
    "            \"type\": \"keyword\",\n",
    "        },\n",
    "        \"text_vector\": {\n",
    "            \"type\": \"dense_vector\",\n",
    "            \"dims\": 768, # got them above\n",
    "            \"index\": True,\n",
    "            \"similarity\": \"cosine\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'ds-interview-questions'})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_name = \"ds-interview-questions\"\n",
    "\n",
    "# it is better to delete the index every time when experimenting\n",
    "es_client.indices.delete(index=index_name, ignore_unavailable=True) \n",
    "es_client.indices.create(index=index_name, body=index_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add documents to the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in documents:\n",
    "    try:\n",
    "        es_client.index(index=index_name, body=doc)\n",
    "    except Exception as e:\n",
    "        print(f\"Error when indexing the document: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term = \"which are the steps of the data science interview process?\"\n",
    "vector_search_term = model.encode(search_term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create search function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_search(query, index=index_name):\n",
    "    \"\"\"\n",
    "    Execute a search query on the specified index.\n",
    "\n",
    "    Parameters:\n",
    "        query (dict): The search query to execute.\n",
    "        index (str): The name of the index to search.\n",
    "\n",
    "    Returns:\n",
    "        None: Prints the search results.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = es_client.search(index=index, body=query)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error during search: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full-text search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_index': 'ds-interview-questions',\n",
       " '_id': 'Alnyn5IBsRH_JjIM5yfP',\n",
       " '_score': 15.321497,\n",
       " '_source': {'chapter': 'CHAPTER 1',\n",
       "  'title': 'Machine Learning Roles and the Interview Process',\n",
       "  'section': 'The Three Pillars of Machine Learning Roles',\n",
       "  'text': 'To set the stage for the rest of the book, I’ll go over what I call the three pillars of ML and data science roles: • Machine learning algorithms and data intuition • Programming and software engineering skills • Execution and communication skills These are the broad categories of skills that you will be evaluated on during ML job interviews. This book focuses a lot on helping you understand these skills and bridge any gaps between your current experiences and skills and those under these three pillars (see Figure 1-6 ). All these skills will be expanded on in the following chapters. Figure 1-6. Three pillars of machine learning jobs. You’re able to understand the underlying workings of ML algorithms and statistics theory and their respective trade-offs—which is essential when you’re faced with an open-ended question in a real-world ML project at work. You’re not just following steps as you would for a school assignment. Having data intuition means that when you’re faced with a new problem, you know how to use data to solve it; and when you encounter new data or data sources, you know how to dive in to evaluate them. You ask yourself, is this data suitable for ML? What types of ML models might it be suitable for? Are there any issues with the data before you can use it for ML? You know what to ask and how to find the answers. In the ML job-interview process, various types of interviews and interview questions are aimed at assessing a candidate’s knowledge and readiness in this pillar, which I’ll cover in Chapters 3 and 4 . While working on a project, you have the programming skills required to deliver, such as manipulating data with Python or using an internal deploy process so that another team can use the results from the ML model. Even if you know the theory well, without the programming or software engineer‐ ing 14 sense, you can’t make ML materialize out of thin air. You need to use code to connect the data with ML algorithms, which are also implemented with code—that is, you must convert theory to practice. Other programming skills in high demand for ML roles are the (software) engineer’s ability to transition from prototype to production—that is, the ML is integrated and released. Some roles are responsible for end-to-end ML: from researching and train‐ ing models to deployment and production. Some ML roles, such as MLOps engi‐ neers, are responsible for building software infrastructure that can handle the demands of processing large amounts of data to send ML responses to users in sec‐ onds or even milliseconds. In the ML job-interview process, various types of interviews and interview questions assess a candidate’s skills in this pillar, which I’ll walk through in Chapters 5 and 6 . You’re able to work with people who aren’t in the same role as you. In ML, we work with software engineers, data engineers, product managers, and many other collea‐ gues. The ability to get things done in a team encompasses a few soft skills such as communication and some project management skills. For example, being unable to communicate with team members is a real blocker 15 for projects and could cause your ML projects to languish or even be deprioritized. Even in cases where you work with only one person (say, your boss), you still need to be able to report on your projects, which requires communication skills. Consequently, in the ML field a highly in-demand skill is being able to communicate technical con‐ cepts with nontechnical stakeholders. You’ll also need some project management skills to keep your tasks on track. We all learn to how manage our to-do lists and calendars during the process of education or self-learning, but it’s more chaotic since now your project calendar depends on oth‐ ers’ calendars and priorities. Even if you have a project and/or product manager to keep the team on track, you still need to manage yourself to some extent. Without soft skills, things don’t get done, full stop. Don’t be that candidate who focu‐ ses only on technical skills but neglects building and demonstrating their soft skills in interviews. I’ll delve into the details of how ML interviews evaluate candidates on this pillar in Chapter 7 . Growing your skills in all three ML pillars is a tall order, and for entry-level roles you’re usually only expected to have a minimum (such as a 3/10) for each pillar, as illustrated in Figure 1-7 . For example, a job candidate who has some exposure to pro‐ gramming, even if they aren’t skilled or experienced, can be taught to improve. Ideally, you’d be stronger on at least one pillar (such as 5/10 for programming) that is most related to the particular ML role in order to stand out from other job candidates. Figure 1-7. Minimum required skill levels for ML jobs (example). For senior roles, the bare minimum requirements are higher, but a similar rule of thumb applies: clear the minimum skill requirements. From then on, you’ll be com‐ pared with other candidates on the skills that you are great in, depending on the role. Data scientists who only train ML models but don’t deploy them might not need to develop their programming skills as much as their ML theory and communication skills. For entry level roles, I’d argue that the communication pillar has a lower requirement (but not 0/10, please!) because it takes the hard-earned experience of working with a larger group of people, including nontechnical teammates, to raise it higher. This also gives some candidates an edge in this pillar: for those with a nontraditional back‐ ground, such as candidates who are self-taught or switching from software engineer roles or another field, the ability to adeptly tell a story and showcase a portfolio can set them apart from other candidates. Now that you’ve had an overview of the three pillars, you can use this mental model to stand out.'}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_text_query = {\n",
    "    \"size\": 5,  \n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"must\": [\n",
    "                {\n",
    "                    \"match\": {\n",
    "                        \"text\": {\n",
    "                            \"query\": search_term,\n",
    "                            \"boost\": 1.0  \n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"multi_match\": {\n",
    "                        \"query\": search_term,\n",
    "                        \"fields\": [\"text^2\", \"section\", \"title\"],  \n",
    "                        \"type\": \"best_fields\"\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "            \"should\": [\n",
    "                {\n",
    "                    \"terms\": {\n",
    "                        \"title\": [\"technical\", \"behavioral\"],\n",
    "                          \n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "full_text_results = execute_search(full_text_query)\n",
    "full_text_results['hits']['hits'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the dense vector using the pre-trained model\n",
    "\n",
    "A dense vector typically represents a word, sentence, or document as a fixed-length array of numbers, also known as an embedding. Dense vectors are crucial for Elasticsearch, when we want to perform tasks where understanding the meaning behind the words is more important than just matching exact terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "operations = []\n",
    "for doc in documents:\n",
    "    doc[\"text_vector\"] = model.encode(doc[\"text\"]).tolist()\n",
    "    operations.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chapter': 'CHAPTER 1',\n",
       " 'title': 'Machine Learning Roles and the Interview Process',\n",
       " 'section': 'A Brief History of Machine Learning and Data Science Job Titles',\n",
       " 'text': 'First, let’s walk through a brief history of job titles. I decided to start with this section to dispel some myths about the “data scientist” job title and shed some light on why there are so many ML-related job titles. After understanding this history, you should be more aware of what job titles to aim for yourself. If you’ve ever been confused about the litany of titles such as machine learning engineer (MLE), product data sci‐ entist, MLOps engineer, and more, this section is for you. ML techniques aren’t a new thing; in 1985, David Ackley, Geoffrey E. Hinton, and Terrence J. Sejnowski popularized the Boltzmann Machine algorithm. 3 Even before that, regression techniques 4 had early developments in the 1800s. There have long been jobs and roles that use modeling techniques to forecast and predict. Econome‐ tricians, statisticians, financial modelers, physics modelers, and biochemical modelers have existed as professions for decades. The main difference is that there were much smaller datasets compared to the modern day (barring simulations). It was only in recent years, just before the 21st century, when compute power started to increase exponentially. In addition, advances in distributed and parallel computing created a cycle in which “big data” became more readily available. This allowed practitioners to apply that advanced compute power to millions or billions of data points. Larger datasets started being accumulated and distributed for ML research, such as WordNet, 5 and, subsequently, ImageNet, 6 a project led by Fei-Fei Li. These collective efforts laid the foundation for even more ML breakthroughs. AlexNet 7 was released in 2012, achieving high accuracy in the ImageNet challenge, 8 which demonstrated that deep learning can be adept at humanlike tasks at a scale that had not been seen before. Many ML practitioners see this as a time when machine learning, deep learning, and related topics increased by leaps and bounds in terms of recognition from the broader population, not just the AI community. The recent popularity of generative AI (such as ChatGPT) in 2022 and 2023 didn’t come out of nowhere, nor did the deepfakes, self-driving cars, chess bots, and more that came before it; these applica‐ tions were the results of many advances over recent years. “Data scientist” as a job title began as an umbrella term, when the ML and data fields were less mature. The term “data scientist” on Google Trends , which measures the popularity of search terms, surged in 2012. That was the year when that article was published by Harvard Business Review: “Data Scientist: The Sexiest Job of the 21st Century.” 9 By April 2013, the search popularity of “data scientist” was already tied with “statistician” and subsequently surpassed it by magnitudes, as shown in Figure 1-2 . Back in those days, there wasn’t a narrow divide between infrastructure jobs and model training, though. For example, Kubernetes was first released in 2014, but companies have taken some time to adopt it for orchestrating ML jobs. So now there are more specific job titles for ML infrastructure that didn’t exist before. Figure 1-2. Search popularity for the terms “data scientist,” “machine learning engi‐ neer,” and “statistician” on Google Trends (retrieved August 9, 2023). As social media, web recommender systems, and other modern use cases increased, companies started gathering much more granular data, such as clickstream data , which is data collected as a user browses a website or app. Another recent advance‐ ment is an average corporation being able to store the sheer amount of telemetry from machines and Internet of Things (IoT) devices. Previously, data scientists may have worked with data that was updated weekly or daily. Now, as many applications update more frequently or in real time, more infrastructure is needed to serve ML functionality in web products and apps, so more jobs have been created around those functions as well. In short: as the machine learning lifecycle grew more complex, more job titles were created to describe the new skills that a full ML team now requires. I’ll elaborate more on the job titles and ML lifecycle later in this chapter. All of this happened within the last decade, and companies don’t always change their job titles to reflect how the roles have become more specialized. Regardless, as a can‐ didate, knowing this history can help reduce confusion and frustration from applying for a job and finding the role is different from another company’s job with the exact same title. See Table 1-1 for previous trends in ML-related job titles and Table 1-2 for current trends in ML job titles. Table 1-1. Previous trends of ML and data job titles Table 1-2. Current trends of ML and data job titles With that history to explain why you will encounter different job titles, I’ll elaborate on each of these job titles and their responsibilities.',\n",
       " 'text_vector': [0.06606891006231308,\n",
       "  0.04531856253743172,\n",
       "  -0.07584855705499649,\n",
       "  -0.015627508983016014,\n",
       "  -0.00380276283249259,\n",
       "  0.028007833287119865,\n",
       "  0.0754760354757309,\n",
       "  -0.018123220652341843,\n",
       "  0.008582300506532192,\n",
       "  0.030267365276813507,\n",
       "  0.053311970084905624,\n",
       "  0.04067913070321083,\n",
       "  -0.0052957115694880486,\n",
       "  0.03953951597213745,\n",
       "  0.03824090212583542,\n",
       "  -0.06374721229076385,\n",
       "  0.003309109713882208,\n",
       "  0.013971514068543911,\n",
       "  -0.0002465914294589311,\n",
       "  -0.010918994434177876,\n",
       "  -0.03460480645298958,\n",
       "  -0.007678858004510403,\n",
       "  0.009301344864070415,\n",
       "  0.07720332592725754,\n",
       "  -0.0413946695625782,\n",
       "  -0.0010991183808073401,\n",
       "  0.029970597475767136,\n",
       "  -0.013293800875544548,\n",
       "  0.0029220327269285917,\n",
       "  0.03814472630620003,\n",
       "  0.022476427257061005,\n",
       "  -0.042439717799425125,\n",
       "  0.021363502368330956,\n",
       "  0.03257731720805168,\n",
       "  2.079729256365681e-06,\n",
       "  -0.0034328100737184286,\n",
       "  0.027217580005526543,\n",
       "  0.044020239263772964,\n",
       "  -0.03004377707839012,\n",
       "  -0.09820341318845749,\n",
       "  0.03330150246620178,\n",
       "  -0.06206883490085602,\n",
       "  0.0117797227576375,\n",
       "  0.026507128030061722,\n",
       "  -0.018568409606814384,\n",
       "  0.007624240592122078,\n",
       "  0.1042281910777092,\n",
       "  -0.028022192418575287,\n",
       "  -0.025550968945026398,\n",
       "  -0.01998484879732132,\n",
       "  0.01871972344815731,\n",
       "  -0.06893045455217361,\n",
       "  0.05362166836857796,\n",
       "  -0.00026781688211485744,\n",
       "  0.0027046920731663704,\n",
       "  0.01537004392594099,\n",
       "  0.018221532925963402,\n",
       "  0.06257844716310501,\n",
       "  0.01126670092344284,\n",
       "  -0.0395016111433506,\n",
       "  -0.005242536310106516,\n",
       "  0.07580775022506714,\n",
       "  0.018005216494202614,\n",
       "  -0.0005650149541907012,\n",
       "  0.01900913193821907,\n",
       "  0.05989791080355644,\n",
       "  0.0017558903200551867,\n",
       "  -0.009293802082538605,\n",
       "  -0.021128876134753227,\n",
       "  -0.00982658937573433,\n",
       "  -0.003831487149000168,\n",
       "  -0.033346984535455704,\n",
       "  -0.025827178731560707,\n",
       "  -0.021122025325894356,\n",
       "  0.02961868979036808,\n",
       "  0.04232271760702133,\n",
       "  -0.03837832435965538,\n",
       "  -0.0046745711006224155,\n",
       "  0.007481447421014309,\n",
       "  -0.03835546225309372,\n",
       "  0.01964222826063633,\n",
       "  0.02881629578769207,\n",
       "  0.015349695459008217,\n",
       "  0.03103875368833542,\n",
       "  -0.03303079679608345,\n",
       "  0.047387149184942245,\n",
       "  -0.04087924212217331,\n",
       "  -0.023438507691025734,\n",
       "  0.07196937501430511,\n",
       "  0.02605152316391468,\n",
       "  0.049340229481458664,\n",
       "  -0.0011889338493347168,\n",
       "  0.028738314285874367,\n",
       "  0.009924503974616528,\n",
       "  -0.011536595411598682,\n",
       "  0.013317717239260674,\n",
       "  -0.0069163525477051735,\n",
       "  -0.030664220452308655,\n",
       "  -0.03107013739645481,\n",
       "  -0.03370391204953194,\n",
       "  0.0018016074318438768,\n",
       "  -0.014760669320821762,\n",
       "  0.040581174194812775,\n",
       "  0.011700952425599098,\n",
       "  -0.014479339122772217,\n",
       "  0.020870819687843323,\n",
       "  0.0002638025034684688,\n",
       "  -0.00817932840436697,\n",
       "  0.0025816624984145164,\n",
       "  -0.04894799739122391,\n",
       "  -0.04194409400224686,\n",
       "  -0.041730381548404694,\n",
       "  0.011076243594288826,\n",
       "  -0.031705331057310104,\n",
       "  0.01189903263002634,\n",
       "  0.027185821905732155,\n",
       "  0.00047855201410129666,\n",
       "  0.03057236783206463,\n",
       "  -0.03890026733279228,\n",
       "  -0.046381451189517975,\n",
       "  -0.05449884757399559,\n",
       "  0.04013647511601448,\n",
       "  0.032110102474689484,\n",
       "  0.053272999823093414,\n",
       "  -0.04501698911190033,\n",
       "  -0.04790934920310974,\n",
       "  -0.005251242779195309,\n",
       "  0.03849092498421669,\n",
       "  0.04074351117014885,\n",
       "  -0.006842250470072031,\n",
       "  0.024030065163969994,\n",
       "  -0.01352503802627325,\n",
       "  -0.0047929114662110806,\n",
       "  0.004801218397915363,\n",
       "  0.07695091515779495,\n",
       "  0.03469907492399216,\n",
       "  0.0024456805549561977,\n",
       "  0.04805701598525047,\n",
       "  -0.005186579190194607,\n",
       "  -0.033008210361003876,\n",
       "  -0.007145306095480919,\n",
       "  -0.03687674179673195,\n",
       "  0.0069240303710103035,\n",
       "  0.01807449385523796,\n",
       "  0.006329546682536602,\n",
       "  0.017354682087898254,\n",
       "  -0.04495503753423691,\n",
       "  -0.0035321894101798534,\n",
       "  -0.006921124178916216,\n",
       "  0.0018134437268599868,\n",
       "  -0.0738353282213211,\n",
       "  0.020462598651647568,\n",
       "  -0.029939092695713043,\n",
       "  -0.008973566815257072,\n",
       "  0.07127561420202255,\n",
       "  0.004359736107289791,\n",
       "  -0.014378378167748451,\n",
       "  0.027597876265645027,\n",
       "  0.03494704142212868,\n",
       "  0.02766389772295952,\n",
       "  -0.029038937762379646,\n",
       "  0.006079044658690691,\n",
       "  0.07711493968963623,\n",
       "  0.03673067316412926,\n",
       "  -0.013300592079758644,\n",
       "  -0.03998706117272377,\n",
       "  -0.027641819790005684,\n",
       "  0.005531721282750368,\n",
       "  -0.05451449751853943,\n",
       "  0.05231975018978119,\n",
       "  -0.017225516960024834,\n",
       "  -0.01773797534406185,\n",
       "  0.003748881397768855,\n",
       "  0.028564603999257088,\n",
       "  0.021129801869392395,\n",
       "  0.009148807264864445,\n",
       "  0.02449561096727848,\n",
       "  0.08051370084285736,\n",
       "  0.03769601136445999,\n",
       "  -0.008803902193903923,\n",
       "  0.02850445546209812,\n",
       "  -0.04532962664961815,\n",
       "  0.017087947577238083,\n",
       "  0.022892268374562263,\n",
       "  -0.045863185077905655,\n",
       "  -0.011087359860539436,\n",
       "  -0.07216333597898483,\n",
       "  -0.038531720638275146,\n",
       "  -0.013985269702970982,\n",
       "  -0.023124217987060547,\n",
       "  0.0766829401254654,\n",
       "  0.01453989278525114,\n",
       "  -0.019872210919857025,\n",
       "  0.020869798958301544,\n",
       "  0.03782374784350395,\n",
       "  -0.03671935945749283,\n",
       "  -0.050103649497032166,\n",
       "  -0.0010222081327810884,\n",
       "  -0.001558128627948463,\n",
       "  0.04118221625685692,\n",
       "  -0.05239838734269142,\n",
       "  -0.03697893023490906,\n",
       "  -0.06598374247550964,\n",
       "  0.029649225994944572,\n",
       "  0.034157101064920425,\n",
       "  -0.02445690892636776,\n",
       "  -0.06481662392616272,\n",
       "  -0.04938056319952011,\n",
       "  0.002515654545277357,\n",
       "  -0.01163905393332243,\n",
       "  -0.012934372760355473,\n",
       "  0.008459903299808502,\n",
       "  -0.050200898200273514,\n",
       "  0.03042571432888508,\n",
       "  -0.02022688090801239,\n",
       "  -0.005065116565674543,\n",
       "  0.006720611825585365,\n",
       "  -0.0393737368285656,\n",
       "  0.014697501435875893,\n",
       "  0.00017506499716546386,\n",
       "  -0.001132841338403523,\n",
       "  -0.002252863720059395,\n",
       "  -0.09398359805345535,\n",
       "  -0.016102958470582962,\n",
       "  -0.0696796178817749,\n",
       "  0.0237945057451725,\n",
       "  0.05232834443449974,\n",
       "  0.004358089063316584,\n",
       "  0.010724320076406002,\n",
       "  0.05368935316801071,\n",
       "  -0.002584524219855666,\n",
       "  -0.004418307915329933,\n",
       "  -0.006053366232663393,\n",
       "  -0.012549171224236488,\n",
       "  -0.01523069478571415,\n",
       "  -0.01238216832280159,\n",
       "  -0.011928270570933819,\n",
       "  0.03676788881421089,\n",
       "  -0.0376051664352417,\n",
       "  -0.009780357591807842,\n",
       "  -0.0011541753774508834,\n",
       "  0.03440144285559654,\n",
       "  -0.03515739366412163,\n",
       "  -0.017253302037715912,\n",
       "  0.002684232546016574,\n",
       "  -0.013121460564434528,\n",
       "  -0.008725123479962349,\n",
       "  0.012661011889576912,\n",
       "  -0.025318561121821404,\n",
       "  -0.057607922703027725,\n",
       "  0.05355115607380867,\n",
       "  -0.0400671549141407,\n",
       "  -0.028454983606934547,\n",
       "  0.03415612876415253,\n",
       "  -0.012389509938657284,\n",
       "  0.0042282165959477425,\n",
       "  0.007561819162219763,\n",
       "  -0.003925757948309183,\n",
       "  -0.011161931790411472,\n",
       "  -0.01257438212633133,\n",
       "  0.004901573061943054,\n",
       "  -0.010814359411597252,\n",
       "  0.017711933702230453,\n",
       "  0.020955758169293404,\n",
       "  -0.040588606148958206,\n",
       "  0.010667256079614162,\n",
       "  0.01803452894091606,\n",
       "  0.0024050564970821142,\n",
       "  -0.03758038580417633,\n",
       "  0.00028565613320097327,\n",
       "  0.003009356325492263,\n",
       "  0.0007151829195208848,\n",
       "  -0.04722214862704277,\n",
       "  -0.027198662981390953,\n",
       "  0.0008122437866404653,\n",
       "  0.008109611459076405,\n",
       "  0.022873103618621826,\n",
       "  -0.0020602906588464975,\n",
       "  -0.040730416774749756,\n",
       "  0.04841279610991478,\n",
       "  0.019382940605282784,\n",
       "  -0.04489132761955261,\n",
       "  -0.004527580924332142,\n",
       "  -0.04680744186043739,\n",
       "  -0.03085365891456604,\n",
       "  0.031186342239379883,\n",
       "  0.0713123083114624,\n",
       "  -0.01749725081026554,\n",
       "  0.01957760937511921,\n",
       "  -0.03963230550289154,\n",
       "  -0.036592934280633926,\n",
       "  0.01598355919122696,\n",
       "  -0.030292702838778496,\n",
       "  0.024436619132757187,\n",
       "  0.0023214814718812704,\n",
       "  0.02914116345345974,\n",
       "  -0.007625013589859009,\n",
       "  0.012297776527702808,\n",
       "  -0.009883164428174496,\n",
       "  0.022692741826176643,\n",
       "  -0.03238226845860481,\n",
       "  0.010917801409959793,\n",
       "  0.0478123240172863,\n",
       "  0.042903609573841095,\n",
       "  -0.024091273546218872,\n",
       "  -0.002253559185191989,\n",
       "  0.03988434374332428,\n",
       "  0.045700594782829285,\n",
       "  -0.021282298490405083,\n",
       "  -0.05791624262928963,\n",
       "  -0.005728038959205151,\n",
       "  -0.03647732362151146,\n",
       "  0.04149213060736656,\n",
       "  0.10073183476924896,\n",
       "  0.020527750253677368,\n",
       "  -0.08045247197151184,\n",
       "  -0.04772065579891205,\n",
       "  0.01932142861187458,\n",
       "  -0.007101878989487886,\n",
       "  0.030127782374620438,\n",
       "  -0.024174990132451057,\n",
       "  -0.039210692048072815,\n",
       "  0.0008273554849438369,\n",
       "  0.027834270149469376,\n",
       "  0.00034315374796278775,\n",
       "  -0.024012722074985504,\n",
       "  -0.09624817967414856,\n",
       "  0.0013241141568869352,\n",
       "  0.03476903960108757,\n",
       "  -0.013779825530946255,\n",
       "  0.010820349678397179,\n",
       "  -0.0029222164303064346,\n",
       "  -0.012951036915183067,\n",
       "  -0.01282590813934803,\n",
       "  0.01177946850657463,\n",
       "  -0.05410123988986015,\n",
       "  -0.00535534368827939,\n",
       "  -0.01906205341219902,\n",
       "  -0.02466508187353611,\n",
       "  -0.004356122575700283,\n",
       "  0.006209157407283783,\n",
       "  0.017151547595858574,\n",
       "  -0.01954115554690361,\n",
       "  -0.05501613765954971,\n",
       "  -0.033230748027563095,\n",
       "  0.006833230145275593,\n",
       "  0.007417644374072552,\n",
       "  -0.01304837130010128,\n",
       "  -0.04773737117648125,\n",
       "  0.004429795779287815,\n",
       "  0.06166398525238037,\n",
       "  -0.00835472159087658,\n",
       "  0.005322097800672054,\n",
       "  -0.008785232901573181,\n",
       "  -0.00791714433580637,\n",
       "  -0.05100962147116661,\n",
       "  0.025196321308612823,\n",
       "  0.03646881878376007,\n",
       "  0.031153438612818718,\n",
       "  0.03680706024169922,\n",
       "  0.017008377239108086,\n",
       "  -0.049645163118839264,\n",
       "  -0.026324093341827393,\n",
       "  -0.02027708850800991,\n",
       "  -0.10212179273366928,\n",
       "  0.00787843856960535,\n",
       "  0.0002377682103542611,\n",
       "  -0.023459874093532562,\n",
       "  0.005685360170900822,\n",
       "  0.019479062408208847,\n",
       "  -0.03959747031331062,\n",
       "  -0.05073043331503868,\n",
       "  0.060207799077034,\n",
       "  0.04221693053841591,\n",
       "  -0.04446297138929367,\n",
       "  0.0145115302875638,\n",
       "  -0.01644154079258442,\n",
       "  0.04055437818169594,\n",
       "  -0.0269129890948534,\n",
       "  0.012730925343930721,\n",
       "  0.04868648573756218,\n",
       "  -0.05640551447868347,\n",
       "  0.008165411651134491,\n",
       "  0.023618215695023537,\n",
       "  0.010857492685317993,\n",
       "  -0.06186135858297348,\n",
       "  -0.05651771277189255,\n",
       "  -0.028259294107556343,\n",
       "  -0.008443672209978104,\n",
       "  0.05899100378155708,\n",
       "  -0.028514690697193146,\n",
       "  -0.0429510734975338,\n",
       "  -0.015106553211808205,\n",
       "  -0.039634548127651215,\n",
       "  -0.01071388553828001,\n",
       "  -0.007325134705752134,\n",
       "  0.02280302532017231,\n",
       "  -0.005779926665127277,\n",
       "  0.04193408042192459,\n",
       "  -0.007463559973984957,\n",
       "  0.0552663579583168,\n",
       "  -0.052493467926979065,\n",
       "  -0.005243164021521807,\n",
       "  0.020683612674474716,\n",
       "  -0.04542155563831329,\n",
       "  0.008331715129315853,\n",
       "  0.09297247231006622,\n",
       "  0.04360513389110565,\n",
       "  -0.09098030626773834,\n",
       "  0.09756780415773392,\n",
       "  -0.02945149689912796,\n",
       "  0.08422981202602386,\n",
       "  0.05244465172290802,\n",
       "  0.031239798292517662,\n",
       "  0.06164417788386345,\n",
       "  -0.015262868255376816,\n",
       "  -0.04119983687996864,\n",
       "  -0.04646046832203865,\n",
       "  0.022454669699072838,\n",
       "  -0.012235848233103752,\n",
       "  0.07515323907136917,\n",
       "  0.03544388711452484,\n",
       "  0.036679137498140335,\n",
       "  -0.03558730334043503,\n",
       "  -0.03826139122247696,\n",
       "  -0.054116927087306976,\n",
       "  -0.006655484903603792,\n",
       "  0.012116723693907261,\n",
       "  0.019443180412054062,\n",
       "  -0.043282344937324524,\n",
       "  -0.030636893585324287,\n",
       "  -0.019596515223383904,\n",
       "  -0.007878200151026249,\n",
       "  0.03868275508284569,\n",
       "  0.04207286611199379,\n",
       "  -0.007899442687630653,\n",
       "  -0.003663739887997508,\n",
       "  -0.04174764081835747,\n",
       "  -0.05212956294417381,\n",
       "  0.0777641162276268,\n",
       "  -0.016771579161286354,\n",
       "  0.06696739792823792,\n",
       "  -0.013741860166192055,\n",
       "  0.03219130262732506,\n",
       "  -0.03206454962491989,\n",
       "  0.04737213999032974,\n",
       "  -0.029888931661844254,\n",
       "  -0.08089249581098557,\n",
       "  -0.033023182302713394,\n",
       "  0.014175611548125744,\n",
       "  0.054730355739593506,\n",
       "  0.019531190395355225,\n",
       "  -0.012452976778149605,\n",
       "  0.07712072879076004,\n",
       "  -0.04848906770348549,\n",
       "  -0.02792203240096569,\n",
       "  -0.05703088268637657,\n",
       "  -0.020144730806350708,\n",
       "  -0.008493867702782154,\n",
       "  -0.022559810429811478,\n",
       "  0.04497664421796799,\n",
       "  0.008116108365356922,\n",
       "  -0.028598355129361153,\n",
       "  0.020461125299334526,\n",
       "  -0.062424518167972565,\n",
       "  0.046411123126745224,\n",
       "  0.11154387891292572,\n",
       "  0.0010989821748808026,\n",
       "  -0.00826630275696516,\n",
       "  -0.007976618595421314,\n",
       "  0.0016863218042999506,\n",
       "  -0.02629786916077137,\n",
       "  -0.026995521038770676,\n",
       "  0.014994101598858833,\n",
       "  0.03660009801387787,\n",
       "  0.013781029731035233,\n",
       "  -0.025223184376955032,\n",
       "  -0.009965931996703148,\n",
       "  0.00458912318572402,\n",
       "  -0.03425789624452591,\n",
       "  -0.0008328360272571445,\n",
       "  0.060735978186130524,\n",
       "  -0.03258571773767471,\n",
       "  -0.03275550901889801,\n",
       "  0.002879028907045722,\n",
       "  -0.027904454618692398,\n",
       "  0.03861277550458908,\n",
       "  0.007317420560866594,\n",
       "  -0.02847658284008503,\n",
       "  -0.054346490651369095,\n",
       "  0.05064050108194351,\n",
       "  0.05081033334136009,\n",
       "  0.015568842180073261,\n",
       "  -0.043860748410224915,\n",
       "  0.06052583083510399,\n",
       "  -0.06325481832027435,\n",
       "  -0.059563327580690384,\n",
       "  0.029067600145936012,\n",
       "  -0.027052899822592735,\n",
       "  0.050313252955675125,\n",
       "  0.018823884427547455,\n",
       "  0.007639425341039896,\n",
       "  -0.012172972783446312,\n",
       "  -0.03006257303059101,\n",
       "  0.04391228035092354,\n",
       "  -0.03157825395464897,\n",
       "  0.039480943232774734,\n",
       "  0.004037121776491404,\n",
       "  0.0026923182886093855,\n",
       "  0.0418529212474823,\n",
       "  -0.03694061562418938,\n",
       "  0.002784724347293377,\n",
       "  -0.061942461878061295,\n",
       "  -0.05743245780467987,\n",
       "  0.015872392803430557,\n",
       "  0.010635687969624996,\n",
       "  0.03938867151737213,\n",
       "  -0.008741693571209908,\n",
       "  -0.031767409294843674,\n",
       "  -0.010295013897120953,\n",
       "  0.002296960912644863,\n",
       "  -0.0021257305052131414,\n",
       "  -0.03231678530573845,\n",
       "  -0.00729492399841547,\n",
       "  0.03506103903055191,\n",
       "  -0.0038116180803626776,\n",
       "  -0.0356922447681427,\n",
       "  -0.03699394688010216,\n",
       "  -0.01977047324180603,\n",
       "  0.003479261649772525,\n",
       "  0.03993493691086769,\n",
       "  0.034821610897779465,\n",
       "  0.01446866150945425,\n",
       "  -0.0465506985783577,\n",
       "  0.03628890588879585,\n",
       "  -0.01677398383617401,\n",
       "  0.03791150450706482,\n",
       "  -0.012815563939511776,\n",
       "  0.05356059595942497,\n",
       "  -0.007672739215195179,\n",
       "  -0.004740254953503609,\n",
       "  0.01181875728070736,\n",
       "  0.0102028027176857,\n",
       "  -0.017471682280302048,\n",
       "  -0.02144976705312729,\n",
       "  -0.07845401763916016,\n",
       "  -0.013074511662125587,\n",
       "  0.04433221369981766,\n",
       "  0.012511640787124634,\n",
       "  0.03956323489546776,\n",
       "  0.005207272246479988,\n",
       "  0.022034581750631332,\n",
       "  -0.01341076847165823,\n",
       "  0.017888829112052917,\n",
       "  0.010471689514815807,\n",
       "  -5.4023564066342544e-33,\n",
       "  0.06273004412651062,\n",
       "  -0.03525228425860405,\n",
       "  0.07765353471040726,\n",
       "  0.061391446739435196,\n",
       "  -0.02384204789996147,\n",
       "  0.014497761614620686,\n",
       "  -0.0176616869866848,\n",
       "  0.028972890228033066,\n",
       "  -0.0482834130525589,\n",
       "  0.009528030641376972,\n",
       "  -0.03902558982372284,\n",
       "  -0.0032943724654614925,\n",
       "  0.005756794940680265,\n",
       "  0.015592397190630436,\n",
       "  0.04055174067616463,\n",
       "  -0.021079260855913162,\n",
       "  0.011296295560896397,\n",
       "  0.0051696146838366985,\n",
       "  0.06545768678188324,\n",
       "  0.023220039904117584,\n",
       "  -0.07087520509958267,\n",
       "  0.01078211423009634,\n",
       "  -0.012268850579857826,\n",
       "  -0.04977979511022568,\n",
       "  0.006346903275698423,\n",
       "  0.024544019252061844,\n",
       "  0.02209760993719101,\n",
       "  -0.03226298838853836,\n",
       "  -0.007752036675810814,\n",
       "  -0.005610586144030094,\n",
       "  0.038247209042310715,\n",
       "  0.07052814215421677,\n",
       "  -0.004235969390720129,\n",
       "  -0.06332997977733612,\n",
       "  -0.06598049402236938,\n",
       "  0.036838412284851074,\n",
       "  -0.050730396062135696,\n",
       "  0.052630193531513214,\n",
       "  -0.02322184108197689,\n",
       "  -0.022501178085803986,\n",
       "  0.018955077975988388,\n",
       "  0.019748743623495102,\n",
       "  0.06697692722082138,\n",
       "  0.04411890730261803,\n",
       "  -0.0765247792005539,\n",
       "  0.0074395383708179,\n",
       "  0.024983147159218788,\n",
       "  -0.02730625681579113,\n",
       "  0.02067146636545658,\n",
       "  -0.04548447206616402,\n",
       "  0.02184279076755047,\n",
       "  0.00266981590539217,\n",
       "  -0.051851674914360046,\n",
       "  0.04077094420790672,\n",
       "  0.03492320701479912,\n",
       "  0.02548070438206196,\n",
       "  -0.017479341477155685,\n",
       "  0.021920453757047653,\n",
       "  -0.06614820659160614,\n",
       "  0.0018970170058310032,\n",
       "  0.04006213694810867,\n",
       "  -0.04857800155878067,\n",
       "  0.07349228858947754,\n",
       "  0.06810744106769562,\n",
       "  0.041173502802848816,\n",
       "  0.04260774701833725,\n",
       "  0.13043127954006195,\n",
       "  0.02215079963207245,\n",
       "  -0.01227223314344883,\n",
       "  0.06180323660373688,\n",
       "  0.02483632043004036,\n",
       "  0.06593676656484604,\n",
       "  0.0071989563293755054,\n",
       "  0.010368443094193935,\n",
       "  0.013322046957910061,\n",
       "  -0.0590553879737854,\n",
       "  -0.03556472063064575,\n",
       "  -0.021196307614445686,\n",
       "  -0.00106766726821661,\n",
       "  -0.06417965888977051,\n",
       "  0.010434252209961414,\n",
       "  0.004031035583466291,\n",
       "  -0.008688749745488167,\n",
       "  -0.08099931478500366,\n",
       "  0.004019645042717457,\n",
       "  -0.0077246930450201035,\n",
       "  0.028778111562132835,\n",
       "  0.025643453001976013,\n",
       "  -0.02500726282596588,\n",
       "  -0.04285277798771858,\n",
       "  0.0018891413928940892,\n",
       "  0.1072169840335846,\n",
       "  0.003145250491797924,\n",
       "  -0.07025501877069473,\n",
       "  0.006670638918876648,\n",
       "  -0.013953391462564468,\n",
       "  -0.021985316649079323,\n",
       "  0.037968479096889496,\n",
       "  0.04393798112869263,\n",
       "  0.020094111561775208,\n",
       "  -0.09405159950256348,\n",
       "  -0.025714024901390076,\n",
       "  -0.07681666314601898,\n",
       "  0.033112142235040665,\n",
       "  0.022592831403017044,\n",
       "  0.03209444135427475,\n",
       "  -0.01951802708208561,\n",
       "  0.01557836215943098,\n",
       "  0.001193723175674677,\n",
       "  -0.022580455988645554,\n",
       "  0.006208548787981272,\n",
       "  0.026494471356272697,\n",
       "  0.010143030434846878,\n",
       "  -0.012842662632465363,\n",
       "  -0.005834595300257206,\n",
       "  -0.028992321342229843,\n",
       "  0.024958712980151176,\n",
       "  -0.024606449529528618,\n",
       "  -0.007937591522932053,\n",
       "  -0.014013553969562054,\n",
       "  0.031215166673064232,\n",
       "  -0.02569190226495266,\n",
       "  -0.014582696370780468,\n",
       "  -0.029290003702044487,\n",
       "  0.003837253199890256,\n",
       "  -0.04034789651632309,\n",
       "  -0.013712234795093536,\n",
       "  -0.045362070202827454,\n",
       "  0.03771521523594856,\n",
       "  -0.046821728348731995,\n",
       "  -0.029563140124082565,\n",
       "  -0.024172931909561157,\n",
       "  2.674981942618615e-07,\n",
       "  0.00598176522180438,\n",
       "  0.05083223059773445,\n",
       "  0.007102744188159704,\n",
       "  0.028811227530241013,\n",
       "  0.01831391453742981,\n",
       "  0.016095614060759544,\n",
       "  0.025160428136587143,\n",
       "  -0.02432354725897312,\n",
       "  0.004398027900606394,\n",
       "  0.030485166236758232,\n",
       "  0.02945248782634735,\n",
       "  -0.011351828463375568,\n",
       "  0.000556918210349977,\n",
       "  -0.06351268291473389,\n",
       "  -0.027407990768551826,\n",
       "  -0.09931714832782745,\n",
       "  -0.028732728213071823,\n",
       "  -0.030628109350800514,\n",
       "  -0.06863461434841156,\n",
       "  0.0421827994287014,\n",
       "  0.025273695588111877,\n",
       "  0.018677765503525734,\n",
       "  0.035644251853227615,\n",
       "  -0.03694610297679901,\n",
       "  0.01975378952920437,\n",
       "  -0.039749741554260254,\n",
       "  -0.01602136343717575,\n",
       "  -0.02045288495719433,\n",
       "  0.07754141837358475,\n",
       "  0.03762979060411453,\n",
       "  0.0281163789331913,\n",
       "  0.06645344942808151,\n",
       "  0.021740468218922615,\n",
       "  0.023830760270357132,\n",
       "  0.020803887397050858,\n",
       "  -0.00778147904202342,\n",
       "  0.01782580278813839,\n",
       "  0.00946755614131689,\n",
       "  0.02644900418817997,\n",
       "  0.04422738403081894,\n",
       "  0.025743382051587105,\n",
       "  0.017160259187221527,\n",
       "  0.01792924292385578,\n",
       "  -0.026933642104268074,\n",
       "  0.026158511638641357,\n",
       "  -0.021689968183636665,\n",
       "  -0.08127816766500473,\n",
       "  -0.035310667008161545,\n",
       "  -0.012588229030370712,\n",
       "  -0.013935275375843048,\n",
       "  -0.0015044432366266847,\n",
       "  -0.040959812700748444,\n",
       "  0.028187310323119164,\n",
       "  -0.04556509107351303,\n",
       "  -0.002020201412960887,\n",
       "  -0.024906707927584648,\n",
       "  -0.0238063745200634,\n",
       "  0.02405397593975067,\n",
       "  0.02506897784769535,\n",
       "  0.0264270082116127,\n",
       "  -0.03306831046938896,\n",
       "  -0.03008778765797615,\n",
       "  0.02162666618824005,\n",
       "  -0.01203906536102295,\n",
       "  0.07563776522874832,\n",
       "  0.009789579547941685,\n",
       "  0.018244445323944092,\n",
       "  2.2403955758075737e-34,\n",
       "  0.0027073740493506193,\n",
       "  0.03626491501927376,\n",
       "  -0.010984865948557854,\n",
       "  -0.015721598640084267,\n",
       "  -0.004851104225963354,\n",
       "  0.007580568082630634,\n",
       "  -0.012230148538947105,\n",
       "  -0.00745063740760088,\n",
       "  -0.013364853337407112,\n",
       "  0.0005683943745680153,\n",
       "  -0.06282488256692886]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "operations[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add documents to the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in operations:\n",
    "    try:\n",
    "        es_client.index(index=index_name, body=doc)\n",
    "    except Exception as e:\n",
    "        print(f\"Error when indexing the document: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_index': 'ds-interview-questions',\n",
       " '_id': 'SVnzn5IBsRH_JjIMNycO',\n",
       " '_score': 0.80450153,\n",
       " '_source': {'section': 'Python Coding Interview: Data- and ML-Related Questions',\n",
       "  'text': 'Now, let’s dive into the first type of programming/coding interview questions: data Now, let’s dive into the first type of programming/coding interview questions: dataand ML-related questions. These questions focus on using Python, such as using the NumPy and pandas libraries or ML libraries like XGBoost, to code up solutions to interview questions. The main difference between this type of question and the brain‐ teaser/LeetCode questions covered in the next subsection is that this type of question will relate more to what you’d be doing in your day-to-day role in an ML job. Depending on the type of company you’re interviewing for, these questions may be themed around the company’s product. For example, a social media company could ask a series of questions on how you’d pull information about new user signups, how you’d extract the answers to how active the users have been, and how many users have churned (left) within the last week. In this section, I’ll walk you through an interview scenario and provide two sample data- and ML-related Python questions that might be asked in the interview. Note that the datasets given in these examples are intentionally small and simple for the purpose of understanding. On interview day, your interviewer sends you a link to a HackerRank interface. When you open it, you see an interface where you can write your code. You double When you open it, you see an interface where you can write your code. You doublecheck with the interviewer that there are two questions in total during this one-hour interview and gauge that you should aim to spend 15 minutes on the first question and 30 minutes on the second one. The rest is buffer time and Q&A with the interviewer. The interviewer copies and pastes the first question at the top of the page, which out‐ lines the question in commented-out code. The coding interview interface may already have the question prepopulated, but in many data interviews I’ve been through at various types of companies, from big tech to startups, the question may not be on the sidebar but instead pasted into the coding area. As an interviewer, I’m speculating that this is more suitable for questions where you don’t have to run the full script as well as for interviews that focus more on the back-and-forth discussion. You double-check that you aren’t expected to run the code fully, as the HackerRank environment doesn’t happen to connect to an actual database. The interviewer con‐ firms this. At [the social media company that you’re interviewing for] , we are looking into user behavior. The data format we have is [sample data in .json format]. The data is pro‐ vided in the following two .json objects (referred to as “tables” for convenience). Table 1 : Table 2 : Table 1 has the new user signup time: • user_id • timestamp Table 2 has current accounts: • user_identifier • login_time • logoff_time Question: Given these two tables, which users’ latest activity is beyond 60 days since signup? Here are some things you should do as you begin to answer this first question: • Confirm what each data type means if unclear: is user_id in Table 1 the same as user_identifier in Table 2? (For the example answer, we assume that it is.) • Load up the .json format into your format of choice—for example, into a pandas DataFrame. • Think out loud—explain your approach and thoughts as you code. Confirm the question if you’re unsure; in this case, even if there are several columns, you don’t need some of them in the end, or you could simplify the results. The following code is an example answer for question 5-1 (a): Read in the .json objects as pandas DataFrames. For each user, get their latest login time, and store in a DataFrame named latest_login_times . Merge the two DataFrames. Now, for each user, it displays their signup time (timestamp) and latest login time (login_time). (Tip: if time allows in the inter‐ view, rename these columns to have clearer names.) Calculate the time between signup timestamp and the latest login_time , putting the results into a new column. Filter the users, keeping only the one(s) that have logged in over 60 days after signing up. For the code samples in this book, sometimes the line breaks aren’t as clean as what a code formatter would output, due to for‐ matting for print books. Now, the interviewer pastes the second question into the HackerRank interface. Suppose you have a new dataset, Table 3. Table 3 : Question: Based on the tables in the previous question as well as this new table (Table 3), how would you approach building a predictive churn model with this toy data, assuming that the model can be run on much more data than this? Assume that the day the analysis is run is July 25, 2023. Create the churn indicators and feature table, and verbally describe how you would proceed with modeling. Here are some tips for answering question 5-1 (b): • You should analyze the data, even if it’s a toy dataset, and walk through what you would do if it were larger. • Define what it means that the user will stay or leave. Does the company have a definition of churned users? (For example, are they users who haven’t logged on in 30 days?) Get clarity on anything that’s unclear to you. • Suggest some possible ways to find correlation—for example, if the user has a lower profile completion rate, are they more likely to churn? Also, outline and code out how you can test and confirm those assumptions in the dataset. • For modeling, what are some low-effort baseline models you could try? Perhaps regression or a simple tree-based model? • For a more complex model, what would you do? • If you notice that your time is running out, tell the interviewer you’ll give a quick high-level overview of how a complex model might work, and wrap it up. For illustration, here’s an example of what some rows of the table could look like after you load in the table form: The following code is the first part of an example answer for question 5-1 (b), which loads Table 3: The interviewer confirmed that if the user hasn’t logged in for 30 days, then they can be considered as churned. Note that we assume the current date is July 25, 2023. You create a binary column that indicates churn. The following code is the second part of the answer for question 5-1 (b), creating a churn indicator: From here you can join it to the features table: The DataFrame merged_df has your churn indicators, and now you join it to the table with features, features_df . Next, you decide on a simple ML model, CatBoost, and proceed to convert this Data‐ Frame to the required format (in this case, it’s easier if the columns are the features). An easy rules-based method could simply be that if a user doesn’t log on for 20 days, they might already be likely to churn (not logged in for 30 days). It’s kind of a simple “wait and see” rules-based approach, but it’s an option. Another option is that they will likely churn if they have not logged in for 14 days and also have added no friends. Our guess is that if they have no friends at that point, then maybe they don’t have an incentive to come back, so it’s possible they’ll churn. As mentioned in Chapter 4 , this could be a place to discuss scaling the data so that the features with larger “numbers” don’t throw off the scale of magnitude. Discuss cases if there are missing values (in this toy example, there aren’t, but there likely are in the real world). In the previous scenario, I walked through how a data- and ML-focused interview might go. I’ve been through many of these, sometimes as the interviewee and some‐ times as the interviewer. Here are some additional observations and tips that I hope will help you. FAQ: It seems like the interviews differ a lot. Should I try to practice on platforms such as HackerRank and CoderPad myself? A: The interview format might differ, but don’t fret about it. Once you do a few, you’ll get more used to the variety. For example, I’ve had interviews where I was expected to run the code correctly, and I’ve had others where best-effort pseudocode is fine. To use the Google interview example, coding in Google Docs, of course you can’t execute the code just like that. But if you aren’t used to the syntax or make a lot of obvious mistakes, the interviewer can still tell, even if you don’t need to run the code. FAQ: Thinking out loud seems distracting. What if I really can’t do it? A: If you know thinking aloud will distract you, then you don’t have to talk as frequently. However, I still recommend that when you’re taking a natural pause, such as when you just finished defining a function, take that time in between to quickly summarize what you did and why. As an interviewer, even if I’m watch‐ ing every single character the candidate is typing, I might have assumptions that are incorrect—I can’t read minds! In summary, try to explain at least during a natural pause. FAQ: I don’t have previous job experience working with data. How can I answer these questions well? A: I find that one big thing that people who work with data think about are the pros and cons of various algorithms. They also think about being more mindful of nuances in data. For example, going back to the interview scenario at the social media company, the interviewer might bring up that a tree-based model could work better on users with longer tenure, but it might not work so well for new users. It’s good when interviewees are able to come up with a solution to the cons—for example, a solution could be to use this model only for users that have more than 30 days of activity. As an example of nuances in data: if a table has a user ID as well as friend connections, but no information about how long the user has been using the social media platform, the numbers might not mean the same. Users who have used a platform longer might naturally have more friend connections. The signif‐ icance of having 10 friends differs between a user who has been signed up for just a month and a user who has a tenure of one year, despite the same friend count. The good news is that even without work experience in data, now that you know these main points, you can gain the same type of thinking via side projects. It sounds like a lot of requirements, but as a new grad, I was able to answer ques‐ tions like this based on what I learned from school assignments in econometrics. As long as you’ve had some hands-on experience (which you should during your interview prep and in building your project portfolio), you can do it. Here are some resources for further practicing data- and ML-related interview questions: • NumPy exercises with solutions (10.6k stars on GitHub at the time of writing) • pandas exercises (9.2k stars on GitHub at the time of writing) • pandas practice with Google Colab (University of Berkeley) When googling interview questions about data and coding, I find that the search results favor the brainteaser-type questions if you search for “machine learning programming questions.” The way I get around this is by specifying the Python library and then “exercises”—for example, “numpy programming exercises” or “pandas programming exercises.”',\n",
       "  'title': 'Technical Interview: Coding'}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_query = {\n",
    "    \"size\": 5,\n",
    "    \"knn\": {\n",
    "        \"field\": \"text_vector\",\n",
    "        \"query_vector\": vector_search_term,\n",
    "        \"k\": 3,  \n",
    "        \"num_candidates\": 1000  \n",
    "    },\n",
    "    \"_source\": [\"text\", \"section\", \"title\"] #fields I want to retrieve\n",
    "}\n",
    "\n",
    "semantic_results = execute_search(semantic_query)\n",
    "semantic_results['hits']['hits'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Search\n",
    "\n",
    "Combination of both full-text and vector search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_index': 'ds-interview-questions',\n",
       " '_id': 'Alnyn5IBsRH_JjIM5yfP',\n",
       " '_score': 15.1664,\n",
       " '_source': {'chapter': 'CHAPTER 1',\n",
       "  'title': 'Machine Learning Roles and the Interview Process',\n",
       "  'section': 'The Three Pillars of Machine Learning Roles',\n",
       "  'text': 'To set the stage for the rest of the book, I’ll go over what I call the three pillars of ML and data science roles: • Machine learning algorithms and data intuition • Programming and software engineering skills • Execution and communication skills These are the broad categories of skills that you will be evaluated on during ML job interviews. This book focuses a lot on helping you understand these skills and bridge any gaps between your current experiences and skills and those under these three pillars (see Figure 1-6 ). All these skills will be expanded on in the following chapters. Figure 1-6. Three pillars of machine learning jobs. You’re able to understand the underlying workings of ML algorithms and statistics theory and their respective trade-offs—which is essential when you’re faced with an open-ended question in a real-world ML project at work. You’re not just following steps as you would for a school assignment. Having data intuition means that when you’re faced with a new problem, you know how to use data to solve it; and when you encounter new data or data sources, you know how to dive in to evaluate them. You ask yourself, is this data suitable for ML? What types of ML models might it be suitable for? Are there any issues with the data before you can use it for ML? You know what to ask and how to find the answers. In the ML job-interview process, various types of interviews and interview questions are aimed at assessing a candidate’s knowledge and readiness in this pillar, which I’ll cover in Chapters 3 and 4 . While working on a project, you have the programming skills required to deliver, such as manipulating data with Python or using an internal deploy process so that another team can use the results from the ML model. Even if you know the theory well, without the programming or software engineer‐ ing 14 sense, you can’t make ML materialize out of thin air. You need to use code to connect the data with ML algorithms, which are also implemented with code—that is, you must convert theory to practice. Other programming skills in high demand for ML roles are the (software) engineer’s ability to transition from prototype to production—that is, the ML is integrated and released. Some roles are responsible for end-to-end ML: from researching and train‐ ing models to deployment and production. Some ML roles, such as MLOps engi‐ neers, are responsible for building software infrastructure that can handle the demands of processing large amounts of data to send ML responses to users in sec‐ onds or even milliseconds. In the ML job-interview process, various types of interviews and interview questions assess a candidate’s skills in this pillar, which I’ll walk through in Chapters 5 and 6 . You’re able to work with people who aren’t in the same role as you. In ML, we work with software engineers, data engineers, product managers, and many other collea‐ gues. The ability to get things done in a team encompasses a few soft skills such as communication and some project management skills. For example, being unable to communicate with team members is a real blocker 15 for projects and could cause your ML projects to languish or even be deprioritized. Even in cases where you work with only one person (say, your boss), you still need to be able to report on your projects, which requires communication skills. Consequently, in the ML field a highly in-demand skill is being able to communicate technical con‐ cepts with nontechnical stakeholders. You’ll also need some project management skills to keep your tasks on track. We all learn to how manage our to-do lists and calendars during the process of education or self-learning, but it’s more chaotic since now your project calendar depends on oth‐ ers’ calendars and priorities. Even if you have a project and/or product manager to keep the team on track, you still need to manage yourself to some extent. Without soft skills, things don’t get done, full stop. Don’t be that candidate who focu‐ ses only on technical skills but neglects building and demonstrating their soft skills in interviews. I’ll delve into the details of how ML interviews evaluate candidates on this pillar in Chapter 7 . Growing your skills in all three ML pillars is a tall order, and for entry-level roles you’re usually only expected to have a minimum (such as a 3/10) for each pillar, as illustrated in Figure 1-7 . For example, a job candidate who has some exposure to pro‐ gramming, even if they aren’t skilled or experienced, can be taught to improve. Ideally, you’d be stronger on at least one pillar (such as 5/10 for programming) that is most related to the particular ML role in order to stand out from other job candidates. Figure 1-7. Minimum required skill levels for ML jobs (example). For senior roles, the bare minimum requirements are higher, but a similar rule of thumb applies: clear the minimum skill requirements. From then on, you’ll be com‐ pared with other candidates on the skills that you are great in, depending on the role. Data scientists who only train ML models but don’t deploy them might not need to develop their programming skills as much as their ML theory and communication skills. For entry level roles, I’d argue that the communication pillar has a lower requirement (but not 0/10, please!) because it takes the hard-earned experience of working with a larger group of people, including nontechnical teammates, to raise it higher. This also gives some candidates an edge in this pillar: for those with a nontraditional back‐ ground, such as candidates who are self-taught or switching from software engineer roles or another field, the ability to adeptly tell a story and showcase a portfolio can set them apart from other candidates. Now that you’ve had an overview of the three pillars, you can use this mental model to stand out.'}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vector_query = {\n",
    "    \"size\": 5,\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"must\": [\n",
    "                {\n",
    "                    \"match\": {\n",
    "                        \"text\": {\n",
    "                            \"query\": search_term,\n",
    "                            \"boost\": 1.0\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"multi_match\": {\n",
    "                        \"query\": search_term,\n",
    "                        \"fields\": [\"text^2\", \"section\", \"title\"],\n",
    "                        \"type\": \"best_fields\"\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "            \"should\": [\n",
    "                {\n",
    "                    \"terms\": {\n",
    "                        \"title\": [\"technical\", \"behavioral\"]\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    \"knn\": {\n",
    "        \"field\": \"text_vector\",\n",
    "        \"query_vector\": vector_search_term,\n",
    "        \"k\": 3,\n",
    "        \"num_candidates\": 1000\n",
    "    }\n",
    "}\n",
    "\n",
    "text_vector_results = execute_search(text_vector_query)\n",
    "text_vector_results['hits']['hits'][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>text_id</th>\n",
       "      <th>chapter</th>\n",
       "      <th>title</th>\n",
       "      <th>section</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Can you describe the differences between a dat...</td>\n",
       "      <td>86fd49a66d</td>\n",
       "      <td>CHAPTER 1</td>\n",
       "      <td>Machine Learning Roles and the Interview Process</td>\n",
       "      <td>Overview of This Book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How do you approach technical interviews for d...</td>\n",
       "      <td>86fd49a66d</td>\n",
       "      <td>CHAPTER 1</td>\n",
       "      <td>Machine Learning Roles and the Interview Process</td>\n",
       "      <td>Overview of This Book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Can you walk me through your process for evalu...</td>\n",
       "      <td>86fd49a66d</td>\n",
       "      <td>CHAPTER 1</td>\n",
       "      <td>Machine Learning Roles and the Interview Process</td>\n",
       "      <td>Overview of This Book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In what ways can a data scientist collaborate ...</td>\n",
       "      <td>86fd49a66d</td>\n",
       "      <td>CHAPTER 1</td>\n",
       "      <td>Machine Learning Roles and the Interview Process</td>\n",
       "      <td>Overview of This Book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How do you stay current with the latest develo...</td>\n",
       "      <td>86fd49a66d</td>\n",
       "      <td>CHAPTER 1</td>\n",
       "      <td>Machine Learning Roles and the Interview Process</td>\n",
       "      <td>Overview of This Book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>What is the most effective way to network and ...</td>\n",
       "      <td>1026686599</td>\n",
       "      <td>CHAPTER 9</td>\n",
       "      <td>Post-Interview and Follow-up</td>\n",
       "      <td>What to Do Between Interviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>How do you balance the need for a strong resum...</td>\n",
       "      <td>1026686599</td>\n",
       "      <td>CHAPTER 9</td>\n",
       "      <td>Post-Interview and Follow-up</td>\n",
       "      <td>What to Do Between Interviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>How do RSUs and stock options differ, and what...</td>\n",
       "      <td>dee0126444</td>\n",
       "      <td>CHAPTER 9</td>\n",
       "      <td>Post-Interview and Follow-up</td>\n",
       "      <td>Steps of the Offer Stage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>Can you provide examples of non-base pay optio...</td>\n",
       "      <td>dee0126444</td>\n",
       "      <td>CHAPTER 9</td>\n",
       "      <td>Post-Interview and Follow-up</td>\n",
       "      <td>Steps of the Offer Stage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>How do benefits like health and dental insuran...</td>\n",
       "      <td>dee0126444</td>\n",
       "      <td>CHAPTER 9</td>\n",
       "      <td>Post-Interview and Follow-up</td>\n",
       "      <td>Steps of the Offer Stage</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question     text_id    chapter  \\\n",
       "0    Can you describe the differences between a dat...  86fd49a66d  CHAPTER 1   \n",
       "1    How do you approach technical interviews for d...  86fd49a66d  CHAPTER 1   \n",
       "2    Can you walk me through your process for evalu...  86fd49a66d  CHAPTER 1   \n",
       "3    In what ways can a data scientist collaborate ...  86fd49a66d  CHAPTER 1   \n",
       "4    How do you stay current with the latest develo...  86fd49a66d  CHAPTER 1   \n",
       "..                                                 ...         ...        ...   \n",
       "235  What is the most effective way to network and ...  1026686599  CHAPTER 9   \n",
       "236  How do you balance the need for a strong resum...  1026686599  CHAPTER 9   \n",
       "237  How do RSUs and stock options differ, and what...  dee0126444  CHAPTER 9   \n",
       "238  Can you provide examples of non-base pay optio...  dee0126444  CHAPTER 9   \n",
       "239  How do benefits like health and dental insuran...  dee0126444  CHAPTER 9   \n",
       "\n",
       "                                                title  \\\n",
       "0    Machine Learning Roles and the Interview Process   \n",
       "1    Machine Learning Roles and the Interview Process   \n",
       "2    Machine Learning Roles and the Interview Process   \n",
       "3    Machine Learning Roles and the Interview Process   \n",
       "4    Machine Learning Roles and the Interview Process   \n",
       "..                                                ...   \n",
       "235                      Post-Interview and Follow-up   \n",
       "236                      Post-Interview and Follow-up   \n",
       "237                      Post-Interview and Follow-up   \n",
       "238                      Post-Interview and Follow-up   \n",
       "239                      Post-Interview and Follow-up   \n",
       "\n",
       "                           section  \n",
       "0            Overview of This Book  \n",
       "1            Overview of This Book  \n",
       "2            Overview of This Book  \n",
       "3            Overview of This Book  \n",
       "4            Overview of This Book  \n",
       "..                             ...  \n",
       "235  What to Do Between Interviews  \n",
       "236  What to Do Between Interviews  \n",
       "237       Steps of the Offer Stage  \n",
       "238       Steps of the Offer Stage  \n",
       "239       Steps of the Offer Stage  \n",
       "\n",
       "[240 rows x 5 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_df = pd.read_csv('../../data/ground_truth_data.csv')\n",
    "gt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for index, row in gt_df.iterrows():\n",
    "    search_term = row['question']\n",
    "    vector_search_term = model.encode(search_term).tolist()  \n",
    "\n",
    "    # Update the queries with the current search term\n",
    "    full_text_query['query']['bool']['must'][0]['match']['text']['query'] = search_term\n",
    "    semantic_query['knn']['query_vector'] = vector_search_term\n",
    "    text_vector_query['knn']['query_vector'] = vector_search_term\n",
    "\n",
    "    # Execute each search type\n",
    "    full_text_results = execute_search(full_text_query)  # Execute full-text search\n",
    "    semantic_results = execute_search(semantic_query)    # Execute semantic search\n",
    "    text_vector_results = execute_search(text_vector_query)  # Execute text-vector search\n",
    "\n",
    "    # Collect results\n",
    "    results[search_term] = {\n",
    "        'full_text': full_text_results['hits']['hits'],\n",
    "        'semantic': semantic_results['hits']['hits'],\n",
    "        'text_vector': text_vector_results['hits']['hits'],\n",
    "        'expected_answers': row['question']  # Adjust this if your column name is different\n",
    "    }\n",
    "\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [0, 3]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m evaluations\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Run the evaluation\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m evaluation_results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[28], line 16\u001b[0m, in \u001b[0;36mevaluate_search\u001b[0;34m(results, gt_df)\u001b[0m\n\u001b[1;32m     13\u001b[0m actual_text_vector_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(doc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_source\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m results[search_term][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_vector\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Calculate precision, recall, and F1 score for full text search\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m precision_full_text \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_score\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexpected_answers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mactual_full_text_set\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbinary\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m recall_full_text \u001b[38;5;241m=\u001b[39m recall_score(\u001b[38;5;28mlist\u001b[39m(expected_answers), \u001b[38;5;28mlist\u001b[39m(actual_full_text_set), average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m, zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     18\u001b[0m f1_full_text \u001b[38;5;241m=\u001b[39m f1_score(\u001b[38;5;28mlist\u001b[39m(expected_answers), \u001b[38;5;28mlist\u001b[39m(actual_full_text_set), average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m, zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/interview_preparation_bot-ZQDkHgpI/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/interview_preparation_bot-ZQDkHgpI/lib/python3.10/site-packages/sklearn/metrics/_classification.py:2204\u001b[0m, in \u001b[0;36mprecision_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   2037\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m   2038\u001b[0m     {\n\u001b[1;32m   2039\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2064\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2065\u001b[0m ):\n\u001b[1;32m   2066\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the precision.\u001b[39;00m\n\u001b[1;32m   2067\u001b[0m \n\u001b[1;32m   2068\u001b[0m \u001b[38;5;124;03m    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2202\u001b[0m \u001b[38;5;124;03m    array([0.5, 1. , 1. ])\u001b[39;00m\n\u001b[1;32m   2203\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2204\u001b[0m     p, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_recall_fscore_support\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2205\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2206\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2209\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarn_for\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprecision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2211\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2213\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m p\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/interview_preparation_bot-ZQDkHgpI/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:186\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/interview_preparation_bot-ZQDkHgpI/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1789\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1626\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute precision, recall, F-measure and support for each class.\u001b[39;00m\n\u001b[1;32m   1627\u001b[0m \n\u001b[1;32m   1628\u001b[0m \u001b[38;5;124;03mThe precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1786\u001b[0m \u001b[38;5;124;03m array([2, 2, 2]))\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1788\u001b[0m _check_zero_division(zero_division)\n\u001b[0;32m-> 1789\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43m_check_set_wise_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[1;32m   1792\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/interview_preparation_bot-ZQDkHgpI/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1561\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m average_options \u001b[38;5;129;01mand\u001b[39;00m average \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1559\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage has to be one of \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(average_options))\n\u001b[0;32m-> 1561\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1562\u001b[0m \u001b[38;5;66;03m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[1;32m   1563\u001b[0m \u001b[38;5;66;03m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[1;32m   1564\u001b[0m present_labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/interview_preparation_bot-ZQDkHgpI/lib/python3.10/site-packages/sklearn/metrics/_classification.py:103\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03mThis converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03my_pred : array or indicator matrix\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    102\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred)\n\u001b[0;32m--> 103\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    105\u001b[0m type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/interview_preparation_bot-ZQDkHgpI/lib/python3.10/site-packages/sklearn/utils/validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    460\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [0, 3]"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_search(results, gt_df):\n",
    "    evaluations = {}\n",
    "    \n",
    "    for index, row in gt_df.iterrows():\n",
    "        search_term = row['question']  # Access the 'question' field correctly\n",
    "        expected_answers = set(row['expected_answers']) if 'expected_answers' in row else set()  # Make sure this column exists\n",
    "\n",
    "        # Get the actual results for the current search term\n",
    "        actual_full_text_set = set(doc['_source']['title'] for doc in results[search_term]['full_text'])\n",
    "        actual_semantic_set = set(doc['_source']['title'] for doc in results[search_term]['semantic'])\n",
    "        actual_text_vector_set = set(doc['_source']['title'] for doc in results[search_term]['text_vector'])\n",
    "\n",
    "        # Calculate precision, recall, and F1 score for full text search\n",
    "        precision_full_text = precision_score(list(expected_answers), list(actual_full_text_set), average='binary', zero_division=0)\n",
    "        recall_full_text = recall_score(list(expected_answers), list(actual_full_text_set), average='binary', zero_division=0)\n",
    "        f1_full_text = f1_score(list(expected_answers), list(actual_full_text_set), average='binary', zero_division=0)\n",
    "        \n",
    "        # Repeat for semantic search\n",
    "        precision_semantic = precision_score(list(expected_answers), list(actual_semantic_set), average='binary', zero_division=0)\n",
    "        recall_semantic = recall_score(list(expected_answers), list(actual_semantic_set), average='binary', zero_division=0)\n",
    "        f1_semantic = f1_score(list(expected_answers), list(actual_semantic_set), average='binary', zero_division=0)\n",
    "\n",
    "        # Repeat for text vector search\n",
    "        precision_vector = precision_score(list(expected_answers), list(actual_text_vector_set), average='binary', zero_division=0)\n",
    "        recall_vector = recall_score(list(expected_answers), list(actual_text_vector_set), average='binary', zero_division=0)\n",
    "        f1_vector = f1_score(list(expected_answers), list(actual_text_vector_set), average='binary', zero_division=0)\n",
    "\n",
    "        # Store evaluations\n",
    "        evaluations[search_term] = {\n",
    "            'full_text': {\n",
    "                'precision': precision_full_text,\n",
    "                'recall': recall_full_text,\n",
    "                'f1': f1_full_text\n",
    "            },\n",
    "            'semantic': {\n",
    "                'precision': precision_semantic,\n",
    "                'recall': recall_semantic,\n",
    "                'f1': f1_semantic\n",
    "            },\n",
    "            'vector': {\n",
    "                'precision': precision_vector,\n",
    "                'recall': recall_vector,\n",
    "                'f1': f1_vector\n",
    "            }\n",
    "        }\n",
    "\n",
    "    return evaluations\n",
    "\n",
    "# Run the evaluation\n",
    "evaluation_results = evaluate_search(results, gt_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interview_preparation_bot-ZQDkHgpI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
