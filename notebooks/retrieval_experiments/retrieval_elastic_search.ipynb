{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval approach with Elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maldu/.local/share/virtualenvs/interview_preparation_bot-ZQDkHgpI/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/parsed_book.json', 'rt') as f_in:\n",
    "    book_raw = json.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chapter': 'CHAPTER 1',\n",
       " 'title': 'Machine Learning Roles and the Interview Process',\n",
       " 'content': [{'section': 'Overview of This Book',\n",
       "   'text': 'In the first part of this chapter, I’ll walk through the structure of this book. Then, I’ll discuss the various job titles and roles that use ML skills in industry. 1 I’ll also clarify the responsibilities of various job titles, such as data scientist, machine learning engineer, and so on, as this is a common point of confusion for job seekers. These will be illustrated with an ML skills matrix and ML lifecycle that will be referenced throughout the book. The second part of this chapter walks through the interview process, from beginning to end. I’ve mentored candidates who appreciated this overview since online resources often focus on specific pieces of the interview but not how they all connect together and result in an offer. Especially for new graduates 2 and readers coming from different industries, this chapter helps get everyone on the same page as well as clarifies the process. The interconnecting pieces of interviews are complex, with many types of combina‐ tions depending on the ML role you’re aiming for. This overview will help set the stage, so you’ll know what to focus your time on. For example, some online resources focus on knowledge specific to “product data scientists,” but will title the course or article “data scientist interview tips” without differentiating. For a newcomer, it’s hard to tell if that is relevant to your own career interests. After this chapter, you’ll be able to tell what skills are required for each job title, and in Chapter 2 , you’ll be able to parse out that information yourself from job postings and make your resume as relevant to the job title and job posting as possible. This chapter focuses on helping you differentiate among various ML roles, and walks through the entire interview process, as illustrated in Figure 1-1 : • Job applications and resume ( Chapter 2 ) • Technical interviews — Machine learning (Chapters 3 , 4 , and 6 ) — Coding/programming ( Chapter 5 ) • Behavioral interviews ( Chapter 7 ) • Your interview roadmap ( Chapter 8 ) • Post-interview and follow-up ( Chapter 9 ) Figure 1-1. Overview of the chapters and how they tie into the ML interview process. Depending on where you are in your ML interview journey, I encourage you to focus on the chapters and sections that seem relevant to you. I’ve also planned the book to be referenced as you go along; for example, you might iterate on your resume multi‐ ple times and then flip back to Chapter 2 when needed. The same applies to the other chapters. With that overview, let’s continue. The companion site to this book, https://susanshu.substack.com , features bonus content, helper resources, and more.'},\n",
       "  {'section': 'A Brief History of Machine Learning and Data Science Job Titles',\n",
       "   'text': 'First, let’s walk through a brief history of job titles. I decided to start with this section to dispel some myths about the “data scientist” job title and shed some light on why there are so many ML-related job titles. After understanding this history, you should be more aware of what job titles to aim for yourself. If you’ve ever been confused about the litany of titles such as machine learning engineer (MLE), product data sci‐ entist, MLOps engineer, and more, this section is for you. ML techniques aren’t a new thing; in 1985, David Ackley, Geoffrey E. Hinton, and Terrence J. Sejnowski popularized the Boltzmann Machine algorithm. 3 Even before that, regression techniques 4 had early developments in the 1800s. There have long been jobs and roles that use modeling techniques to forecast and predict. Econome‐ tricians, statisticians, financial modelers, physics modelers, and biochemical modelers have existed as professions for decades. The main difference is that there were much smaller datasets compared to the modern day (barring simulations). It was only in recent years, just before the 21st century, when compute power started to increase exponentially. In addition, advances in distributed and parallel computing created a cycle in which “big data” became more readily available. This allowed practitioners to apply that advanced compute power to millions or billions of data points. Larger datasets started being accumulated and distributed for ML research, such as WordNet, 5 and, subsequently, ImageNet, 6 a project led by Fei-Fei Li. These collective efforts laid the foundation for even more ML breakthroughs. AlexNet 7 was released in 2012, achieving high accuracy in the ImageNet challenge, 8 which demonstrated that deep learning can be adept at humanlike tasks at a scale that had not been seen before. Many ML practitioners see this as a time when machine learning, deep learning, and related topics increased by leaps and bounds in terms of recognition from the broader population, not just the AI community. The recent popularity of generative AI (such as ChatGPT) in 2022 and 2023 didn’t come out of nowhere, nor did the deepfakes, self-driving cars, chess bots, and more that came before it; these applica‐ tions were the results of many advances over recent years. “Data scientist” as a job title began as an umbrella term, when the ML and data fields were less mature. The term “data scientist” on Google Trends , which measures the popularity of search terms, surged in 2012. That was the year when that article was published by Harvard Business Review: “Data Scientist: The Sexiest Job of the 21st Century.” 9 By April 2013, the search popularity of “data scientist” was already tied with “statistician” and subsequently surpassed it by magnitudes, as shown in Figure 1-2 . Back in those days, there wasn’t a narrow divide between infrastructure jobs and model training, though. For example, Kubernetes was first released in 2014, but companies have taken some time to adopt it for orchestrating ML jobs. So now there are more specific job titles for ML infrastructure that didn’t exist before. Figure 1-2. Search popularity for the terms “data scientist,” “machine learning engi‐ neer,” and “statistician” on Google Trends (retrieved August 9, 2023). As social media, web recommender systems, and other modern use cases increased, companies started gathering much more granular data, such as clickstream data , which is data collected as a user browses a website or app. Another recent advance‐ ment is an average corporation being able to store the sheer amount of telemetry from machines and Internet of Things (IoT) devices. Previously, data scientists may have worked with data that was updated weekly or daily. Now, as many applications update more frequently or in real time, more infrastructure is needed to serve ML functionality in web products and apps, so more jobs have been created around those functions as well. In short: as the machine learning lifecycle grew more complex, more job titles were created to describe the new skills that a full ML team now requires. I’ll elaborate more on the job titles and ML lifecycle later in this chapter. All of this happened within the last decade, and companies don’t always change their job titles to reflect how the roles have become more specialized. Regardless, as a can‐ didate, knowing this history can help reduce confusion and frustration from applying for a job and finding the role is different from another company’s job with the exact same title. See Table 1-1 for previous trends in ML-related job titles and Table 1-2 for current trends in ML job titles. Table 1-1. Previous trends of ML and data job titles Table 1-2. Current trends of ML and data job titles With that history to explain why you will encounter different job titles, I’ll elaborate on each of these job titles and their responsibilities.'},\n",
       "  {'section': 'Job Titles Requiring ML Experience',\n",
       "   'text': 'Here is a nonexhaustive list of job titles for ML (or closely related) roles: • Data scientist • Machine learning engineer • Applied scientist • Software engineer, machine learning • MLOps engineer • Product data scientist • Data analyst • Decision scientist • Research scientist As I discussed “A Brief History of Machine Learning and Data Science Job Titles” on page 3 , each role is responsible for a different part of the ML lifecycle. A job title alone does not convey what the job entails. As a job seeker, be warned: in different companies, completely different titles might end up doing similar jobs! As illustrated in Figure 1-3 , your ML job title will depend on the company, the team, and which part(s) of the ML lifecycle your role is responsible for. To give specific examples of how job titles can depend on the company or organiza‐ tion that is hiring for the job—based on real people I’ve spoken to, job descriptions, and job interviews—the person responsible for training ML models but not for build‐ ing the underlying platform might be called the following: • Software engineer (ML) or data scientist (Google) • Applied scientist (Amazon) • Machine learning engineer (Meta, Pinterest) • Data scientist (Elastic, the team where I work) • Data scientist (Unity) Figure 1-3. What’s in a machine learning job title? By the time this book is published, any of the job titles within these companies and teams may have changed. Regardless, it demon‐ strates the point that ML titles may vary between companies and even between different teams in the same company. The job title also depends on the organization, the department, and so on. Some departments in Google have the data scientist 12 job title, and some don’t. At the com‐ panies where I’ve worked, my teams had data scientists train ML models while MLEs built the infrastructure (working all day in tools such as Kubernetes, Terraform, Jen‐ kins, and so on). In some other companies, MLEs are the ones who train ML models. As a personal example, my work experience has heavily involved ML model training, so I apply for jobs that have the title “machine learning engineer” or “data scientist.” I’ll provide more examples of skills and roles that could be a good fit for your inter‐ ests and skills in the following sections. As mentioned in the Preface , this book focuses more on the industry applications of ML rather than research roles. Here’s a brief overview of research roles: Requirements Most often PhD. Responsibilities Similar to academic roles/academia, such as conducting research and coming up with novel algorithms and improvements, authoring papers, presenting at aca‐ demic conferences, and so on. If you’re in a research role in industry , such as a researcher at Google DeepMind, the main difference is that there isn’t a require‐ ment for teaching (that I know of).'},\n",
       "  {'section': 'Machine Learning Lifecycle',\n",
       "   'text': 'In industry, it is an expectation for applied ML projects to eventually improve the customer experience—for example, a better recommender system that shows the user more relevant videos, news, and social media posts. In industry, “customer” can also mean internal customers: people within the same company or organization. For example, your team builds ML models that predict demand, which helps your com‐ pany’s logistics department better plan its shipment schedules. Regardless of whether the user is external or internal, many components are involved in building a full the user is external or internal, many components are involved in building a fullfledged, end-to-end ML product. I’ll walk through a simplified example. First, there needs to be data, as most ML is trained and tested with large amounts of data. Someone needs to make sure the raw data is brought in (ingested) so that it’s easily accessible later on for data analysis, ML, reporting and monitoring, and so on. This is illustrated by step A (data) in Figure 1-4 . Next, with the data in place, someone with knowledge of ML algorithms and tools will use the data to start ML development. This is illustrated by step B (machine learning development) in Figure 1-4 . This involves feature engineering, model train‐ ing, and evaluation. If the results aren’t great, there is a lot of iteration in step B, and this person might enhance their feature engineering or model training, or even go back to step A and request that more data be ingested. Once there are somewhat satisfactory results, they’ll move on to step C (machine learning deployment), which connects the ML models to customers. Depending on the type of ML project, it could be deployed to a website, app, internal dashboard, and so on. Of course, they’d like to make sure the ML is working properly, so any good team will have a way to monitor the results. In ML there are two main types of potential issues. The first is that something in the software layer doesn’t work, such as bugs in the code. The second is a data or ML model issue—for example, in the model-development phase, the model outputs normal results, but after deployment/ release, there is data imbalance, so then the model results become undesirable. From step C onward, there can be more iteration back to step B to improve the models and run more experiments in step C again. Figure 1-4. Machine learning lifecycle (the graph is simplified for understanding). In the machine learning lifecycle I just walked through, a lot of skills are required. Data pipelines, model training, maintaining continuous integration and continuous deployment (CI/CD): as a job candidate, what should you be learning to prepare for the interview? Thankfully, as I mentioned in “A Brief History of Machine Learning and Data Science Job Titles” on page 3 companies nowadays might hire people who have a subset of these skills. For example, they need some people specialized in step A (data engineering), some specialized in step B (ML development), some in step C (ML deployment), and so forth. I emphasize the might since it still differs depending on the company or team; I will walk through some scenarios. Startup roles will usually wear more hats, meaning they will need to do the jobs in multiple steps in the machine learning lifecycle as illustrated in Figure 1-4 . Here’s an example: Usually, startup companies have the goal of shipping 13 an end-to-end product, but because they have fewer customers they might care less about the scale and stability (at an early stage). Hence, it’s more likely that the person developing and training ML models is the same person doing data analysis and presenting to stakeholders, or even the same person building the platform infrastructure. An ML team in a startup might simply have fewer people. For example, the startup might have 30 software engineers and data people in total, whereas larger corporations could have a team of data ana‐ lysts alone numbering 30 people to disperse the workload. If the company and/or team has grown enough, it is more likely the ML roles have become more specialized. Generally, the larger the team, the more specialized the role. If the “machine learning engineer” at a larger company trains models, then it’s likely they don’t wear two or three hats at once, as they might at a startup. Instead, the big company hires more people to fill those roles. That isn’t to say the work is simpler at a larger company. In fact, there’s often more data, more scale, and more downsides if the ML functionality goes down, so each MLE’s time could be com‐ pletely tied up wearing only one hat. Larger company size often corresponds to larger ML teams, but it depends. For example, a large company in a traditionally nontech industry, might have its first ML team hires operate in more of a startup-like environment while they figure out how ML best works for the company. Let’s go one level deeper and add more details of ML or data responsibilities. Figure 1-5 expands on the machine learning lifecycle from Figure 1-4 to reflect teams or companies with more fine-grained roles. (It’s worth repeating that even if this list is a useful and common enough heuristic, it’s still a bit simplified for illustration pur‐ poses since there will always be exceptions and outliers.) Figure 1-5. Machine learning lifecycle with more fine-grained roles (extended version of Figure 1-4 ). Here’s an example of what your role might be responsible for within these more fine Here’s an example of what your role might be responsible for within these more finegrained roles, as illustrated in Figure 1-5 : • You build the data pipelines for analytics and ML (step A). • You train ML models (step B). • You build the infrastructure for ML models to be deployed (step C.1). • You design and conduct hypothesis testing, often A/B tests, for new ML product features (step C.2). • You do data analysis, build reports and dashboards, and present to stakeholders (step D). Figure 1-5 is often referred to in later chapters, so save or book‐ mark it!'},\n",
       "  {'section': 'The Three Pillars of Machine Learning Roles',\n",
       "   'text': 'To set the stage for the rest of the book, I’ll go over what I call the three pillars of ML and data science roles: • Machine learning algorithms and data intuition • Programming and software engineering skills • Execution and communication skills These are the broad categories of skills that you will be evaluated on during ML job interviews. This book focuses a lot on helping you understand these skills and bridge any gaps between your current experiences and skills and those under these three pillars (see Figure 1-6 ). All these skills will be expanded on in the following chapters. Figure 1-6. Three pillars of machine learning jobs. You’re able to understand the underlying workings of ML algorithms and statistics theory and their respective trade-offs—which is essential when you’re faced with an open-ended question in a real-world ML project at work. You’re not just following steps as you would for a school assignment. Having data intuition means that when you’re faced with a new problem, you know how to use data to solve it; and when you encounter new data or data sources, you know how to dive in to evaluate them. You ask yourself, is this data suitable for ML? What types of ML models might it be suitable for? Are there any issues with the data before you can use it for ML? You know what to ask and how to find the answers. In the ML job-interview process, various types of interviews and interview questions are aimed at assessing a candidate’s knowledge and readiness in this pillar, which I’ll cover in Chapters 3 and 4 . While working on a project, you have the programming skills required to deliver, such as manipulating data with Python or using an internal deploy process so that another team can use the results from the ML model. Even if you know the theory well, without the programming or software engineer‐ ing 14 sense, you can’t make ML materialize out of thin air. You need to use code to connect the data with ML algorithms, which are also implemented with code—that is, you must convert theory to practice. Other programming skills in high demand for ML roles are the (software) engineer’s ability to transition from prototype to production—that is, the ML is integrated and released. Some roles are responsible for end-to-end ML: from researching and train‐ ing models to deployment and production. Some ML roles, such as MLOps engi‐ neers, are responsible for building software infrastructure that can handle the demands of processing large amounts of data to send ML responses to users in sec‐ onds or even milliseconds. In the ML job-interview process, various types of interviews and interview questions assess a candidate’s skills in this pillar, which I’ll walk through in Chapters 5 and 6 . You’re able to work with people who aren’t in the same role as you. In ML, we work with software engineers, data engineers, product managers, and many other collea‐ gues. The ability to get things done in a team encompasses a few soft skills such as communication and some project management skills. For example, being unable to communicate with team members is a real blocker 15 for projects and could cause your ML projects to languish or even be deprioritized. Even in cases where you work with only one person (say, your boss), you still need to be able to report on your projects, which requires communication skills. Consequently, in the ML field a highly in-demand skill is being able to communicate technical con‐ cepts with nontechnical stakeholders. You’ll also need some project management skills to keep your tasks on track. We all learn to how manage our to-do lists and calendars during the process of education or self-learning, but it’s more chaotic since now your project calendar depends on oth‐ ers’ calendars and priorities. Even if you have a project and/or product manager to keep the team on track, you still need to manage yourself to some extent. Without soft skills, things don’t get done, full stop. Don’t be that candidate who focu‐ ses only on technical skills but neglects building and demonstrating their soft skills in interviews. I’ll delve into the details of how ML interviews evaluate candidates on this pillar in Chapter 7 . Growing your skills in all three ML pillars is a tall order, and for entry-level roles you’re usually only expected to have a minimum (such as a 3/10) for each pillar, as illustrated in Figure 1-7 . For example, a job candidate who has some exposure to pro‐ gramming, even if they aren’t skilled or experienced, can be taught to improve. Ideally, you’d be stronger on at least one pillar (such as 5/10 for programming) that is most related to the particular ML role in order to stand out from other job candidates. Figure 1-7. Minimum required skill levels for ML jobs (example). For senior roles, the bare minimum requirements are higher, but a similar rule of thumb applies: clear the minimum skill requirements. From then on, you’ll be com‐ pared with other candidates on the skills that you are great in, depending on the role. Data scientists who only train ML models but don’t deploy them might not need to develop their programming skills as much as their ML theory and communication skills. For entry level roles, I’d argue that the communication pillar has a lower requirement (but not 0/10, please!) because it takes the hard-earned experience of working with a larger group of people, including nontechnical teammates, to raise it higher. This also gives some candidates an edge in this pillar: for those with a nontraditional back‐ ground, such as candidates who are self-taught or switching from software engineer roles or another field, the ability to adeptly tell a story and showcase a portfolio can set them apart from other candidates. Now that you’ve had an overview of the three pillars, you can use this mental model to stand out.'},\n",
       "  {'section': 'Machine Learning Skills Matrix',\n",
       "   'text': 'Congratulations! You’ve made it to the end of a pretty dense section! Now that you’ve gone through the overview of the machine learning lifecycle and three pillars of ML skills, it’s time for you to map your interests and skills to job titles. Table 1-3 will give you a rough idea of what skills you will need to learn in order to succeed in specific roles. On a scale from one to three stars, one star represents a skill of lower importance, and three stars represents a highly important skill. Table 1-3. Machine learning and data skills matrix Table 1-3 is often referred to in later chapters, so save or bookmark it! Taking a look at these skills, you can roughly map them to the three pillars of ML skills in the previous section, as shown in Table 1-4 . Table 1-4. Machine learning and data skills mapped to the three pillars of ML jobs It’s OK if you aren’t completely sure what each type of skill might entail just yet. In Chapter 2 , we will revisit this matrix, and there will be details and a checklist for self assessment. You don’t need to worry about every single skill; companies are aware that they are responsible for training up new grads. But you can stand out from other job candi‐ dates by showing that you are more easily trainable and can learn fast. An easy way to demonstrate this earlier in your ML career is to gain high-level (not necessarily deep) exposure to topics that you don’t have experience with yet. For example, even if you haven’t worked much with version control, it’s a bonus to be familiar with. You can achieve this by watching some videos (30 minutes) and installing/testing it out on a project (one hour). Now, let’s tie all this together. We’ve looked at the machine learning lifecycle ( Figure 1-5 ) and machine learning skills matrix ( Table 1-3 ). What’s left is to see what jobs are best for you to apply to now or to gain the skills for! To do so, let’s connect everything to the current trend of ML and data job titles ( Table 1-2 ). This is illustra‐ ted in Figure 1-8 . Figure 1-8. Common ML job titles and how they correspond to the ML lifecycle. The alphabetical annotations in Figure 1-8 can be mapped to those in Figure 1-5 , lis‐ ted here for convenience: • (A) Data • (B) Machine learning development • (C.1) ML/software infrastructure • (C.2) ML hypothesis testing/monitoring • (D) Reports and dashboards Figure 1-8 is often referred to in later chapters, so save or book‐ mark it! When you see a job title and check the details of the job posting, you can map it to what that job is likely responsible for in the day to day. In addition, based on what part of the ML lifecycle you’re interested in, you can better prepare and target your job applications, so you don’t accidentally bark up the wrong tree. Go to a job board of your choice, such as LinkedIn, Indeed, or others listed in Chap‐ ter 2 . Search “machine learning,” “data scientist,” “data,” “AI,” “generative AI,” and so on. What are you seeing? Do you see different types of jobs being advertised that all use ML?'},\n",
       "  {'section': 'Introduction to ML Job Interviews',\n",
       "   'text': 'Now that I’ve introduced many job titles that might be of interest to you, it’s time to go through all the steps and types of interviews you will encounter during the pro‐ cess! This book is called Machine Learning Interviews , but interviews are so much more than just interview questions. There are job applications and your resume, which are how you get interviews in the first place. If you don’t increase your chances of getting more interviews, then you won’t even get the chance to answer any inter‐ view questions! I’ll be covering the process from beginning to end, including how to follow up after the interview ( Chapter 9 ). To quickly set the stage, here are some common terms used in this book. When I use the term “interviewee,” I am referring to the person currently seeking employment, while the “interviewer” is currently employed at the company that the interviewee is interviewing to join. The interviewee is also referred to as a “candidate” or “job candidate,” since they are a candidate to be the successfully hired person (see Table 1-5 ). Table 1-5. Synonyms of common terms used in this book “Big tech” refers to the major, large tech companies. Because of constant changes in the industry—for example, Facebook rebranding to Meta as the parent company and Google doing something similar with Alphabet—the popular FAANG. 16 (Facebook, Apple, Amazon, Netflix, Google) acronym has already become outdated. To keep things simple, I will use the umbrella term “big tech.”'},\n",
       "  {'section': 'Machine Learning Job-Interview Process',\n",
       "   'text': 'Now let’s get into the entire job-interview process. You’ll start by applying to jobs, then interviewing, and then, after some rounds of interviewing, finally receiving offers. This process is detailed in Figure 1-9 . Figure 1-9 is often referred to in later chapters, so save or book‐ mark it! Figure 1-9. ML interview process. Let’s imagine that you’re just starting out and applying for an ML role at a company with an established HR 17 and hiring process. You can begin your application in a few ways: by cold applying through the company website or job board (discussed in Chapter 2 ) or through a referral from someone within the team or company. You can also get interviews through cold messaging on LinkedIn or by emailing recruiters. Usually, at companies that have an HR-tracking software system, even if someone refers you, you’ll still need to upload a standard application into the online portal, which means you’ll need to prepare an updated resume and fill in your information. You may also choose to supplement your job-search efforts by working with a third-party recruiter, which is differentfrom an in from an inhouse recruiter who works or contracts specifically for the hiring company. Third-party recruiters often work with multiple compa‐ nies at once. Professional peers I know recommend working only with specific trusted third-party recruiters but warned me to beware those who make too many unrealistic promises or aren’t reputable. You can read more about third-party recruiters in this Forbes article . Using the first method—cold applying through company websites or third-party job boards—you’ve been browsing job boards like Indeed 18 as well as going directly to the career pages of companies you’re interested in working for. In this scenario, you don’t have someone referring you to the team or company (I’ll cover that in “Apply‐ ing via a Referral” on page 22 ). You’ve seen some ML-related jobs that seem relevant to you, and you clicked the links to apply. After you submit your application and the company has your information and resume, an HR member, recruiter, or whoever is in charge of resume screening, will proceed with the next step. The reality is that jobs have many applicants, and you should assume the first batch of applicants will be filtered before the hiring manager sees them. The hiring manager is the manager you’ll work with and report to if you join the team. So you can usually assume that a generalized HR partner or internal or external recruiters will be read‐ ing your resume first. These recruiters may be somewhat familiar with the roles they are screening resumes for, but they are still predominantly generalists, not as special‐ ized as the engineers and ML professionals you’ll actually be working with. This part of the screening process leads to several hidden criteria for your resume, which is why it might be baffling when your resume doesn’t clear this step even if you have a relevant background. It’s important to remember that these generalists will likely pass along your resume to the hiring manager if they: • See key technologies or experiences on your resume based on the job posting • See years of experience in key technologies or experiences or, in the case of entry • See years of experience in key technologies or experiences or, in the case of entrylevel or new-grad jobs, sufficient evidence that you can be easily trained • Understand that your skills and accomplishments are relevant, in plain language To determine whether your resume meets the criteria, the recruiter will likely be searching for keywords and comparing your resume to the job posting. They will not automatically “translate” skills on your resume for you. For example, if the job description says “Python” and your resume says “C++,” at this step they will likely not consider that, since both programming languages are object oriented, you could probably learn Python quickly if you put in the effort. There has been some debate at this stage on ATS , which is an acronym for applicant tracking system . While companies do use systems such as Workday to manage appli‐ cations, there hasn’t been concrete proof that companies are using them to program‐ matically filter out resumes at this step for each job posting ( Figure 1-10 ). Figure 1-10. Gergely Orosz (founder of The Pragmatic Engineer publication and former manager at Uber) on ATS (screenshot via Twitter). In practice, recruiters use them at most to filter resumes into a selection to fulfill existing criteria on the job posting, such as those mentioned on the previous page. I also haven’t seen ATS automatically filter out qualified candidates during full-time work experience, and once I was responsible for manually reading a PDF containing more than 50 resumes. However, I don’t want to claim that automated ATS rejections are fully untrue. To be safe, in this book I assume both standpoints have some truth to it. So even if ATS is an issue, the steps in this book will help you, since I’ll teach you the principles for how resume selection is conducted. (You can read more on ATS on thetechresume.com .) If you’re able to describe your experiences at a level that HR recruiters can under‐ stand is relevant to the job posting, you will increase your chances at the resume stand is relevant to the job posting, you will increase your chances at the resumescreening step. HR and recruiters, by nature of the role, are aware of higher-level technologies and what’s popular with the roles they’re hiring for but not the details, so it’s important for your resume to be optimized well. (Read more on how to opti‐ mize your resume in Chapter 2 .) Now that I’ve walked through cold applications directly via job boards or websites without any referral, I’ll provide some examples of how referrals can help you fast without any referral, I’ll provide some examples of how referrals can help you fasttrack the process. Let’s say you’re interested in an ML job at ARI Corporation. 19 You know an alum of your university who works on the ML team. You catch up with them and express your interest in the job. During the chat, you show the alum some of your personal ML projects, which are relevant to the ML job you’re interested in. The school alum agrees to refer you and gives you instructions for how to be referred, something that depends on the way the HR system of the company is set up. Since this alum knows you and is willing to vouch for your skills after seeing your personal projects, you get your resume to the “top of the pile.” Depending on the strength of the referral/recommendation, you may skip the resume screening alto‐ gether and get a highly guaranteed callback from a recruiter or even bypass the recruiter directly and get to the rest of the interview rounds. This is illustrated in Figure 1-11 . Note that I say “highly” guaranteed here since it still depends on various factors such as timing. As an example: maybe you got referred, but the job posting has coincidentally just been filled. Thus, you didn’t get to the rest of the interview. I will cover more on referrals and how to get them via professional networking in Chapter 2 . Figure 1-11. The interview process can be shortcut with a strong referral. You’ve been invited to an interview! How do you perform your best? Maybe time is limited; what do you do to ensure that you can maximize your outcome? My personal tactic is to first narrow down the types of questions that might be asked. For example, in the first round of an Amazon interview, the recruiter has outlined the format and it will focus on statistical theory questions. I will read online resources, skim over my notes, and see what topics I’m the weakest on. I will focus less on the questions that I know I can answer confidently and more on those that seem more likely to be asked but that I don’t know well. As to how I “guess” what will likely be asked, that is based mostly on conversations with the recruiter and my follow-up questions to the recruiter or hiring manager. I’m not super accurate at guessing, and this is similar to trying to guess what will come up in a university exam—it could work well, or it could backfire! Either way, there’s the trade-off between knowing a subset of questions well or know‐ ing all questions roughly but not as well (depth vs. breadth). When reviewing my preparation notes, I personally go for breadth, but your results may vary depending on how well you know the material already. Depending on your location and your interviewers’ location, there may be time zone differences. I try to find the time when I have the most energy possible. Sometimes the available interview time slots aren’t ideal, so I choose the lesser of the evils (for example, interviewing from GMT+8 and talking to someone in GMT-4 while travel‐ ing abroad). To make it easy to figure out time zones for candidates invited to an interview, it’s common for HR-scheduling software to have a calendar feature where you can input your preferred times and it will account for your local time zones. However, sometimes the time will be set via back-and-forth emails, and tools such as Cal‐ endly or Cal.com can help. As both an interviewer and interviewee, I am wary of scheduling right at the begin‐ ning of a workday. This is so that I have more time to prepare after I wake up. But of course, if no other time slots are available, then I will select the early time. As an interviewer, I’ve seen countless candidates’ interviews start late because of con‐ nection issues or using a new web-conferencing software—for example, not being able to set up Zoom on time because they hadn’t used it before. As a candidate, I’ve been tripped up and wasted time when needing to use Microsoft Teams because on my personal computers I only had Zoom and Google Meet. In the end, I used the browser version, but there was an issue with my login since my Microsoft student account had expired. We finally got it sorted out, a few minutes later. This could have been avoided if I had tried to sign in a bit earlier or on the day before the interview. Here are some tips to help your interview go more smoothly: Try your best to be in a quiet environment. Some software, such as Zoom, has pretty good built-in noise canceling, as do some wireless headphones. Check your audio and video beforehand. Video-wise, make sure the lighting is good and your camera lens is clean. Sound Video-wise, make sure the lighting is good and your camera lens is clean. Soundwise, make sure your mic sounds clear. On Windows and Mac, there are built-in camera and voice recording apps that I use. You could also start a new Zoom, Google Meet, or Teams session and run a test. Keep a mental list of backup options. Did the internet at your home suddenly go down before the interview? Is there a nearby cafe with (preferably secure) internet that you could go to? Can you use your phone data? Are there dial-in options via phone on the calendar invite? Knowing these things beforehand can help you a lot. I’ve had to dial in once to an interview, and thankfully, I knew that I had the option to. Congrats, your resume has made it past the resume screening! Now let’s go through an example to illustrate what might happen next. Let’s imagine that there were 200 applicants for the role. The recruiter has gone through them and removed 170 that either lacked relevant experience or for some reason didn’t seem to fit the role. Recall that this is based on the impression your resume gave the recruiter; it’s possible that with the same job title and same recruiter team, an improved resume would have passed. If you had a good referral, your resume might have already moved forward. Now that there are 30 applicants, the recruiter will call each of them; this is usually a shorter interview, 15 to 30 minutes long. We refer to this as the “recruiter screening” or “recruiter call.” Generally, the recruiter wants to see what you’re like as a person and if you’re easy to work with. If someone blatantly claims to have experience that they don’t, the call could reveal fabricated work or school experiences. There are other logistical issues to screen for, such as location, salary expectations, and legal status. The recruiter screening is more of a “smell test” instead of an in The recruiter screening is more of a “smell test” instead of an indepth test of your technical skills and experience. My tip for success is to optimize for one thing: that the recruiter understands that you are a good candidate, that your experience is relevant (or you can learn fast), and that you can fit well into the team and role they are hiring for. This is different from con‐ vincing a hiring manager of the same things, or an interview panel of senior MLEs. Instead, you will succeed here if you make the additional effort to connect your resume to the job description on this call. Here’s an example of some bullet points in a job description: • “The candidate has experience with recommender systems.” • “Experience with data processing such as Spark, Snowflake, or Hadoop.” • “The candidate has experience with Python.” A bad example of explaining your experience on the recruiter call for this job is: “For that past project, I used the ALS algorithm, which was implemented with PySpark.” A better example of explaining your experience on the recruiter call for this job is: “For that past project, I used the alternating least squares (ALS) algorithm, which is a recommender systems algorithm based off of matrix factorization, and I used PySpark, which is Spark that’s wrapped with a Python API.” Note that the italicized phrases also appear in the job description. The better example allows a recruiter to match up more of your skills to the job description, whereas the bad example doesn’t match up to the posted skills in an obvious manner. When you’re writing your resume, you have limited space; the real obvious manner. When you’re writing your resume, you have limited space; the realtime conversation of an interview is a chance for you to fill in gaps that the recruiter may not have noticed. It’s also important to expand on acronyms. This is true for interviews conducted with technical people too. I’m relatively specialized in recommender systems and rein‐ forcement learning, but I don’t work with computer vision tasks in my day-to-day work. I appreciated it when a candidate I interviewed was talking about computer vision projects and generally explained the more niche techniques. You can (and should) do this in a way that’s not condescending to your interviewer, whether they are a recruiter or part of your future team. The recruiter call is a good time for you as the candidate to assess the job as well. You can ask questions that you care about, to see if you should continue to interview. For example, I might ask about the team size and if this job focuses more on ML or data analyst responsibilities. You can also prepare some questions about the company and their products. For example, is the team’s current project focused on improving the click-through rate or long-term engagement? If you’re a user of the product, you might have a lot of ideas and questions to discuss. This is also a chance to show your enthusiasm and knowledge of the company. On to the next step. Good news: the recruiter cleared you! You explained your previ‐ ous experience well, and the recruiter was able to understand your past work and how it connects to the job description they have on hand. But it’s not over yet. You’re among 15 other candidates who succeeded at the first recruiter screening. The recruiter informs you of upcoming technical interviews which include ML theory, programming, and a case study interview. There are also behavioral interviews sprinkled throughout. If you pass those, you’ll make it to the on-site interview, which is often the final round. These days, there are also virtual on on-site interview, which is often the final round. These days, there are also virtual onsites/final rounds. If you pass the final round, you’ll be extended an offer. Let’s break down the various types of interviews that take place after the recruiter screening, the first being technical interviews. Technical interviews are typically con‐ ducted with technical individual contributors (ICs), such as an MLE or a data scientist. There may be multiple rounds of technical interviews; there could be one that is a data-focused coding round or one in which the interviewer presents some fictitious example data and asks you to use SQL or Python pandas/NumPy (sometimes there are multiple questions, and you use various programming tools throughout the inter‐ view). I’ll expand more on this type of interview structure and interview questions in Chapter 5 . Apart from ML and data-focused programming interviews, you might be asked brainteaser-type questions. For this type of interview, you might use an interview platform such as CoderPad or HackerRank, where the interviewer presents you with a question and you code in the online integrated development environment (IDE) that both you and your interviewer can see in real time. Sometimes you’ll get other formats, such as technical deep dives, systems design, take-home exercises in a pri‐ vate repo or Google Colab, and so on. I’ll elaborate on how to prepare for these types of interviews in Chapters 5 and 6 . These subsequent interview rounds could further reduce the number of candidates before the final round. In our example, fifteen candidates passed the recruiter screen, and eight passed the first round of technical interviews. After the second round of technical interviews, we’re left with three candidates who will proceed to the on-site interview. Interspersed during the interview process are questions meant to assess how you react in certain situations. The intent often is to use past experience to predict future performance and understand how you react to high-stress or difficult situations. In addition, these questions assess your soft skills, such as communication and team‐ work skills. You’ll want to prepare a few past experiences and relay them in a story‐ telling fashion. For example, during your first recruiter call, the recruiter might ask about a time when you dealt with a difficult timeline on a project. Once you’ve responded you won’t be out of the woods yet. During the on-site, an hour is often dedicated to behavioral questions. And in some technical interviews, you might be asked a couple of questions that are a mix between a purely technical question and a behavioral question. I’ll help you succeed with behavioral interviews in Chapter 7 , which also has tips on company-specific preparation, such as Amazon’s Leadership Principles . For many companies there is an “on-site” final round or the virtual equivalent. These are usually back-to-back interviews. For example, starting in the morning, you might meet with a technical director for a case study interview and then a senior data scien‐ tist for a programming interview. After a lunch break, you might meet with two data scientists who ask about ML theory, and then the hiring manager asks more behavio‐ ral questions and probes about your past experiences. In addition to technical inter‐ viewers, you may speak with a stakeholder (e.g., a product manager on an adjacent team that the team you’re interviewing for works closely with). In several final-round interviews I’ve been through, there was a product manager interviewer or someone from another department that the ML team worked closely with, such as marketing or advertising. Some companies will have an additional mini round after this, such as a quick chat with a skip level (your manager’s manager).In this chapter, you’ve learned about various ML roles, the ML lifecycle, and the dif‐ ferent responsibilities that map onto the ML lifecycle. You’ve also seen how you make your way from the beginning of the process to the final round of interviews. There’s a lot to prepare for and to learn about, but now you have an overview and hopefully some thoughts on how you can target your preparations. Now that this chapter has set the foundation, I’ll walk through a detailed job applica‐ tion guide, including a resume guide, to help you greatly increase your chances of getting interviews.'}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_raw[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sudo docker run -it \\\n",
    "    --rm \\\n",
    "    --name elasticsearch \\\n",
    "    -m 4GB \\\n",
    "    -p 9200:9200 \\\n",
    "    -p 9300:9300 \\\n",
    "    -e \"discovery.type=single-node\" \\\n",
    "    -e \"xpack.security.enabled=false\" \\\n",
    "    docker.elastic.co/elasticsearch/elasticsearch:8.4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create embeddings using pretrained model\n",
    "\n",
    "The model used to create the embeddings can be found in this website\n",
    "https://www.sbert.net/docs/sentence_transformer/pretrained_models.html#semantic-search-models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flattening the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "for chapter in book_raw:\n",
    "    chapter_name = chapter['chapter']\n",
    "    title = chapter['title']\n",
    "\n",
    "    for doc in chapter['content']:\n",
    "        new_doc = {\n",
    "            'chapter': chapter_name,\n",
    "            'title': title,\n",
    "            'section': doc['section'],\n",
    "            'text': doc['text']\n",
    "        }\n",
    "        documents.append(new_doc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chapter': 'CHAPTER 1',\n",
       " 'title': 'Machine Learning Roles and the Interview Process',\n",
       " 'section': 'Overview of This Book',\n",
       " 'text': 'In the first part of this chapter, I’ll walk through the structure of this book. Then, I’ll discuss the various job titles and roles that use ML skills in industry. 1 I’ll also clarify the responsibilities of various job titles, such as data scientist, machine learning engineer, and so on, as this is a common point of confusion for job seekers. These will be illustrated with an ML skills matrix and ML lifecycle that will be referenced throughout the book. The second part of this chapter walks through the interview process, from beginning to end. I’ve mentored candidates who appreciated this overview since online resources often focus on specific pieces of the interview but not how they all connect together and result in an offer. Especially for new graduates 2 and readers coming from different industries, this chapter helps get everyone on the same page as well as clarifies the process. The interconnecting pieces of interviews are complex, with many types of combina‐ tions depending on the ML role you’re aiming for. This overview will help set the stage, so you’ll know what to focus your time on. For example, some online resources focus on knowledge specific to “product data scientists,” but will title the course or article “data scientist interview tips” without differentiating. For a newcomer, it’s hard to tell if that is relevant to your own career interests. After this chapter, you’ll be able to tell what skills are required for each job title, and in Chapter 2 , you’ll be able to parse out that information yourself from job postings and make your resume as relevant to the job title and job posting as possible. This chapter focuses on helping you differentiate among various ML roles, and walks through the entire interview process, as illustrated in Figure 1-1 : • Job applications and resume ( Chapter 2 ) • Technical interviews — Machine learning (Chapters 3 , 4 , and 6 ) — Coding/programming ( Chapter 5 ) • Behavioral interviews ( Chapter 7 ) • Your interview roadmap ( Chapter 8 ) • Post-interview and follow-up ( Chapter 9 ) Figure 1-1. Overview of the chapters and how they tie into the ML interview process. Depending on where you are in your ML interview journey, I encourage you to focus on the chapters and sections that seem relevant to you. I’ve also planned the book to be referenced as you go along; for example, you might iterate on your resume multi‐ ple times and then flip back to Chapter 2 when needed. The same applies to the other chapters. With that overview, let’s continue. The companion site to this book, https://susanshu.substack.com , features bonus content, helper resources, and more.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.encode(\"Hello I am just checking that you are working properly\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the dense vector using the pre-trained model\n",
    "\n",
    "A dense vector typically represents a word, sentence, or document as a fixed-length array of numbers, also known as an embedding. Dense vectors are crucial for Elasticsearch, especially for tasks where understanding the meaning behind the words is more important than just matching exact terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "operations = []\n",
    "for doc in documents:\n",
    "    doc[\"text_vector\"] = model.encode(doc[\"text\"]).tolist()\n",
    "    operations.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chapter': 'CHAPTER 1',\n",
       " 'title': 'Machine Learning Roles and the Interview Process',\n",
       " 'section': 'A Brief History of Machine Learning and Data Science Job Titles',\n",
       " 'text': 'First, let’s walk through a brief history of job titles. I decided to start with this section to dispel some myths about the “data scientist” job title and shed some light on why there are so many ML-related job titles. After understanding this history, you should be more aware of what job titles to aim for yourself. If you’ve ever been confused about the litany of titles such as machine learning engineer (MLE), product data sci‐ entist, MLOps engineer, and more, this section is for you. ML techniques aren’t a new thing; in 1985, David Ackley, Geoffrey E. Hinton, and Terrence J. Sejnowski popularized the Boltzmann Machine algorithm. 3 Even before that, regression techniques 4 had early developments in the 1800s. There have long been jobs and roles that use modeling techniques to forecast and predict. Econome‐ tricians, statisticians, financial modelers, physics modelers, and biochemical modelers have existed as professions for decades. The main difference is that there were much smaller datasets compared to the modern day (barring simulations). It was only in recent years, just before the 21st century, when compute power started to increase exponentially. In addition, advances in distributed and parallel computing created a cycle in which “big data” became more readily available. This allowed practitioners to apply that advanced compute power to millions or billions of data points. Larger datasets started being accumulated and distributed for ML research, such as WordNet, 5 and, subsequently, ImageNet, 6 a project led by Fei-Fei Li. These collective efforts laid the foundation for even more ML breakthroughs. AlexNet 7 was released in 2012, achieving high accuracy in the ImageNet challenge, 8 which demonstrated that deep learning can be adept at humanlike tasks at a scale that had not been seen before. Many ML practitioners see this as a time when machine learning, deep learning, and related topics increased by leaps and bounds in terms of recognition from the broader population, not just the AI community. The recent popularity of generative AI (such as ChatGPT) in 2022 and 2023 didn’t come out of nowhere, nor did the deepfakes, self-driving cars, chess bots, and more that came before it; these applica‐ tions were the results of many advances over recent years. “Data scientist” as a job title began as an umbrella term, when the ML and data fields were less mature. The term “data scientist” on Google Trends , which measures the popularity of search terms, surged in 2012. That was the year when that article was published by Harvard Business Review: “Data Scientist: The Sexiest Job of the 21st Century.” 9 By April 2013, the search popularity of “data scientist” was already tied with “statistician” and subsequently surpassed it by magnitudes, as shown in Figure 1-2 . Back in those days, there wasn’t a narrow divide between infrastructure jobs and model training, though. For example, Kubernetes was first released in 2014, but companies have taken some time to adopt it for orchestrating ML jobs. So now there are more specific job titles for ML infrastructure that didn’t exist before. Figure 1-2. Search popularity for the terms “data scientist,” “machine learning engi‐ neer,” and “statistician” on Google Trends (retrieved August 9, 2023). As social media, web recommender systems, and other modern use cases increased, companies started gathering much more granular data, such as clickstream data , which is data collected as a user browses a website or app. Another recent advance‐ ment is an average corporation being able to store the sheer amount of telemetry from machines and Internet of Things (IoT) devices. Previously, data scientists may have worked with data that was updated weekly or daily. Now, as many applications update more frequently or in real time, more infrastructure is needed to serve ML functionality in web products and apps, so more jobs have been created around those functions as well. In short: as the machine learning lifecycle grew more complex, more job titles were created to describe the new skills that a full ML team now requires. I’ll elaborate more on the job titles and ML lifecycle later in this chapter. All of this happened within the last decade, and companies don’t always change their job titles to reflect how the roles have become more specialized. Regardless, as a can‐ didate, knowing this history can help reduce confusion and frustration from applying for a job and finding the role is different from another company’s job with the exact same title. See Table 1-1 for previous trends in ML-related job titles and Table 1-2 for current trends in ML job titles. Table 1-1. Previous trends of ML and data job titles Table 1-2. Current trends of ML and data job titles With that history to explain why you will encounter different job titles, I’ll elaborate on each of these job titles and their responsibilities.',\n",
       " 'text_vector': [0.06606891006231308,\n",
       "  0.04531856253743172,\n",
       "  -0.07584855705499649,\n",
       "  -0.015627508983016014,\n",
       "  -0.00380276283249259,\n",
       "  0.028007833287119865,\n",
       "  0.0754760354757309,\n",
       "  -0.018123220652341843,\n",
       "  0.008582300506532192,\n",
       "  0.030267365276813507,\n",
       "  0.053311970084905624,\n",
       "  0.04067913070321083,\n",
       "  -0.0052957115694880486,\n",
       "  0.03953951597213745,\n",
       "  0.03824090212583542,\n",
       "  -0.06374721229076385,\n",
       "  0.003309109713882208,\n",
       "  0.013971514068543911,\n",
       "  -0.0002465914294589311,\n",
       "  -0.010918994434177876,\n",
       "  -0.03460480645298958,\n",
       "  -0.007678858004510403,\n",
       "  0.009301344864070415,\n",
       "  0.07720332592725754,\n",
       "  -0.0413946695625782,\n",
       "  -0.0010991183808073401,\n",
       "  0.029970597475767136,\n",
       "  -0.013293800875544548,\n",
       "  0.0029220327269285917,\n",
       "  0.03814472630620003,\n",
       "  0.022476427257061005,\n",
       "  -0.042439717799425125,\n",
       "  0.021363502368330956,\n",
       "  0.03257731720805168,\n",
       "  2.079729256365681e-06,\n",
       "  -0.0034328100737184286,\n",
       "  0.027217580005526543,\n",
       "  0.044020239263772964,\n",
       "  -0.03004377707839012,\n",
       "  -0.09820341318845749,\n",
       "  0.03330150246620178,\n",
       "  -0.06206883490085602,\n",
       "  0.0117797227576375,\n",
       "  0.026507128030061722,\n",
       "  -0.018568409606814384,\n",
       "  0.007624240592122078,\n",
       "  0.1042281910777092,\n",
       "  -0.028022192418575287,\n",
       "  -0.025550968945026398,\n",
       "  -0.01998484879732132,\n",
       "  0.01871972344815731,\n",
       "  -0.06893045455217361,\n",
       "  0.05362166836857796,\n",
       "  -0.00026781688211485744,\n",
       "  0.0027046920731663704,\n",
       "  0.01537004392594099,\n",
       "  0.018221532925963402,\n",
       "  0.06257844716310501,\n",
       "  0.01126670092344284,\n",
       "  -0.0395016111433506,\n",
       "  -0.005242536310106516,\n",
       "  0.07580775022506714,\n",
       "  0.018005216494202614,\n",
       "  -0.0005650149541907012,\n",
       "  0.01900913193821907,\n",
       "  0.05989791080355644,\n",
       "  0.0017558903200551867,\n",
       "  -0.009293802082538605,\n",
       "  -0.021128876134753227,\n",
       "  -0.00982658937573433,\n",
       "  -0.003831487149000168,\n",
       "  -0.033346984535455704,\n",
       "  -0.025827178731560707,\n",
       "  -0.021122025325894356,\n",
       "  0.02961868979036808,\n",
       "  0.04232271760702133,\n",
       "  -0.03837832435965538,\n",
       "  -0.0046745711006224155,\n",
       "  0.007481447421014309,\n",
       "  -0.03835546225309372,\n",
       "  0.01964222826063633,\n",
       "  0.02881629578769207,\n",
       "  0.015349695459008217,\n",
       "  0.03103875368833542,\n",
       "  -0.03303079679608345,\n",
       "  0.047387149184942245,\n",
       "  -0.04087924212217331,\n",
       "  -0.023438507691025734,\n",
       "  0.07196937501430511,\n",
       "  0.02605152316391468,\n",
       "  0.049340229481458664,\n",
       "  -0.0011889338493347168,\n",
       "  0.028738314285874367,\n",
       "  0.009924503974616528,\n",
       "  -0.011536595411598682,\n",
       "  0.013317717239260674,\n",
       "  -0.0069163525477051735,\n",
       "  -0.030664220452308655,\n",
       "  -0.03107013739645481,\n",
       "  -0.03370391204953194,\n",
       "  0.0018016074318438768,\n",
       "  -0.014760669320821762,\n",
       "  0.040581174194812775,\n",
       "  0.011700952425599098,\n",
       "  -0.014479339122772217,\n",
       "  0.020870819687843323,\n",
       "  0.0002638025034684688,\n",
       "  -0.00817932840436697,\n",
       "  0.0025816624984145164,\n",
       "  -0.04894799739122391,\n",
       "  -0.04194409400224686,\n",
       "  -0.041730381548404694,\n",
       "  0.011076243594288826,\n",
       "  -0.031705331057310104,\n",
       "  0.01189903263002634,\n",
       "  0.027185821905732155,\n",
       "  0.00047855201410129666,\n",
       "  0.03057236783206463,\n",
       "  -0.03890026733279228,\n",
       "  -0.046381451189517975,\n",
       "  -0.05449884757399559,\n",
       "  0.04013647511601448,\n",
       "  0.032110102474689484,\n",
       "  0.053272999823093414,\n",
       "  -0.04501698911190033,\n",
       "  -0.04790934920310974,\n",
       "  -0.005251242779195309,\n",
       "  0.03849092498421669,\n",
       "  0.04074351117014885,\n",
       "  -0.006842250470072031,\n",
       "  0.024030065163969994,\n",
       "  -0.01352503802627325,\n",
       "  -0.0047929114662110806,\n",
       "  0.004801218397915363,\n",
       "  0.07695091515779495,\n",
       "  0.03469907492399216,\n",
       "  0.0024456805549561977,\n",
       "  0.04805701598525047,\n",
       "  -0.005186579190194607,\n",
       "  -0.033008210361003876,\n",
       "  -0.007145306095480919,\n",
       "  -0.03687674179673195,\n",
       "  0.0069240303710103035,\n",
       "  0.01807449385523796,\n",
       "  0.006329546682536602,\n",
       "  0.017354682087898254,\n",
       "  -0.04495503753423691,\n",
       "  -0.0035321894101798534,\n",
       "  -0.006921124178916216,\n",
       "  0.0018134437268599868,\n",
       "  -0.0738353282213211,\n",
       "  0.020462598651647568,\n",
       "  -0.029939092695713043,\n",
       "  -0.008973566815257072,\n",
       "  0.07127561420202255,\n",
       "  0.004359736107289791,\n",
       "  -0.014378378167748451,\n",
       "  0.027597876265645027,\n",
       "  0.03494704142212868,\n",
       "  0.02766389772295952,\n",
       "  -0.029038937762379646,\n",
       "  0.006079044658690691,\n",
       "  0.07711493968963623,\n",
       "  0.03673067316412926,\n",
       "  -0.013300592079758644,\n",
       "  -0.03998706117272377,\n",
       "  -0.027641819790005684,\n",
       "  0.005531721282750368,\n",
       "  -0.05451449751853943,\n",
       "  0.05231975018978119,\n",
       "  -0.017225516960024834,\n",
       "  -0.01773797534406185,\n",
       "  0.003748881397768855,\n",
       "  0.028564603999257088,\n",
       "  0.021129801869392395,\n",
       "  0.009148807264864445,\n",
       "  0.02449561096727848,\n",
       "  0.08051370084285736,\n",
       "  0.03769601136445999,\n",
       "  -0.008803902193903923,\n",
       "  0.02850445546209812,\n",
       "  -0.04532962664961815,\n",
       "  0.017087947577238083,\n",
       "  0.022892268374562263,\n",
       "  -0.045863185077905655,\n",
       "  -0.011087359860539436,\n",
       "  -0.07216333597898483,\n",
       "  -0.038531720638275146,\n",
       "  -0.013985269702970982,\n",
       "  -0.023124217987060547,\n",
       "  0.0766829401254654,\n",
       "  0.01453989278525114,\n",
       "  -0.019872210919857025,\n",
       "  0.020869798958301544,\n",
       "  0.03782374784350395,\n",
       "  -0.03671935945749283,\n",
       "  -0.050103649497032166,\n",
       "  -0.0010222081327810884,\n",
       "  -0.001558128627948463,\n",
       "  0.04118221625685692,\n",
       "  -0.05239838734269142,\n",
       "  -0.03697893023490906,\n",
       "  -0.06598374247550964,\n",
       "  0.029649225994944572,\n",
       "  0.034157101064920425,\n",
       "  -0.02445690892636776,\n",
       "  -0.06481662392616272,\n",
       "  -0.04938056319952011,\n",
       "  0.002515654545277357,\n",
       "  -0.01163905393332243,\n",
       "  -0.012934372760355473,\n",
       "  0.008459903299808502,\n",
       "  -0.050200898200273514,\n",
       "  0.03042571432888508,\n",
       "  -0.02022688090801239,\n",
       "  -0.005065116565674543,\n",
       "  0.006720611825585365,\n",
       "  -0.0393737368285656,\n",
       "  0.014697501435875893,\n",
       "  0.00017506499716546386,\n",
       "  -0.001132841338403523,\n",
       "  -0.002252863720059395,\n",
       "  -0.09398359805345535,\n",
       "  -0.016102958470582962,\n",
       "  -0.0696796178817749,\n",
       "  0.0237945057451725,\n",
       "  0.05232834443449974,\n",
       "  0.004358089063316584,\n",
       "  0.010724320076406002,\n",
       "  0.05368935316801071,\n",
       "  -0.002584524219855666,\n",
       "  -0.004418307915329933,\n",
       "  -0.006053366232663393,\n",
       "  -0.012549171224236488,\n",
       "  -0.01523069478571415,\n",
       "  -0.01238216832280159,\n",
       "  -0.011928270570933819,\n",
       "  0.03676788881421089,\n",
       "  -0.0376051664352417,\n",
       "  -0.009780357591807842,\n",
       "  -0.0011541753774508834,\n",
       "  0.03440144285559654,\n",
       "  -0.03515739366412163,\n",
       "  -0.017253302037715912,\n",
       "  0.002684232546016574,\n",
       "  -0.013121460564434528,\n",
       "  -0.008725123479962349,\n",
       "  0.012661011889576912,\n",
       "  -0.025318561121821404,\n",
       "  -0.057607922703027725,\n",
       "  0.05355115607380867,\n",
       "  -0.0400671549141407,\n",
       "  -0.028454983606934547,\n",
       "  0.03415612876415253,\n",
       "  -0.012389509938657284,\n",
       "  0.0042282165959477425,\n",
       "  0.007561819162219763,\n",
       "  -0.003925757948309183,\n",
       "  -0.011161931790411472,\n",
       "  -0.01257438212633133,\n",
       "  0.004901573061943054,\n",
       "  -0.010814359411597252,\n",
       "  0.017711933702230453,\n",
       "  0.020955758169293404,\n",
       "  -0.040588606148958206,\n",
       "  0.010667256079614162,\n",
       "  0.01803452894091606,\n",
       "  0.0024050564970821142,\n",
       "  -0.03758038580417633,\n",
       "  0.00028565613320097327,\n",
       "  0.003009356325492263,\n",
       "  0.0007151829195208848,\n",
       "  -0.04722214862704277,\n",
       "  -0.027198662981390953,\n",
       "  0.0008122437866404653,\n",
       "  0.008109611459076405,\n",
       "  0.022873103618621826,\n",
       "  -0.0020602906588464975,\n",
       "  -0.040730416774749756,\n",
       "  0.04841279610991478,\n",
       "  0.019382940605282784,\n",
       "  -0.04489132761955261,\n",
       "  -0.004527580924332142,\n",
       "  -0.04680744186043739,\n",
       "  -0.03085365891456604,\n",
       "  0.031186342239379883,\n",
       "  0.0713123083114624,\n",
       "  -0.01749725081026554,\n",
       "  0.01957760937511921,\n",
       "  -0.03963230550289154,\n",
       "  -0.036592934280633926,\n",
       "  0.01598355919122696,\n",
       "  -0.030292702838778496,\n",
       "  0.024436619132757187,\n",
       "  0.0023214814718812704,\n",
       "  0.02914116345345974,\n",
       "  -0.007625013589859009,\n",
       "  0.012297776527702808,\n",
       "  -0.009883164428174496,\n",
       "  0.022692741826176643,\n",
       "  -0.03238226845860481,\n",
       "  0.010917801409959793,\n",
       "  0.0478123240172863,\n",
       "  0.042903609573841095,\n",
       "  -0.024091273546218872,\n",
       "  -0.002253559185191989,\n",
       "  0.03988434374332428,\n",
       "  0.045700594782829285,\n",
       "  -0.021282298490405083,\n",
       "  -0.05791624262928963,\n",
       "  -0.005728038959205151,\n",
       "  -0.03647732362151146,\n",
       "  0.04149213060736656,\n",
       "  0.10073183476924896,\n",
       "  0.020527750253677368,\n",
       "  -0.08045247197151184,\n",
       "  -0.04772065579891205,\n",
       "  0.01932142861187458,\n",
       "  -0.007101878989487886,\n",
       "  0.030127782374620438,\n",
       "  -0.024174990132451057,\n",
       "  -0.039210692048072815,\n",
       "  0.0008273554849438369,\n",
       "  0.027834270149469376,\n",
       "  0.00034315374796278775,\n",
       "  -0.024012722074985504,\n",
       "  -0.09624817967414856,\n",
       "  0.0013241141568869352,\n",
       "  0.03476903960108757,\n",
       "  -0.013779825530946255,\n",
       "  0.010820349678397179,\n",
       "  -0.0029222164303064346,\n",
       "  -0.012951036915183067,\n",
       "  -0.01282590813934803,\n",
       "  0.01177946850657463,\n",
       "  -0.05410123988986015,\n",
       "  -0.00535534368827939,\n",
       "  -0.01906205341219902,\n",
       "  -0.02466508187353611,\n",
       "  -0.004356122575700283,\n",
       "  0.006209157407283783,\n",
       "  0.017151547595858574,\n",
       "  -0.01954115554690361,\n",
       "  -0.05501613765954971,\n",
       "  -0.033230748027563095,\n",
       "  0.006833230145275593,\n",
       "  0.007417644374072552,\n",
       "  -0.01304837130010128,\n",
       "  -0.04773737117648125,\n",
       "  0.004429795779287815,\n",
       "  0.06166398525238037,\n",
       "  -0.00835472159087658,\n",
       "  0.005322097800672054,\n",
       "  -0.008785232901573181,\n",
       "  -0.00791714433580637,\n",
       "  -0.05100962147116661,\n",
       "  0.025196321308612823,\n",
       "  0.03646881878376007,\n",
       "  0.031153438612818718,\n",
       "  0.03680706024169922,\n",
       "  0.017008377239108086,\n",
       "  -0.049645163118839264,\n",
       "  -0.026324093341827393,\n",
       "  -0.02027708850800991,\n",
       "  -0.10212179273366928,\n",
       "  0.00787843856960535,\n",
       "  0.0002377682103542611,\n",
       "  -0.023459874093532562,\n",
       "  0.005685360170900822,\n",
       "  0.019479062408208847,\n",
       "  -0.03959747031331062,\n",
       "  -0.05073043331503868,\n",
       "  0.060207799077034,\n",
       "  0.04221693053841591,\n",
       "  -0.04446297138929367,\n",
       "  0.0145115302875638,\n",
       "  -0.01644154079258442,\n",
       "  0.04055437818169594,\n",
       "  -0.0269129890948534,\n",
       "  0.012730925343930721,\n",
       "  0.04868648573756218,\n",
       "  -0.05640551447868347,\n",
       "  0.008165411651134491,\n",
       "  0.023618215695023537,\n",
       "  0.010857492685317993,\n",
       "  -0.06186135858297348,\n",
       "  -0.05651771277189255,\n",
       "  -0.028259294107556343,\n",
       "  -0.008443672209978104,\n",
       "  0.05899100378155708,\n",
       "  -0.028514690697193146,\n",
       "  -0.0429510734975338,\n",
       "  -0.015106553211808205,\n",
       "  -0.039634548127651215,\n",
       "  -0.01071388553828001,\n",
       "  -0.007325134705752134,\n",
       "  0.02280302532017231,\n",
       "  -0.005779926665127277,\n",
       "  0.04193408042192459,\n",
       "  -0.007463559973984957,\n",
       "  0.0552663579583168,\n",
       "  -0.052493467926979065,\n",
       "  -0.005243164021521807,\n",
       "  0.020683612674474716,\n",
       "  -0.04542155563831329,\n",
       "  0.008331715129315853,\n",
       "  0.09297247231006622,\n",
       "  0.04360513389110565,\n",
       "  -0.09098030626773834,\n",
       "  0.09756780415773392,\n",
       "  -0.02945149689912796,\n",
       "  0.08422981202602386,\n",
       "  0.05244465172290802,\n",
       "  0.031239798292517662,\n",
       "  0.06164417788386345,\n",
       "  -0.015262868255376816,\n",
       "  -0.04119983687996864,\n",
       "  -0.04646046832203865,\n",
       "  0.022454669699072838,\n",
       "  -0.012235848233103752,\n",
       "  0.07515323907136917,\n",
       "  0.03544388711452484,\n",
       "  0.036679137498140335,\n",
       "  -0.03558730334043503,\n",
       "  -0.03826139122247696,\n",
       "  -0.054116927087306976,\n",
       "  -0.006655484903603792,\n",
       "  0.012116723693907261,\n",
       "  0.019443180412054062,\n",
       "  -0.043282344937324524,\n",
       "  -0.030636893585324287,\n",
       "  -0.019596515223383904,\n",
       "  -0.007878200151026249,\n",
       "  0.03868275508284569,\n",
       "  0.04207286611199379,\n",
       "  -0.007899442687630653,\n",
       "  -0.003663739887997508,\n",
       "  -0.04174764081835747,\n",
       "  -0.05212956294417381,\n",
       "  0.0777641162276268,\n",
       "  -0.016771579161286354,\n",
       "  0.06696739792823792,\n",
       "  -0.013741860166192055,\n",
       "  0.03219130262732506,\n",
       "  -0.03206454962491989,\n",
       "  0.04737213999032974,\n",
       "  -0.029888931661844254,\n",
       "  -0.08089249581098557,\n",
       "  -0.033023182302713394,\n",
       "  0.014175611548125744,\n",
       "  0.054730355739593506,\n",
       "  0.019531190395355225,\n",
       "  -0.012452976778149605,\n",
       "  0.07712072879076004,\n",
       "  -0.04848906770348549,\n",
       "  -0.02792203240096569,\n",
       "  -0.05703088268637657,\n",
       "  -0.020144730806350708,\n",
       "  -0.008493867702782154,\n",
       "  -0.022559810429811478,\n",
       "  0.04497664421796799,\n",
       "  0.008116108365356922,\n",
       "  -0.028598355129361153,\n",
       "  0.020461125299334526,\n",
       "  -0.062424518167972565,\n",
       "  0.046411123126745224,\n",
       "  0.11154387891292572,\n",
       "  0.0010989821748808026,\n",
       "  -0.00826630275696516,\n",
       "  -0.007976618595421314,\n",
       "  0.0016863218042999506,\n",
       "  -0.02629786916077137,\n",
       "  -0.026995521038770676,\n",
       "  0.014994101598858833,\n",
       "  0.03660009801387787,\n",
       "  0.013781029731035233,\n",
       "  -0.025223184376955032,\n",
       "  -0.009965931996703148,\n",
       "  0.00458912318572402,\n",
       "  -0.03425789624452591,\n",
       "  -0.0008328360272571445,\n",
       "  0.060735978186130524,\n",
       "  -0.03258571773767471,\n",
       "  -0.03275550901889801,\n",
       "  0.002879028907045722,\n",
       "  -0.027904454618692398,\n",
       "  0.03861277550458908,\n",
       "  0.007317420560866594,\n",
       "  -0.02847658284008503,\n",
       "  -0.054346490651369095,\n",
       "  0.05064050108194351,\n",
       "  0.05081033334136009,\n",
       "  0.015568842180073261,\n",
       "  -0.043860748410224915,\n",
       "  0.06052583083510399,\n",
       "  -0.06325481832027435,\n",
       "  -0.059563327580690384,\n",
       "  0.029067600145936012,\n",
       "  -0.027052899822592735,\n",
       "  0.050313252955675125,\n",
       "  0.018823884427547455,\n",
       "  0.007639425341039896,\n",
       "  -0.012172972783446312,\n",
       "  -0.03006257303059101,\n",
       "  0.04391228035092354,\n",
       "  -0.03157825395464897,\n",
       "  0.039480943232774734,\n",
       "  0.004037121776491404,\n",
       "  0.0026923182886093855,\n",
       "  0.0418529212474823,\n",
       "  -0.03694061562418938,\n",
       "  0.002784724347293377,\n",
       "  -0.061942461878061295,\n",
       "  -0.05743245780467987,\n",
       "  0.015872392803430557,\n",
       "  0.010635687969624996,\n",
       "  0.03938867151737213,\n",
       "  -0.008741693571209908,\n",
       "  -0.031767409294843674,\n",
       "  -0.010295013897120953,\n",
       "  0.002296960912644863,\n",
       "  -0.0021257305052131414,\n",
       "  -0.03231678530573845,\n",
       "  -0.00729492399841547,\n",
       "  0.03506103903055191,\n",
       "  -0.0038116180803626776,\n",
       "  -0.0356922447681427,\n",
       "  -0.03699394688010216,\n",
       "  -0.01977047324180603,\n",
       "  0.003479261649772525,\n",
       "  0.03993493691086769,\n",
       "  0.034821610897779465,\n",
       "  0.01446866150945425,\n",
       "  -0.0465506985783577,\n",
       "  0.03628890588879585,\n",
       "  -0.01677398383617401,\n",
       "  0.03791150450706482,\n",
       "  -0.012815563939511776,\n",
       "  0.05356059595942497,\n",
       "  -0.007672739215195179,\n",
       "  -0.004740254953503609,\n",
       "  0.01181875728070736,\n",
       "  0.0102028027176857,\n",
       "  -0.017471682280302048,\n",
       "  -0.02144976705312729,\n",
       "  -0.07845401763916016,\n",
       "  -0.013074511662125587,\n",
       "  0.04433221369981766,\n",
       "  0.012511640787124634,\n",
       "  0.03956323489546776,\n",
       "  0.005207272246479988,\n",
       "  0.022034581750631332,\n",
       "  -0.01341076847165823,\n",
       "  0.017888829112052917,\n",
       "  0.010471689514815807,\n",
       "  -5.4023564066342544e-33,\n",
       "  0.06273004412651062,\n",
       "  -0.03525228425860405,\n",
       "  0.07765353471040726,\n",
       "  0.061391446739435196,\n",
       "  -0.02384204789996147,\n",
       "  0.014497761614620686,\n",
       "  -0.0176616869866848,\n",
       "  0.028972890228033066,\n",
       "  -0.0482834130525589,\n",
       "  0.009528030641376972,\n",
       "  -0.03902558982372284,\n",
       "  -0.0032943724654614925,\n",
       "  0.005756794940680265,\n",
       "  0.015592397190630436,\n",
       "  0.04055174067616463,\n",
       "  -0.021079260855913162,\n",
       "  0.011296295560896397,\n",
       "  0.0051696146838366985,\n",
       "  0.06545768678188324,\n",
       "  0.023220039904117584,\n",
       "  -0.07087520509958267,\n",
       "  0.01078211423009634,\n",
       "  -0.012268850579857826,\n",
       "  -0.04977979511022568,\n",
       "  0.006346903275698423,\n",
       "  0.024544019252061844,\n",
       "  0.02209760993719101,\n",
       "  -0.03226298838853836,\n",
       "  -0.007752036675810814,\n",
       "  -0.005610586144030094,\n",
       "  0.038247209042310715,\n",
       "  0.07052814215421677,\n",
       "  -0.004235969390720129,\n",
       "  -0.06332997977733612,\n",
       "  -0.06598049402236938,\n",
       "  0.036838412284851074,\n",
       "  -0.050730396062135696,\n",
       "  0.052630193531513214,\n",
       "  -0.02322184108197689,\n",
       "  -0.022501178085803986,\n",
       "  0.018955077975988388,\n",
       "  0.019748743623495102,\n",
       "  0.06697692722082138,\n",
       "  0.04411890730261803,\n",
       "  -0.0765247792005539,\n",
       "  0.0074395383708179,\n",
       "  0.024983147159218788,\n",
       "  -0.02730625681579113,\n",
       "  0.02067146636545658,\n",
       "  -0.04548447206616402,\n",
       "  0.02184279076755047,\n",
       "  0.00266981590539217,\n",
       "  -0.051851674914360046,\n",
       "  0.04077094420790672,\n",
       "  0.03492320701479912,\n",
       "  0.02548070438206196,\n",
       "  -0.017479341477155685,\n",
       "  0.021920453757047653,\n",
       "  -0.06614820659160614,\n",
       "  0.0018970170058310032,\n",
       "  0.04006213694810867,\n",
       "  -0.04857800155878067,\n",
       "  0.07349228858947754,\n",
       "  0.06810744106769562,\n",
       "  0.041173502802848816,\n",
       "  0.04260774701833725,\n",
       "  0.13043127954006195,\n",
       "  0.02215079963207245,\n",
       "  -0.01227223314344883,\n",
       "  0.06180323660373688,\n",
       "  0.02483632043004036,\n",
       "  0.06593676656484604,\n",
       "  0.0071989563293755054,\n",
       "  0.010368443094193935,\n",
       "  0.013322046957910061,\n",
       "  -0.0590553879737854,\n",
       "  -0.03556472063064575,\n",
       "  -0.021196307614445686,\n",
       "  -0.00106766726821661,\n",
       "  -0.06417965888977051,\n",
       "  0.010434252209961414,\n",
       "  0.004031035583466291,\n",
       "  -0.008688749745488167,\n",
       "  -0.08099931478500366,\n",
       "  0.004019645042717457,\n",
       "  -0.0077246930450201035,\n",
       "  0.028778111562132835,\n",
       "  0.025643453001976013,\n",
       "  -0.02500726282596588,\n",
       "  -0.04285277798771858,\n",
       "  0.0018891413928940892,\n",
       "  0.1072169840335846,\n",
       "  0.003145250491797924,\n",
       "  -0.07025501877069473,\n",
       "  0.006670638918876648,\n",
       "  -0.013953391462564468,\n",
       "  -0.021985316649079323,\n",
       "  0.037968479096889496,\n",
       "  0.04393798112869263,\n",
       "  0.020094111561775208,\n",
       "  -0.09405159950256348,\n",
       "  -0.025714024901390076,\n",
       "  -0.07681666314601898,\n",
       "  0.033112142235040665,\n",
       "  0.022592831403017044,\n",
       "  0.03209444135427475,\n",
       "  -0.01951802708208561,\n",
       "  0.01557836215943098,\n",
       "  0.001193723175674677,\n",
       "  -0.022580455988645554,\n",
       "  0.006208548787981272,\n",
       "  0.026494471356272697,\n",
       "  0.010143030434846878,\n",
       "  -0.012842662632465363,\n",
       "  -0.005834595300257206,\n",
       "  -0.028992321342229843,\n",
       "  0.024958712980151176,\n",
       "  -0.024606449529528618,\n",
       "  -0.007937591522932053,\n",
       "  -0.014013553969562054,\n",
       "  0.031215166673064232,\n",
       "  -0.02569190226495266,\n",
       "  -0.014582696370780468,\n",
       "  -0.029290003702044487,\n",
       "  0.003837253199890256,\n",
       "  -0.04034789651632309,\n",
       "  -0.013712234795093536,\n",
       "  -0.045362070202827454,\n",
       "  0.03771521523594856,\n",
       "  -0.046821728348731995,\n",
       "  -0.029563140124082565,\n",
       "  -0.024172931909561157,\n",
       "  2.674981942618615e-07,\n",
       "  0.00598176522180438,\n",
       "  0.05083223059773445,\n",
       "  0.007102744188159704,\n",
       "  0.028811227530241013,\n",
       "  0.01831391453742981,\n",
       "  0.016095614060759544,\n",
       "  0.025160428136587143,\n",
       "  -0.02432354725897312,\n",
       "  0.004398027900606394,\n",
       "  0.030485166236758232,\n",
       "  0.02945248782634735,\n",
       "  -0.011351828463375568,\n",
       "  0.000556918210349977,\n",
       "  -0.06351268291473389,\n",
       "  -0.027407990768551826,\n",
       "  -0.09931714832782745,\n",
       "  -0.028732728213071823,\n",
       "  -0.030628109350800514,\n",
       "  -0.06863461434841156,\n",
       "  0.0421827994287014,\n",
       "  0.025273695588111877,\n",
       "  0.018677765503525734,\n",
       "  0.035644251853227615,\n",
       "  -0.03694610297679901,\n",
       "  0.01975378952920437,\n",
       "  -0.039749741554260254,\n",
       "  -0.01602136343717575,\n",
       "  -0.02045288495719433,\n",
       "  0.07754141837358475,\n",
       "  0.03762979060411453,\n",
       "  0.0281163789331913,\n",
       "  0.06645344942808151,\n",
       "  0.021740468218922615,\n",
       "  0.023830760270357132,\n",
       "  0.020803887397050858,\n",
       "  -0.00778147904202342,\n",
       "  0.01782580278813839,\n",
       "  0.00946755614131689,\n",
       "  0.02644900418817997,\n",
       "  0.04422738403081894,\n",
       "  0.025743382051587105,\n",
       "  0.017160259187221527,\n",
       "  0.01792924292385578,\n",
       "  -0.026933642104268074,\n",
       "  0.026158511638641357,\n",
       "  -0.021689968183636665,\n",
       "  -0.08127816766500473,\n",
       "  -0.035310667008161545,\n",
       "  -0.012588229030370712,\n",
       "  -0.013935275375843048,\n",
       "  -0.0015044432366266847,\n",
       "  -0.040959812700748444,\n",
       "  0.028187310323119164,\n",
       "  -0.04556509107351303,\n",
       "  -0.002020201412960887,\n",
       "  -0.024906707927584648,\n",
       "  -0.0238063745200634,\n",
       "  0.02405397593975067,\n",
       "  0.02506897784769535,\n",
       "  0.0264270082116127,\n",
       "  -0.03306831046938896,\n",
       "  -0.03008778765797615,\n",
       "  0.02162666618824005,\n",
       "  -0.01203906536102295,\n",
       "  0.07563776522874832,\n",
       "  0.009789579547941685,\n",
       "  0.018244445323944092,\n",
       "  2.2403955758075737e-34,\n",
       "  0.0027073740493506193,\n",
       "  0.03626491501927376,\n",
       "  -0.010984865948557854,\n",
       "  -0.015721598640084267,\n",
       "  -0.004851104225963354,\n",
       "  0.007580568082630634,\n",
       "  -0.012230148538947105,\n",
       "  -0.00745063740760088,\n",
       "  -0.013364853337407112,\n",
       "  0.0005683943745680153,\n",
       "  -0.06282488256692886]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "operations[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup ElasticSearch connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'name': 'c4b9a141cb9f', 'cluster_name': 'docker-cluster', 'cluster_uuid': 'zvY8omxTS82s8jtkdJwk3w', 'version': {'number': '8.4.3', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': '42f05b9372a9a4a470db3b52817899b99a76ee73', 'build_date': '2022-10-04T07:17:24.662462378Z', 'build_snapshot': False, 'lucene_version': '9.3.0', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es_client = Elasticsearch('http://localhost:9200') \n",
    "\n",
    "es_client.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating mappings and Index\n",
    "\n",
    "Imagine that you need to create a schema. what do you need? I would say the column names, the table name, the type of data you are going to introduce...\n",
    "\n",
    "The mapping will set this metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0\n",
    "    },\n",
    "    \"mappings\": {\n",
    "    \"properties\": {\n",
    "        \"text\": {\n",
    "            \"type\": \"text\",\n",
    "        },\n",
    "        \"section\": {\n",
    "            \"type\": \"keyword\",\n",
    "        },\n",
    "        \"chapter\": {\n",
    "            \"type\": \"text\",\n",
    "        },\n",
    "        \"title\": {\n",
    "            \"type\": \"text\",\n",
    "        },\n",
    "        \"text_vector\": {\n",
    "            \"type\": \"dense_vector\",\n",
    "            \"dims\": 768, # got them above\n",
    "            \"index\": True,\n",
    "            \"similarity\": \"cosine\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'ml-interview-questions'})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_name = \"ml-interview-questions\"\n",
    "\n",
    "# it is better to delete the index every time when experimenting\n",
    "es_client.indices.delete(index=index_name, ignore_unavailable=True) \n",
    "es_client.indices.create(index=index_name, body=index_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add documents to the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in documents:\n",
    "    try:\n",
    "        es_client.index(index=index_name, body=doc)\n",
    "    except Exception as e:\n",
    "        print(f\"Error when indexing the document: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term = \"what are the steps of ML interviews?\"\n",
    "vector_search_term = model.encode(search_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = es_client.search(\n",
    "    index=\"ml-interview-questions\",\n",
    "    body={\n",
    "        \"size\": 5,  \n",
    "        \"knn\": {\n",
    "            \"field\": \"text_vector\",  \n",
    "            \"query_vector\": vector_search_term,\n",
    "            \"k\": 5,  \n",
    "            \"num_candidates\": 1000  \n",
    "        },\n",
    "        \"_source\": [\"text\", \"section\", \"title\", \"chapter\"]\n",
    "    }\n",
    ")\n",
    "res['hits']['hits']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Search with Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_query = {\n",
    "    \"field\": \"text_vector\",\n",
    "    \"query_vector\": vector_search_term,\n",
    "    \"k\": 5,\n",
    "    \"num_candidates\": 10000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_index': 'ml-interview-questions',\n",
       "  '_id': 'g8XylZIByzqUH6XnCljF',\n",
       "  '_score': 1.8904558,\n",
       "  '_source': {'chapter': 'CHAPTER 8',\n",
       "   'title': 'Tying It All Together: Your Interview Roadmap',\n",
       "   'section': 'Interview Preparation Checklist',\n",
       "   'text': 'Now that you’ve gone through the entire ML interview process, it’s time to create a plan. In Chapters 1 and 2 , you learned about the many types of ML jobs and did a self-assessment of which one(s) might be more suitable for you. Based on that, you also learned about the skills you are expected to be stronger in. In the subsequent chapters, you learned about what types of questions are commonly asked in inter‐ views. Are there any types that you need to prepare more for? The goal of this book is for you to start bridging the gap, not just read about bridging the gap. To succeed in interviews and land the job, taking action will help you—not just thinking about taking action. Follow this checklist to create a plan for your interview process. Refer back to rele‐ vant content or past exercises in this book to help you complete the checklist: • Write down the parts of the ML lifecycle that you’re interested in doing at work. See Figure 1-5 in Chapter 1 for a reminder of the ML lifecycle. • Write down the skills that are required for that role and run the self-assessment of them in Chapter 2 . • Determine what types of interviews could be relevant for that role. Review the overview of the interview process in Chapter 1 . • Make sure your resume is tidied up, with bullet points relevant to the role you picked. Refer to Chapter 2 for more resume tips. • Write down a time frame during which you’re aiming to prepare for interviews and start applying. For example: I aim to prepare for interviews for three months and then start applying. Now that you have these components in place, it’s time to construct your roadmap.',\n",
       "   'text_vector': [0.03385744243860245,\n",
       "    -0.04645149037241936,\n",
       "    -0.04925841465592384,\n",
       "    -0.03212582319974899,\n",
       "    0.016751080751419067,\n",
       "    0.021245524287223816,\n",
       "    -0.05818988010287285,\n",
       "    -0.06031307578086853,\n",
       "    0.03030058927834034,\n",
       "    -0.044351574033498764,\n",
       "    0.05949075147509575,\n",
       "    0.03979422152042389,\n",
       "    -0.01612319052219391,\n",
       "    0.06974305957555771,\n",
       "    0.03858231008052826,\n",
       "    -0.03856825828552246,\n",
       "    0.03471633046865463,\n",
       "    0.02470911107957363,\n",
       "    -0.028088850900530815,\n",
       "    -0.01477313693612814,\n",
       "    -0.005000946577638388,\n",
       "    0.031461700797080994,\n",
       "    -0.02598130702972412,\n",
       "    -0.004161367192864418,\n",
       "    0.0005319927004165947,\n",
       "    -0.06510033458471298,\n",
       "    -0.02346961572766304,\n",
       "    0.04617723450064659,\n",
       "    0.007564730476588011,\n",
       "    -0.01861964352428913,\n",
       "    -0.009351453743875027,\n",
       "    -0.04814695939421654,\n",
       "    0.013376526534557343,\n",
       "    -0.02192872390151024,\n",
       "    2.3820109618100105e-06,\n",
       "    -0.042050980031490326,\n",
       "    -0.012768040411174297,\n",
       "    -0.02535930834710598,\n",
       "    -0.028695832937955856,\n",
       "    -0.08454246073961258,\n",
       "    0.0019332607043907046,\n",
       "    0.013306713663041592,\n",
       "    0.034688811749219894,\n",
       "    0.03008219599723816,\n",
       "    -0.012861883267760277,\n",
       "    0.017564749345183372,\n",
       "    0.027250496670603752,\n",
       "    0.03135230764746666,\n",
       "    0.020456334576010704,\n",
       "    0.05493921414017677,\n",
       "    -0.01980568654835224,\n",
       "    -0.06340477615594864,\n",
       "    0.0357399545609951,\n",
       "    0.03866203501820564,\n",
       "    0.006052416283637285,\n",
       "    -0.03423167020082474,\n",
       "    -0.01869778335094452,\n",
       "    0.024414511397480965,\n",
       "    0.03827270492911339,\n",
       "    -0.06769198179244995,\n",
       "    0.011083517223596573,\n",
       "    0.021974068135023117,\n",
       "    -0.007409246638417244,\n",
       "    -0.002860243897885084,\n",
       "    0.05795564502477646,\n",
       "    0.03302876278758049,\n",
       "    0.046286024153232574,\n",
       "    -0.04777039214968681,\n",
       "    -0.032075896859169006,\n",
       "    -0.02189371921122074,\n",
       "    0.03013654239475727,\n",
       "    -0.02002860978245735,\n",
       "    -0.031126730144023895,\n",
       "    -0.02064143680036068,\n",
       "    0.03793681785464287,\n",
       "    -0.022081848233938217,\n",
       "    -0.0007043401128612459,\n",
       "    -0.02239467203617096,\n",
       "    0.0099478829652071,\n",
       "    -0.01547523494809866,\n",
       "    -0.048805128782987595,\n",
       "    0.008436816744506359,\n",
       "    -0.019155751913785934,\n",
       "    -0.017110256478190422,\n",
       "    0.006571223959326744,\n",
       "    0.07859817147254944,\n",
       "    -0.0024313186295330524,\n",
       "    -9.205275273416191e-05,\n",
       "    0.038217950612306595,\n",
       "    -0.018170608207583427,\n",
       "    0.10034535825252533,\n",
       "    0.015170264057815075,\n",
       "    0.0026716471184045076,\n",
       "    0.060309574007987976,\n",
       "    -0.04979988560080528,\n",
       "    0.004535169806331396,\n",
       "    0.032826848328113556,\n",
       "    0.0543312132358551,\n",
       "    -0.022003525868058205,\n",
       "    -0.06410877406597137,\n",
       "    -0.0405653677880764,\n",
       "    -0.019890474155545235,\n",
       "    0.052130430936813354,\n",
       "    -5.275789590086788e-05,\n",
       "    0.02423614077270031,\n",
       "    0.02406751736998558,\n",
       "    -0.001258672564290464,\n",
       "    -0.04637730121612549,\n",
       "    -0.08221863955259323,\n",
       "    -0.030144646763801575,\n",
       "    -0.026179729029536247,\n",
       "    -0.016616616398096085,\n",
       "    -0.04405856877565384,\n",
       "    0.03192346543073654,\n",
       "    0.008560622110962868,\n",
       "    0.030866172164678574,\n",
       "    0.03034360520541668,\n",
       "    0.006299725733697414,\n",
       "    0.005082567688077688,\n",
       "    -0.04596612602472305,\n",
       "    -0.030440663918852806,\n",
       "    0.015213252045214176,\n",
       "    0.02572941966354847,\n",
       "    0.012222984805703163,\n",
       "    -0.07304003834724426,\n",
       "    -0.0478748194873333,\n",
       "    -0.003961380571126938,\n",
       "    0.01988726109266281,\n",
       "    -0.04307718575000763,\n",
       "    -0.07017438113689423,\n",
       "    0.046511322259902954,\n",
       "    0.014622647315263748,\n",
       "    -0.01652621477842331,\n",
       "    -0.03115561045706272,\n",
       "    0.033837564289569855,\n",
       "    0.04335698485374451,\n",
       "    -0.0237920843064785,\n",
       "    0.02840973064303398,\n",
       "    -0.08474108576774597,\n",
       "    0.0023857872001826763,\n",
       "    0.04394608736038208,\n",
       "    -0.02366788126528263,\n",
       "    0.02711636759340763,\n",
       "    0.0032792522106319666,\n",
       "    0.01063678041100502,\n",
       "    -0.007434335071593523,\n",
       "    -0.010935315862298012,\n",
       "    0.026617614552378654,\n",
       "    0.01225333847105503,\n",
       "    0.02486594207584858,\n",
       "    0.004020769149065018,\n",
       "    0.01151914894580841,\n",
       "    0.000509777688421309,\n",
       "    -0.002421468496322632,\n",
       "    0.07137420028448105,\n",
       "    -0.0012017627013847232,\n",
       "    0.055999767035245895,\n",
       "    0.05359279736876488,\n",
       "    0.025987839326262474,\n",
       "    0.04340115934610367,\n",
       "    0.006000591907650232,\n",
       "    -0.04187452048063278,\n",
       "    0.06820343434810638,\n",
       "    0.05483746528625488,\n",
       "    0.002207511803135276,\n",
       "    -0.04336186498403549,\n",
       "    0.01795649528503418,\n",
       "    0.0649651512503624,\n",
       "    -0.02702251449227333,\n",
       "    0.032012369483709335,\n",
       "    0.02314547263085842,\n",
       "    0.003123006783425808,\n",
       "    -0.010639956220984459,\n",
       "    0.01610160805284977,\n",
       "    0.01204521581530571,\n",
       "    0.01822643354535103,\n",
       "    -0.01489193458110094,\n",
       "    -0.029146833345294,\n",
       "    0.03039919212460518,\n",
       "    0.04578036814928055,\n",
       "    -0.020857494324445724,\n",
       "    -0.03042280115187168,\n",
       "    0.01651897467672825,\n",
       "    0.02572052553296089,\n",
       "    0.013304051011800766,\n",
       "    -0.037659596651792526,\n",
       "    -0.09524177014827728,\n",
       "    0.062342651188373566,\n",
       "    -0.02382611855864525,\n",
       "    0.013778554275631905,\n",
       "    -0.01106303371489048,\n",
       "    0.011925707571208477,\n",
       "    -0.04275854676961899,\n",
       "    0.04913688078522682,\n",
       "    -0.0476374588906765,\n",
       "    -0.009426042437553406,\n",
       "    -0.06904938817024231,\n",
       "    0.002932091476395726,\n",
       "    -0.07359377294778824,\n",
       "    -0.017025718465447426,\n",
       "    -0.04924916476011276,\n",
       "    -0.011546983383595943,\n",
       "    -0.046809930354356766,\n",
       "    -0.0028254734352231026,\n",
       "    0.06629858911037445,\n",
       "    -0.002899667015299201,\n",
       "    -0.038229167461395264,\n",
       "    -0.08761849999427795,\n",
       "    -0.04529225081205368,\n",
       "    0.04271456599235535,\n",
       "    0.042925309389829636,\n",
       "    -0.04442963749170303,\n",
       "    -0.021062510088086128,\n",
       "    -0.012007338926196098,\n",
       "    0.00816300231963396,\n",
       "    -0.017117392271757126,\n",
       "    -0.021404476836323738,\n",
       "    -0.052815791219472885,\n",
       "    -0.026734163984656334,\n",
       "    0.02368505857884884,\n",
       "    -0.01386015210300684,\n",
       "    -0.035785410553216934,\n",
       "    -0.05241508036851883,\n",
       "    0.015150320716202259,\n",
       "    -0.030187560245394707,\n",
       "    -0.0686757043004036,\n",
       "    0.019921008497476578,\n",
       "    0.012889675796031952,\n",
       "    0.07476913928985596,\n",
       "    0.05451134219765663,\n",
       "    0.00487911282107234,\n",
       "    0.038840942084789276,\n",
       "    0.040128663182258606,\n",
       "    0.027717554941773415,\n",
       "    0.01854345202445984,\n",
       "    0.06377103924751282,\n",
       "    0.01502295583486557,\n",
       "    0.0500095933675766,\n",
       "    0.03606262803077698,\n",
       "    -0.058602239936590195,\n",
       "    -0.03868066519498825,\n",
       "    0.050333164632320404,\n",
       "    0.013265864923596382,\n",
       "    0.008068331517279148,\n",
       "    -0.04277640953660011,\n",
       "    -0.0036664188373833895,\n",
       "    -0.03216496855020523,\n",
       "    0.03447549790143967,\n",
       "    -0.019252223894000053,\n",
       "    -0.0005822206730954349,\n",
       "    0.011772396042943,\n",
       "    -0.0037038661539554596,\n",
       "    -0.005708820652216673,\n",
       "    0.04746290668845177,\n",
       "    -0.00955842062830925,\n",
       "    -0.0076475245878100395,\n",
       "    -0.04292896389961243,\n",
       "    0.04344230890274048,\n",
       "    -0.08527366071939468,\n",
       "    0.0497785359621048,\n",
       "    0.026869824156165123,\n",
       "    -0.025574157014489174,\n",
       "    0.0356919951736927,\n",
       "    -0.008642786182463169,\n",
       "    -0.0007606361177749932,\n",
       "    -0.07507435977458954,\n",
       "    0.03665640205144882,\n",
       "    -0.06080729141831398,\n",
       "    -0.029706522822380066,\n",
       "    0.025235919281840324,\n",
       "    0.026009337976574898,\n",
       "    0.0018750026356428862,\n",
       "    -0.029451826587319374,\n",
       "    -0.0316157266497612,\n",
       "    -0.014768223278224468,\n",
       "    -0.002677124459296465,\n",
       "    -0.005711205303668976,\n",
       "    0.019123593345284462,\n",
       "    -0.03745419904589653,\n",
       "    0.036496851593256,\n",
       "    0.014917022548615932,\n",
       "    -0.004522598348557949,\n",
       "    0.029361655935645103,\n",
       "    0.011803930625319481,\n",
       "    -0.029734337702393532,\n",
       "    0.026142939925193787,\n",
       "    -0.00297564803622663,\n",
       "    -0.062430016696453094,\n",
       "    0.04832734167575836,\n",
       "    -0.023526079952716827,\n",
       "    -0.004489198327064514,\n",
       "    0.000660497578792274,\n",
       "    0.0014449575683102012,\n",
       "    0.008570265024900436,\n",
       "    -0.02498866431415081,\n",
       "    0.02113817073404789,\n",
       "    -0.010792085900902748,\n",
       "    0.047801222652196884,\n",
       "    0.011196454986929893,\n",
       "    0.06845764815807343,\n",
       "    -0.005558045580983162,\n",
       "    -0.014752869494259357,\n",
       "    0.012307302094995975,\n",
       "    0.030618388205766678,\n",
       "    0.04385654255747795,\n",
       "    0.06942445784807205,\n",
       "    -0.046954624354839325,\n",
       "    0.008050248958170414,\n",
       "    0.019136954098939896,\n",
       "    -0.028606729581952095,\n",
       "    -0.005985619500279427,\n",
       "    -0.010622737929224968,\n",
       "    0.061261799186468124,\n",
       "    0.028051437810063362,\n",
       "    0.03281163051724434,\n",
       "    -0.04345623403787613,\n",
       "    -0.041708216071128845,\n",
       "    0.02544453553855419,\n",
       "    -0.029679743573069572,\n",
       "    0.06707371771335602,\n",
       "    -0.022707553580403328,\n",
       "    -0.023528695106506348,\n",
       "    -0.02678598277270794,\n",
       "    -0.03095855377614498,\n",
       "    0.01986478455364704,\n",
       "    -0.11373472213745117,\n",
       "    -0.0826740711927414,\n",
       "    -0.0024730649311095476,\n",
       "    0.0739869698882103,\n",
       "    -0.009054135531187057,\n",
       "    0.022903939709067345,\n",
       "    -0.059970371425151825,\n",
       "    -0.0013070210115984082,\n",
       "    -0.022480430081486702,\n",
       "    -0.027496421709656715,\n",
       "    -0.013416122645139694,\n",
       "    -0.01343651581555605,\n",
       "    -0.05695450305938721,\n",
       "    0.001995413564145565,\n",
       "    0.0011926513398066163,\n",
       "    -0.050192173570394516,\n",
       "    0.023252278566360474,\n",
       "    -0.061362624168395996,\n",
       "    0.012927012518048286,\n",
       "    -0.009315337985754013,\n",
       "    0.014207460917532444,\n",
       "    0.0011776197934523225,\n",
       "    0.04675447568297386,\n",
       "    -0.08852622658014297,\n",
       "    0.00955815240740776,\n",
       "    0.02540426515042782,\n",
       "    -0.010172036476433277,\n",
       "    0.013438703492283821,\n",
       "    -0.0003979691828135401,\n",
       "    0.02166524901986122,\n",
       "    0.0038685225881636143,\n",
       "    0.034570902585983276,\n",
       "    0.06609068065881729,\n",
       "    0.06282448768615723,\n",
       "    0.03355367109179497,\n",
       "    0.013769013807177544,\n",
       "    -0.012553057633340359,\n",
       "    0.03862793743610382,\n",
       "    0.005508135538548231,\n",
       "    -0.03999195620417595,\n",
       "    -0.03160218894481659,\n",
       "    0.05585074424743652,\n",
       "    -0.07674337923526764,\n",
       "    0.04896797612309456,\n",
       "    0.040025558322668076,\n",
       "    0.011220492422580719,\n",
       "    0.017830992117524147,\n",
       "    0.003198246005922556,\n",
       "    0.02977447211742401,\n",
       "    -0.011090840213000774,\n",
       "    -0.039699066430330276,\n",
       "    0.002475667279213667,\n",
       "    -0.008875133469700813,\n",
       "    -0.05313372611999512,\n",
       "    0.03525237366557121,\n",
       "    0.11404898762702942,\n",
       "    -0.10628341883420944,\n",
       "    0.03509759530425072,\n",
       "    -0.026259003207087517,\n",
       "    0.006677142810076475,\n",
       "    -0.04096377268433571,\n",
       "    0.012870917096734047,\n",
       "    -0.02316979132592678,\n",
       "    -0.04633148014545441,\n",
       "    0.05985576659440994,\n",
       "    -0.018323279917240143,\n",
       "    -0.029543021693825722,\n",
       "    -0.057406339794397354,\n",
       "    -0.030151303857564926,\n",
       "    -0.024449171498417854,\n",
       "    0.0024237155448645353,\n",
       "    0.036703433841466904,\n",
       "    -0.00929932203143835,\n",
       "    -0.019866064190864563,\n",
       "    0.015292488038539886,\n",
       "    0.019247397780418396,\n",
       "    -0.01259802095592022,\n",
       "    -0.0449029766023159,\n",
       "    0.02822437323629856,\n",
       "    -0.010889442637562752,\n",
       "    -0.010865438729524612,\n",
       "    0.03510375693440437,\n",
       "    0.04369313642382622,\n",
       "    -0.039419595152139664,\n",
       "    0.06398593634366989,\n",
       "    0.013777017593383789,\n",
       "    0.03390997275710106,\n",
       "    0.01844777725636959,\n",
       "    -0.006725012324750423,\n",
       "    -0.019821960479021072,\n",
       "    -0.05506506189703941,\n",
       "    0.0027683984953910112,\n",
       "    0.009525500237941742,\n",
       "    0.008363379165530205,\n",
       "    -0.030127475038170815,\n",
       "    -0.01671650819480419,\n",
       "    0.06392732262611389,\n",
       "    0.033348504453897476,\n",
       "    -0.04905282333493233,\n",
       "    -0.017108185216784477,\n",
       "    -0.044321637600660324,\n",
       "    0.034375615417957306,\n",
       "    -0.017653685063123703,\n",
       "    0.047646794468164444,\n",
       "    -0.015258068218827248,\n",
       "    -0.011203829199075699,\n",
       "    0.03283543512225151,\n",
       "    -0.008147018030285835,\n",
       "    0.03014269843697548,\n",
       "    -0.024112608283758163,\n",
       "    -0.007501413580030203,\n",
       "    0.02338268607854843,\n",
       "    -0.004973664414137602,\n",
       "    0.05554606020450592,\n",
       "    0.06484903395175934,\n",
       "    -0.04565608873963356,\n",
       "    0.04355539008975029,\n",
       "    0.0432501919567585,\n",
       "    0.05304698273539543,\n",
       "    0.009687559679150581,\n",
       "    -0.0006497192662209272,\n",
       "    0.007974175736308098,\n",
       "    -0.048279307782649994,\n",
       "    0.014543289318680763,\n",
       "    -0.0018666688119992614,\n",
       "    0.08193032443523407,\n",
       "    0.017032839357852936,\n",
       "    -0.01825888827443123,\n",
       "    0.0016424690838903189,\n",
       "    -0.09057921916246414,\n",
       "    -0.037285465747117996,\n",
       "    -0.011478252708911896,\n",
       "    -0.026896782219409943,\n",
       "    -0.0010269061895087361,\n",
       "    0.004453262314200401,\n",
       "    0.0512370802462101,\n",
       "    0.01787085086107254,\n",
       "    0.03877165913581848,\n",
       "    0.0022434007842093706,\n",
       "    -0.03722197934985161,\n",
       "    0.0015444193268194795,\n",
       "    -0.00538845919072628,\n",
       "    -0.06145252659916878,\n",
       "    0.02154410444200039,\n",
       "    -0.014493067748844624,\n",
       "    0.028247373178601265,\n",
       "    -0.03272327035665512,\n",
       "    -0.015046266838908195,\n",
       "    -0.008358771912753582,\n",
       "    0.008262124843895435,\n",
       "    -0.028986917808651924,\n",
       "    -0.0497603565454483,\n",
       "    0.03500368073582649,\n",
       "    -0.08809620141983032,\n",
       "    -0.026287762448191643,\n",
       "    0.049603335559368134,\n",
       "    0.018991408869624138,\n",
       "    -0.007039263378828764,\n",
       "    -0.03614218160510063,\n",
       "    -0.04184519127011299,\n",
       "    0.018220486119389534,\n",
       "    0.04309576749801636,\n",
       "    0.002400793135166168,\n",
       "    -0.023632893338799477,\n",
       "    -0.01810850203037262,\n",
       "    0.042681772261857986,\n",
       "    0.032145436853170395,\n",
       "    0.02910034917294979,\n",
       "    -0.0500526949763298,\n",
       "    -0.05331810563802719,\n",
       "    -0.022011155262589455,\n",
       "    -0.04746269807219505,\n",
       "    0.06187107414007187,\n",
       "    0.011436063796281815,\n",
       "    0.039274897426366806,\n",
       "    -0.02904096059501171,\n",
       "    -0.06594076007604599,\n",
       "    -0.012905655428767204,\n",
       "    0.0052593061700463295,\n",
       "    0.011928319931030273,\n",
       "    -0.007644830737262964,\n",
       "    0.006778846029192209,\n",
       "    0.05895879864692688,\n",
       "    -0.02173878438770771,\n",
       "    0.05082307755947113,\n",
       "    0.00016734946984797716,\n",
       "    -0.0002821223170030862,\n",
       "    0.006249383091926575,\n",
       "    -0.052117809653282166,\n",
       "    -0.028158966451883316,\n",
       "    0.005642575677484274,\n",
       "    0.01918194070458412,\n",
       "    -0.042344532907009125,\n",
       "    0.007840354926884174,\n",
       "    -0.016150880604982376,\n",
       "    0.042837709188461304,\n",
       "    0.006729774177074432,\n",
       "    -0.02743992768228054,\n",
       "    -0.01141644362360239,\n",
       "    0.04946492984890938,\n",
       "    0.047539547085762024,\n",
       "    0.01727294921875,\n",
       "    -0.029833465814590454,\n",
       "    -0.0317135751247406,\n",
       "    0.008672365918755531,\n",
       "    -0.01940334402024746,\n",
       "    0.017657006159424782,\n",
       "    0.02296895533800125,\n",
       "    -0.0020701992325484753,\n",
       "    -0.0050049093551933765,\n",
       "    -0.026313990354537964,\n",
       "    0.0517115518450737,\n",
       "    0.06446523219347,\n",
       "    0.0022733553778380156,\n",
       "    0.0018633129075169563,\n",
       "    -0.016328243538737297,\n",
       "    -0.02148253656923771,\n",
       "    -0.004772395361214876,\n",
       "    -0.005436599720269442,\n",
       "    -0.01646612398326397,\n",
       "    -0.03357100859284401,\n",
       "    -0.009827901609241962,\n",
       "    0.053612496703863144,\n",
       "    0.050580620765686035,\n",
       "    0.025444701313972473,\n",
       "    -0.045171529054641724,\n",
       "    -0.020823147147893906,\n",
       "    -0.021790646016597748,\n",
       "    0.008153421804308891,\n",
       "    0.041398175060749054,\n",
       "    -5.729974763732088e-33,\n",
       "    -0.00026729985256679356,\n",
       "    -0.06420250236988068,\n",
       "    0.0856560692191124,\n",
       "    0.017010370269417763,\n",
       "    -0.018181640654802322,\n",
       "    -0.04122904688119888,\n",
       "    0.041310593485832214,\n",
       "    0.02488061971962452,\n",
       "    -0.043646570295095444,\n",
       "    -0.008275746367871761,\n",
       "    0.0032481723465025425,\n",
       "    0.027788110077381134,\n",
       "    0.016718219965696335,\n",
       "    0.017560824751853943,\n",
       "    -0.025264807045459747,\n",
       "    0.04685632884502411,\n",
       "    0.021784355863928795,\n",
       "    0.015050357207655907,\n",
       "    0.011759328655898571,\n",
       "    -0.016550337895751,\n",
       "    -0.14462138712406158,\n",
       "    0.027390118688344955,\n",
       "    -0.02396675944328308,\n",
       "    -0.0341159887611866,\n",
       "    -0.011273411102592945,\n",
       "    -0.028514010831713676,\n",
       "    -0.01986534520983696,\n",
       "    0.00789886899292469,\n",
       "    -0.0348370335996151,\n",
       "    0.03496742993593216,\n",
       "    0.026173781603574753,\n",
       "    -0.031291499733924866,\n",
       "    0.02923251874744892,\n",
       "    -0.021719953045248985,\n",
       "    0.0004337596183177084,\n",
       "    0.01347253005951643,\n",
       "    -0.0661778673529625,\n",
       "    0.014231664128601551,\n",
       "    0.06518945842981339,\n",
       "    0.04887544363737106,\n",
       "    0.0006556006264872849,\n",
       "    -0.04182441905140877,\n",
       "    -0.01821492612361908,\n",
       "    -0.040072232484817505,\n",
       "    0.006025873124599457,\n",
       "    0.0008025417337194085,\n",
       "    0.054838575422763824,\n",
       "    -0.0033194369170814753,\n",
       "    0.03254208341240883,\n",
       "    0.0012072388781234622,\n",
       "    0.0027616890147328377,\n",
       "    0.0061906552873551846,\n",
       "    -0.06341899186372757,\n",
       "    0.04909229278564453,\n",
       "    0.011865561828017235,\n",
       "    0.04893387109041214,\n",
       "    0.011408860795199871,\n",
       "    -0.004910584539175034,\n",
       "    -0.05156150832772255,\n",
       "    0.028925076127052307,\n",
       "    -0.07685288786888123,\n",
       "    -0.022372594103217125,\n",
       "    0.057072896510362625,\n",
       "    0.02287084423005581,\n",
       "    0.017990168184041977,\n",
       "    0.015718061476945877,\n",
       "    0.05094698816537857,\n",
       "    0.01600397564470768,\n",
       "    0.050379619002342224,\n",
       "    0.01903974823653698,\n",
       "    -0.025875896215438843,\n",
       "    0.014788114465773106,\n",
       "    -0.03670822083950043,\n",
       "    0.01832999847829342,\n",
       "    -0.0276804082095623,\n",
       "    -0.0034246279392391443,\n",
       "    0.023648882284760475,\n",
       "    0.011904316022992134,\n",
       "    0.06643778830766678,\n",
       "    -0.011339484713971615,\n",
       "    -0.06692687422037125,\n",
       "    0.04095660522580147,\n",
       "    0.0386740118265152,\n",
       "    0.0014626405900344253,\n",
       "    -0.04116753116250038,\n",
       "    -0.061476413160562515,\n",
       "    0.012443850748240948,\n",
       "    0.026189612224698067,\n",
       "    0.0017588022165000439,\n",
       "    -0.04337722808122635,\n",
       "    -0.024051841348409653,\n",
       "    0.06765501946210861,\n",
       "    0.017763130366802216,\n",
       "    0.013408404774963856,\n",
       "    -0.02155473083257675,\n",
       "    -0.046767912805080414,\n",
       "    -0.011057816445827484,\n",
       "    0.00763482553884387,\n",
       "    0.006030995398759842,\n",
       "    0.06524741649627686,\n",
       "    -0.006747123319655657,\n",
       "    -0.0028311365749686956,\n",
       "    -0.03539561480283737,\n",
       "    0.010686344467103481,\n",
       "    0.017334111034870148,\n",
       "    0.053637273609638214,\n",
       "    -0.05101718008518219,\n",
       "    0.018777847290039062,\n",
       "    0.014122171327471733,\n",
       "    -0.023240774869918823,\n",
       "    -0.002531585982069373,\n",
       "    0.007405618205666542,\n",
       "    -0.0380927175283432,\n",
       "    0.06616596132516861,\n",
       "    -0.034329604357481,\n",
       "    0.027582131326198578,\n",
       "    0.017070148140192032,\n",
       "    0.02230720967054367,\n",
       "    -0.026852255687117577,\n",
       "    -0.054887812584638596,\n",
       "    0.03924591466784477,\n",
       "    -0.06959611922502518,\n",
       "    0.02024713158607483,\n",
       "    -0.02877712994813919,\n",
       "    -0.015024847351014614,\n",
       "    0.0008923560380935669,\n",
       "    -0.014220718294382095,\n",
       "    0.017970502376556396,\n",
       "    0.030941404402256012,\n",
       "    -0.04017666354775429,\n",
       "    0.008337975479662418,\n",
       "    0.05481217801570892,\n",
       "    2.90133954194971e-07,\n",
       "    0.01109637226909399,\n",
       "    -0.011139873415231705,\n",
       "    -0.006980110425502062,\n",
       "    0.05046604201197624,\n",
       "    0.02567274682223797,\n",
       "    -0.025115717202425003,\n",
       "    -0.003204480279237032,\n",
       "    -0.03447434678673744,\n",
       "    -0.012468907982110977,\n",
       "    0.003947559278458357,\n",
       "    -0.011368508450686932,\n",
       "    0.02611328475177288,\n",
       "    -0.029129335656762123,\n",
       "    -0.017078084871172905,\n",
       "    -0.06029406934976578,\n",
       "    -0.09092728048563004,\n",
       "    0.002938011195510626,\n",
       "    -0.024907153099775314,\n",
       "    -0.04012865200638771,\n",
       "    0.041391223669052124,\n",
       "    0.01407963689416647,\n",
       "    0.06657303869724274,\n",
       "    0.0010040142806246877,\n",
       "    0.011603903956711292,\n",
       "    -0.005437854211777449,\n",
       "    0.002068161265924573,\n",
       "    -0.06270793825387955,\n",
       "    -0.01468360424041748,\n",
       "    0.01374070718884468,\n",
       "    0.045714184641838074,\n",
       "    0.0674893781542778,\n",
       "    -0.00343517679721117,\n",
       "    0.03145978972315788,\n",
       "    0.02306610532104969,\n",
       "    -0.016449062153697014,\n",
       "    -0.02722441405057907,\n",
       "    0.004788798745721579,\n",
       "    0.04470475763082504,\n",
       "    -0.017290987074375153,\n",
       "    0.029215456917881966,\n",
       "    0.045464638620615005,\n",
       "    0.016175828874111176,\n",
       "    -0.006507445592433214,\n",
       "    -0.04997609183192253,\n",
       "    0.06347935646772385,\n",
       "    0.004576473031193018,\n",
       "    -0.03486001491546631,\n",
       "    -0.06850578635931015,\n",
       "    -0.028628511354327202,\n",
       "    -0.018780319020152092,\n",
       "    0.0010000040056183934,\n",
       "    0.017576593905687332,\n",
       "    0.027354836463928223,\n",
       "    0.043900344520807266,\n",
       "    0.01956077292561531,\n",
       "    -0.06738484650850296,\n",
       "    -0.04807835817337036,\n",
       "    0.009998329915106297,\n",
       "    0.011013500392436981,\n",
       "    -0.005370414350181818,\n",
       "    -0.009212811477482319,\n",
       "    -0.01640094444155693,\n",
       "    0.018418366089463234,\n",
       "    -0.06540190428495407,\n",
       "    0.07415718585252762,\n",
       "    -0.011686311103403568,\n",
       "    -0.010472569614648819,\n",
       "    2.401950284761308e-34,\n",
       "    -0.022742556408047676,\n",
       "    0.02834090031683445,\n",
       "    -0.03171497955918312,\n",
       "    0.04209112375974655,\n",
       "    0.035772014409303665,\n",
       "    -0.06357941031455994,\n",
       "    -0.05103430151939392,\n",
       "    -0.0006843390874564648,\n",
       "    -0.007011951878666878,\n",
       "    0.030117183923721313,\n",
       "    -0.032156553119421005]}},\n",
       " {'_index': 'ml-interview-questions',\n",
       "  '_id': 'fsXylZIByzqUH6XnClij',\n",
       "  '_score': 1.8674362,\n",
       "  '_source': {'chapter': 'CHAPTER 6',\n",
       "   'title': 'Technical Interview: Model Deployment and End-to-End ML',\n",
       "   'section': 'Additional Technical Interview Components',\n",
       "   'text': 'As you saw in Figure 1-1 , there are additional interview types. Usually, they are more advanced components, assessing candidates on various combinations of ML, coding, training, and deployment (content that has been covered so far in Chapters 3 , 4 , and 5 and this chapter). Additional types that you might commonly hear about are: • Machine learning systems design interview • Technical deep-dive interview • Take-home exercises • Product sense I will briefly go through each of these interview types, so you are aware of how to prepare for them. I personally didn’t need to prepare for these types of interviews when I was looking for my entry-level role, as ML theory and coding were sufficient. However, I have encountered more and more advanced interviews as I progressed to senior and staff+ roles. Each company may ask for only some of these or none at all, so what you encounter in your interviews will differ. For example, Meta asks MLE candidates systems design questions, not just candidates at the “senior” level. ML systems design interviews and questions ask you to design something in an often hypothetical scenario. This could include asking you to design a brand-new system from scratch or how you’d hypothetically design a known system. Examples include: • “Imagine you are part of an ecommerce company’s ML team. It is aiming to use ML to increase customer retention. Walk through your initial approaches and how you would accomplish this.” • “How would you introduce ML-powered restaurant recommendations on Goo‐ gle Maps?” • “The online game our company is developing uses reinforcement learning to improve player experience. How would you design such a system?” ML systems design questions are often open ended, with plenty of back-and-forth with the interviewer asking follow-up questions that they find interesting. ML sys‐ tems design questions can be quite challenging for a couple of reasons: They likely don’t have a 100% correct answer. Since the questions are often about hypothetical scenarios, the questions them‐ selves might also change on the fly. For example, I (as the candidate) might ask the interviewer, “How many daily users are we expecting for this ML system?” Given the same question, by design the interviewer might not have defined all the parameters of the scenario and makes up a plausible number on the spot. Lots of what you do during the ML systems design is merely estimation and back-of-the-envelope math, and there is often no correct tool to choose (e.g., for some scenarios, you could use either XGBoost or CatBoost). ML systems design questions have high variance between each company, team, and interviewer. Much of your performance depends not only on your initial design but also on how you can respond to open-ended questions that could go in any direction. The interviewer could be curious about how you’d deal with the speed of the ML inference, and you could spend another five minutes on that topic. Or, by chance, they might ask instead about how you’d ensure that data quality is high before training the models. Treat it like improv and be able to adjust to how the conversation is flowing between you and your interviewer. Do yourself a favor and check the job posting to see what aspects you should focus on. Even in a scenario where the systems design questions ask you to design an end on. Even in a scenario where the systems design questions ask you to design an endto-end ML project, you can spend more time focusing on the core competencies of the position during the job interview. If you’re interviewing for a data scientist posi‐ tion that trains and evaluates ML models, then elaborate more on that and less on the deployment. Don’t ignore other aspects of the ML system if it’s an end-to-end system question, though. If you’re interviewing for an MLE position that focuses on deploy‐ ment, spend a little more time on that instead of getting stuck in a rabbit hole about data engineering. If in doubt, ask your interviewer if you’re focusing on the right thing and if they’d like to dive deeper on any topic. I won’t provide further examples here, since they would build on and combine the information from the ML algorithm, ML evaluation, ML deployment, and coding interviews that we’ve already discussed in this book. For entry-level roles, if there are systems design questions, they will focus on skills that have been covered in the pre‐ vious chapters. The most advanced systems design questions are mostly reserved for more senior and staff+ roles. For greater depth on this subject, I recommend the following resources: • “ML Systems Design Interview Guide” by Patrick Halina • Machine Learning System Design Interview by Ali Aminian and Alex Xu (ByteByteGo) • Search YouTube videos on example system design interviews for ML; this is a good example: “Harmful Content Removal: Machine Learning (System Design) Staff Level Mentorship” by Interviewing.io. (This question is aimed at the L7 staff position.) At Meta, example questions include “design a personalized news ranking system,” “design a product recommendation system,” and so on. As you can see from the fol‐ lowing breakdown, Meta is looking for an accumulation of the skills discussed in this book, not just a narrow subsection of them. The signals the interviewer is looking for from candidates include: Problem navigation Seeing if the candidate can organize the entire problem. Meta’s interview prep guide highlights that candidates should connect the problem to the business con‐ text. (Refer to Chapter 4 and the section “Product Sense” on page 214 .) Training data How would you collect training data and evaluate the risks? (Refer to Chapter 4 .) Feature engineering How would you come up with relevant features for the ML task? (Refer to Chapter 4 .) Modeling How do you justify choosing a specific model? Explain the training process, and explain the risks and how you’d mitigate them. (Refer to Chapters 3 and 4 .) Evaluation and deployment How do you evaluate and deploy the model? How do you justify which metrics to monitor? (Refer to Chapters 4 and 6 .) You can read Meta’s official resource in its software engineer ML full loop interview prep guide , which can be found on its careers site . In Meta’s interview prep guide, you’ll see repeated mentions of expecting candidates to come up with potential risks and mitigations for the ML designs they propose. This is a useful pattern of thinking for all ML interviews and a sign of a more effective and thoughtful ML practitioner. A useful and important way to improve your discus‐ sion of possible risks is to read about AI biases because they are a big part of risks. Research from Timnit Gebru and Joy Buolamwini are good resources; for example, they investigate ML algorithms’ accuracy disparities on gender and race (via skin type). 19 Meta’s own blog on progress and learnings in AI fairness and transparency also mentions various risks and mitigations. 20 Meta’s efforts include creating more datasets to “help researchers evaluate their computer vision and audio models for accuracy across a diverse set of ages, genders, skin tones, and ambient lighting conditions.” Technical deep-dive questions allow you to walk through something you’ve designed and built from scratch in the past, discussing the trade-offs and challenges you encountered along the way and how you addressed them. I have frequently seen this type of question grouped under behavioral questions that are related to past projects; for example, Shopify places a large emphasis on technical deep dives in its technical There are many companies that do this type of interview, and I’ve heard it called many names: case study interviews (different from the consulting type of case studies), reverse systems design, retro‐ spective systems design, and whatnot. I’m borrowing Shopify’s term, technical  deep dive , to refer to this kind of interview and interview questions in this book. Depending on the interview phase and the interviewer, answering this type of ques‐ tion can require more technical explanation and a deeper dive than the usual behav‐ ioral interview, and it has the depth and volleying that systems design has. However, it differs from the regular systems design questions with plenty of preparation mate‐ rial online since those questions focus on hypothetical situations instead of some‐ thing you actually built in a former job or project. Anecdotally, the more senior I get, the more questions I have gotten that are of the technical deep-dive variant. Here are important things to keep in mind while answering technical deep-dive questions: Justify and understand trade-offs in the system. Why did you choose BERT-cased over BERT-uncased? Why did you choose a basic Q-learning algorithm over a deep Q-network (DQN), or the other way around? Bring up analyses or benchmarks you ran during the time and be pre‐ pared to use them to justify your past choices. Deeply understand the main components you were responsible for. If you were responsible for training the model, be prepared to answer questions on the inner workings or even the mathematical foundations of those algorithms. For example, I’ve been asked how matrix factorization works, stemming from my project that used matrix factorization in the context of collaborative filtering. If you were responsible for the deployment infrastructure, such as in an MLE role, be prepared to answer more details on the Ops side. Sometimes, companies will provide an exercise or assessment for the candidate to do at home. These might be graded automatically, and a candidate would be passed or failed. There are also open-ended take-home exercises where the goal isn’t to pass or fail the candidate by the exercise alone but to combine it with an interview discussion where the candidate walks the interviewers through their solutions. The tips for ML algorithms and coding from previous chapters all apply: • Make sure you can not only explain the algorithms but also the trade-offs as well as why and how you decided on your approach. • Explain your thought process clearly to interviewers with docstrings in the code as well as verbally during the interviews. • Write tests! In data science and ML interviews, especially in big tech, a hidden requirement is that candidates possess some “product sense.” This is an umbrella term some companies use to describe whether a job candidate has practical knowledge on how ML benefits the company’s products. This can be shown when speaking about ML products and when you research the company’s products. It’s important to understand common product objectives for ML, such as: • Increase user convenience • Decrease user churn • Improve onboarding experience This is becoming more well-known nowadays; if you look up “data science product sense” on a search engine, some guides will show up. However, many candidates don’t think about preparing for this unless it’s explicitly mentioned by the recruiter or during the hiring process. As an ML candidate, you can integrate product sense into your behavioral interviews, systems design interviews, technical deep dives, and so on. The way you would prepare is to borrow from product manager interviews. From an interviewer’s perspective, I think of it this way. Does the candidate care only about model accuracy metrics, or do they also care about monthly average users of the product? Do they connect the ML they’re building to the product? Don’t underestimate this—when I was starting out in the ML field, much more expe‐ rienced people and successful peers recommended that I learn more about and understand the business side. This is one way that mentorship benefited my career: there is a lot of information that’s not shared in question-bank-type interview guides. In turn, I have included as much of that latent information as possible in this book. Here are some resources to get started: • “The Ultimate Guide to Cracking Product Case Interviews for Data Scientists” (Part 1) by Emma Ding • Exponent videos on product sense, such as this “Meta/Facebook Product Man‐ ager Mock Interview” • Cracking the PM Interview by Gayle Laakmann McDowell and Jackie Bavaro (CareerCup) Here are some interview questions I’ve used to interview MLOps engineers and MLEs who work on infrastructure. These interview questions include sample answers to help provide inspiration for your own potential responses. I want to point out that these questions mostly hinge on asking about your experience; most likely, the MLOps engineer and MLE will share the core coding interview loop ( Chapter 5 ) with other roles, and then the resume walk-through and technical deep-dive questions will include questions such as the ones I’ve given here. As I mentioned in Chapter 5 , those roles that are more focused on operations might also have more specialized coding questions that are similar to questions asked of DevOps engineers. At the risk of repeating myself too much, it is best that you double-check the job posting and with your recruiter and hiring manager, if possible, on the focus and expectations of the interview. For this chapter, it’s especially important to note that your answers will differ depending on your own experience; these answers are only high-level, relatively generic examples to show you what an answer might look like. Do not use them as a real answer in your interviews unless you’ve done the tasks/projects mentioned in the example answers. Example answer Using scaling on Kubernetes helped; for example, horizontal scaling helped dis‐ tribute the same workload across more instances. In cases when the heavy load came from request volume, I used load balancing with the Google Kubernetes Engine. In the past, I’ve used autoscaling features in cloud platforms, such as when I was working with GCP. Example answer For machine learning, I’ve learned that what’s different between monitoring an ML application in production as opposed to an application without ML is the data and model-related monitoring. This includes monitoring for data drift, model accuracy and drifts, and so on. For this, I use tools such as Great Expecta‐ tions or Alibi Detect . In particular, at my previous company we used Great Expectations to check for sudden large amounts of missing values or distribution shifts. In addition, using those monitoring tools, I can create alerts and have recurring anomaly-detection jobs on those platforms to report errors or drifts. On the ser‐ vice availability side (more general, less ML specific), tools such as Grafana, ELK Stack (Elasticsearch, Logstash, and Kibana, aka Elastic Stack), and Prometheus are commonly used. Example answer I started by automating the steps involved in the ML pipeline, such as tidying up the scripts for data preprocessing, model training, and evaluation. Then I inte‐ grated those steps into a CI/CD pipeline with Jenkins , triggering a pipeline run when there are changes to the code on our GitHub repo. This pipeline includes spinning up the environment, code linting, and testing, followed by automatic model deployment to a staging environment for further testing. Upon successful validation, the model is copied over to the production environment. These steps automated the deployment process, saving on manual deployment time and also allowing for quality control.In this chapter, I walked through the ML roles who might be interviewed on special‐ ized knowledge of their operations and infrastructure experience. Next, I discussed some levels and examples of end-to-end machine learning, which differ depending on the size and maturity of the data team. You also read about different types of cloud environments, trade-offs between private and public cloud, and common tools for ML model deployment and orchestration. I then discussed some model monitoring setups once the model has been deployed. I gave a brief overview of the popular cloud providers, noting that most cloud provid‐ ers have a similar toolkit, and if you use vendors that are not mentioned in this book, you’ll likely still find the equivalent services (just named something else). I brought up some developer best practices that are foundational for operations and software-heavy roles. Many of these things are naturally learned with work experi‐ ence, but for folks with school or bootcamp experience, you can still demonstrate these skills with group projects or open source contributions. Finally, I covered additional types of interviews for ML roles, such as systems design, technical deep dive, and product sense. In the next chapter, we’ll move onto behavio‐ ral interviews and how to succeed at them.',\n",
       "   'text_vector': [0.0877719521522522,\n",
       "    -0.08375181257724762,\n",
       "    -0.0501425601541996,\n",
       "    0.005115136504173279,\n",
       "    0.0027038180269300938,\n",
       "    0.005148849915713072,\n",
       "    0.010910162702202797,\n",
       "    -0.03839466720819473,\n",
       "    8.397534838877618e-05,\n",
       "    -0.05964382365345955,\n",
       "    0.03671305254101753,\n",
       "    0.009345118887722492,\n",
       "    0.02155778929591179,\n",
       "    0.05351285636425018,\n",
       "    0.02447841316461563,\n",
       "    -0.05207628756761551,\n",
       "    0.03418038412928581,\n",
       "    -0.0097039258107543,\n",
       "    -0.04105781391263008,\n",
       "    -0.014344848692417145,\n",
       "    -0.030378226190805435,\n",
       "    0.01931382715702057,\n",
       "    -0.019137533381581306,\n",
       "    0.011896921321749687,\n",
       "    -0.003281181212514639,\n",
       "    -0.027767043560743332,\n",
       "    -0.04999121278524399,\n",
       "    0.04829849675297737,\n",
       "    -0.007479683496057987,\n",
       "    -0.023231005296111107,\n",
       "    -0.02418570965528488,\n",
       "    -0.052168864756822586,\n",
       "    0.04908134043216705,\n",
       "    -0.017405888065695763,\n",
       "    2.1647210814990103e-06,\n",
       "    -0.035969749093055725,\n",
       "    -0.00933582428842783,\n",
       "    -0.00582002243027091,\n",
       "    -0.05814642459154129,\n",
       "    -0.1081121563911438,\n",
       "    0.00823161844164133,\n",
       "    0.02122017927467823,\n",
       "    0.05289062485098839,\n",
       "    0.045589566230773926,\n",
       "    -0.005546514876186848,\n",
       "    -0.022577673196792603,\n",
       "    0.05440079793334007,\n",
       "    0.009253495372831821,\n",
       "    0.00862368568778038,\n",
       "    0.04581226035952568,\n",
       "    -0.0020751238334923983,\n",
       "    -0.06611599773168564,\n",
       "    -0.006357113365083933,\n",
       "    0.03970426321029663,\n",
       "    -0.024106480181217194,\n",
       "    -0.02931840904057026,\n",
       "    0.0008140099816955626,\n",
       "    0.0022882407065480947,\n",
       "    0.017726033926010132,\n",
       "    -0.032369762659072876,\n",
       "    0.042034924030303955,\n",
       "    -0.03404513746500015,\n",
       "    0.02458380162715912,\n",
       "    0.023335346952080727,\n",
       "    0.0385381244122982,\n",
       "    -0.00021901197033002973,\n",
       "    0.03462313488125801,\n",
       "    -0.026068486273288727,\n",
       "    -0.0378149189054966,\n",
       "    -0.004129456356167793,\n",
       "    0.06112721562385559,\n",
       "    0.014663971029222012,\n",
       "    -0.05448748171329498,\n",
       "    -0.006068816874176264,\n",
       "    0.056835971772670746,\n",
       "    -0.016040293499827385,\n",
       "    0.01335164811462164,\n",
       "    -0.007176683284342289,\n",
       "    -0.01766805537045002,\n",
       "    -0.03642319142818451,\n",
       "    -0.04412237927317619,\n",
       "    0.03708980977535248,\n",
       "    -0.03538363054394722,\n",
       "    -0.01712329126894474,\n",
       "    -0.02549702301621437,\n",
       "    0.09942641854286194,\n",
       "    -0.015058265998959541,\n",
       "    -0.014030874706804752,\n",
       "    0.04562992602586746,\n",
       "    -0.000324458465911448,\n",
       "    0.098841592669487,\n",
       "    0.004286894109100103,\n",
       "    0.0032782393973320723,\n",
       "    0.051547519862651825,\n",
       "    -0.04973319172859192,\n",
       "    0.008162043057382107,\n",
       "    0.017563020810484886,\n",
       "    0.06350969523191452,\n",
       "    -0.028029482811689377,\n",
       "    -0.0454261414706707,\n",
       "    -0.04325554892420769,\n",
       "    -0.045101359486579895,\n",
       "    0.05247748643159866,\n",
       "    0.00022804419859312475,\n",
       "    0.07110686600208282,\n",
       "    0.00835905596613884,\n",
       "    0.03146175667643547,\n",
       "    -0.04937789961695671,\n",
       "    -0.09072843939065933,\n",
       "    -0.06304611265659332,\n",
       "    0.015489882789552212,\n",
       "    -0.017029747366905212,\n",
       "    -0.05380034074187279,\n",
       "    0.0371389240026474,\n",
       "    -0.017206134274601936,\n",
       "    0.010526447556912899,\n",
       "    0.007473659235984087,\n",
       "    0.022225722670555115,\n",
       "    0.024433894082903862,\n",
       "    -0.02629227377474308,\n",
       "    -0.07095913589000702,\n",
       "    0.027495786547660828,\n",
       "    0.04098043590784073,\n",
       "    0.0006975952419452369,\n",
       "    -0.056004006415605545,\n",
       "    -0.06435919553041458,\n",
       "    -0.032526008784770966,\n",
       "    0.03436129912734032,\n",
       "    0.010748692788183689,\n",
       "    -0.045127201825380325,\n",
       "    0.054371584206819534,\n",
       "    0.04116837680339813,\n",
       "    -0.031776510179042816,\n",
       "    -0.05595490708947182,\n",
       "    0.04529226943850517,\n",
       "    0.01527219358831644,\n",
       "    -0.012498327530920506,\n",
       "    0.028787905350327492,\n",
       "    -0.05636657029390335,\n",
       "    -0.0025440852623432875,\n",
       "    0.0274649728089571,\n",
       "    -0.06131197139620781,\n",
       "    0.03376685827970505,\n",
       "    0.02923615276813507,\n",
       "    0.006653798744082451,\n",
       "    -0.01331252045929432,\n",
       "    -0.01472800225019455,\n",
       "    0.011102113872766495,\n",
       "    0.007453437894582748,\n",
       "    0.035278186202049255,\n",
       "    -0.031861573457717896,\n",
       "    0.02942207083106041,\n",
       "    0.045145731419324875,\n",
       "    0.011620720848441124,\n",
       "    0.06524604558944702,\n",
       "    -0.004013457801192999,\n",
       "    0.005459158215671778,\n",
       "    0.05006375163793564,\n",
       "    0.020501038059592247,\n",
       "    0.041867371648550034,\n",
       "    0.027822844684123993,\n",
       "    -0.08768259733915329,\n",
       "    0.04008650407195091,\n",
       "    0.07676234096288681,\n",
       "    -0.01750451885163784,\n",
       "    -0.021026095375418663,\n",
       "    0.006556606851518154,\n",
       "    0.06923415511846542,\n",
       "    -0.06997709721326828,\n",
       "    0.029488809406757355,\n",
       "    0.038079313933849335,\n",
       "    0.025104012340307236,\n",
       "    0.019518891349434853,\n",
       "    -0.02146107889711857,\n",
       "    -0.017913818359375,\n",
       "    0.027172857895493507,\n",
       "    -0.016976017504930496,\n",
       "    -0.011034868657588959,\n",
       "    0.059191759675741196,\n",
       "    0.042745307087898254,\n",
       "    0.03567175939679146,\n",
       "    0.050022851675748825,\n",
       "    0.007251773960888386,\n",
       "    0.008524062111973763,\n",
       "    -0.016230542212724686,\n",
       "    -0.035236746072769165,\n",
       "    -0.07932170480489731,\n",
       "    0.040413789451122284,\n",
       "    -0.035338569432497025,\n",
       "    0.011129406280815601,\n",
       "    0.0238513071089983,\n",
       "    0.04693474620580673,\n",
       "    0.0025238022208213806,\n",
       "    0.06157830357551575,\n",
       "    -0.039135947823524475,\n",
       "    -0.01904986798763275,\n",
       "    -0.05703214183449745,\n",
       "    0.00613901624456048,\n",
       "    -0.07746805250644684,\n",
       "    -0.006095858756452799,\n",
       "    -0.05534346401691437,\n",
       "    -0.037369050085544586,\n",
       "    -0.06874975562095642,\n",
       "    0.00844718050211668,\n",
       "    0.047717638313770294,\n",
       "    0.012483634985983372,\n",
       "    -0.021609490737318993,\n",
       "    -0.04860683158040047,\n",
       "    -0.08158981055021286,\n",
       "    0.03201852738857269,\n",
       "    0.09140489250421524,\n",
       "    -0.013869940303266048,\n",
       "    -0.027096521109342575,\n",
       "    -0.001019268180243671,\n",
       "    -0.010671346448361874,\n",
       "    -0.016545161604881287,\n",
       "    -0.010666371323168278,\n",
       "    -0.05442975088953972,\n",
       "    -0.02406987175345421,\n",
       "    -0.01480260118842125,\n",
       "    -0.009907154366374016,\n",
       "    -0.02917368896305561,\n",
       "    -0.050190068781375885,\n",
       "    0.006738487631082535,\n",
       "    -0.013921075500547886,\n",
       "    -0.0312171783298254,\n",
       "    0.0041853743605315685,\n",
       "    0.013622128404676914,\n",
       "    0.03597233444452286,\n",
       "    0.060321144759655,\n",
       "    0.022121304646134377,\n",
       "    0.03621219843626022,\n",
       "    0.020047631114721298,\n",
       "    0.009108793921768665,\n",
       "    0.03754642605781555,\n",
       "    0.031732019037008286,\n",
       "    -0.010995049960911274,\n",
       "    0.05007832124829292,\n",
       "    -0.0012253044405952096,\n",
       "    -0.014316687360405922,\n",
       "    -0.08324581384658813,\n",
       "    0.035149622708559036,\n",
       "    0.008972675539553165,\n",
       "    0.005360947921872139,\n",
       "    -0.018567759543657303,\n",
       "    -0.011370302177965641,\n",
       "    -0.014912763610482216,\n",
       "    0.017180170863866806,\n",
       "    -0.02370661310851574,\n",
       "    0.00025040507898665965,\n",
       "    0.0034878237638622522,\n",
       "    0.00030522668384946883,\n",
       "    0.0019783915486186743,\n",
       "    0.026341453194618225,\n",
       "    -0.019210467115044594,\n",
       "    0.01612047292292118,\n",
       "    -0.021115615963935852,\n",
       "    0.04558586701750755,\n",
       "    -0.07641154527664185,\n",
       "    0.008711841888725758,\n",
       "    0.02228134498000145,\n",
       "    -0.017993247136473656,\n",
       "    0.0008632284589111805,\n",
       "    0.023815499618649483,\n",
       "    -0.03619178757071495,\n",
       "    -0.06028334051370621,\n",
       "    0.00972173921763897,\n",
       "    -0.05029665306210518,\n",
       "    0.005944718606770039,\n",
       "    -0.007742505054920912,\n",
       "    0.04196539521217346,\n",
       "    0.0017450143350288272,\n",
       "    -0.009460528381168842,\n",
       "    -0.015683935955166817,\n",
       "    -0.012752466835081577,\n",
       "    -0.03710882365703583,\n",
       "    -0.005845879204571247,\n",
       "    0.025410600006580353,\n",
       "    0.00551726995036006,\n",
       "    0.026340195909142494,\n",
       "    0.03827199712395668,\n",
       "    0.018087679520249367,\n",
       "    0.028395645320415497,\n",
       "    -0.006349383853375912,\n",
       "    -0.02870037779211998,\n",
       "    0.03312181308865547,\n",
       "    0.007984671741724014,\n",
       "    -0.013020954094827175,\n",
       "    0.019246065989136696,\n",
       "    -0.00968374963849783,\n",
       "    -0.005676678381860256,\n",
       "    0.004679122008383274,\n",
       "    0.007766437251120806,\n",
       "    0.0052763414569199085,\n",
       "    0.007337580900639296,\n",
       "    0.03076355531811714,\n",
       "    -0.025835275650024414,\n",
       "    0.08670858293771744,\n",
       "    0.015759458765387535,\n",
       "    0.07776390761137009,\n",
       "    -0.03926391899585724,\n",
       "    0.005395023617893457,\n",
       "    0.02921813353896141,\n",
       "    0.04233518987894058,\n",
       "    0.03835292160511017,\n",
       "    0.03431721776723862,\n",
       "    -0.05883993208408356,\n",
       "    0.0071153799071908,\n",
       "    0.021630289033055305,\n",
       "    -0.0018368458840996027,\n",
       "    -0.005566491279751062,\n",
       "    -0.013147211633622646,\n",
       "    0.030229879543185234,\n",
       "    0.06536294519901276,\n",
       "    0.04580308496952057,\n",
       "    -0.06558754295110703,\n",
       "    -0.05094953626394272,\n",
       "    0.01833452470600605,\n",
       "    -0.03288126736879349,\n",
       "    0.05285375192761421,\n",
       "    -0.02245856449007988,\n",
       "    -0.03275509178638458,\n",
       "    -0.0007817437872290611,\n",
       "    -0.022248592227697372,\n",
       "    -0.01502246130257845,\n",
       "    -0.10744395107030869,\n",
       "    -0.06720712780952454,\n",
       "    0.017528310418128967,\n",
       "    0.06783626973628998,\n",
       "    -0.014878630638122559,\n",
       "    0.046051621437072754,\n",
       "    0.003655011998489499,\n",
       "    -0.015519320033490658,\n",
       "    -0.0491163544356823,\n",
       "    -0.02986539527773857,\n",
       "    -0.03332182765007019,\n",
       "    -0.0274532251060009,\n",
       "    -0.03649435192346573,\n",
       "    -0.009537195786833763,\n",
       "    -0.022913502529263496,\n",
       "    -0.051937077194452286,\n",
       "    -0.015467251650989056,\n",
       "    -0.05455983430147171,\n",
       "    0.02858954295516014,\n",
       "    0.01743532344698906,\n",
       "    0.019493823871016502,\n",
       "    0.012415144592523575,\n",
       "    0.046370796859264374,\n",
       "    -0.079733707010746,\n",
       "    -0.009292739443480968,\n",
       "    0.025133473798632622,\n",
       "    -0.025750191882252693,\n",
       "    -0.003581745084375143,\n",
       "    0.004316915757954121,\n",
       "    0.0004332583339419216,\n",
       "    0.02006867527961731,\n",
       "    0.0628747045993805,\n",
       "    0.05825426056981087,\n",
       "    0.05396519973874092,\n",
       "    0.03696366026997566,\n",
       "    0.025259999558329582,\n",
       "    -0.01029710378497839,\n",
       "    0.025949452072381973,\n",
       "    -0.022065335884690285,\n",
       "    -0.032364942133426666,\n",
       "    -0.014872444793581963,\n",
       "    0.04053383693099022,\n",
       "    -0.05054384842514992,\n",
       "    0.060092926025390625,\n",
       "    0.04115308076143265,\n",
       "    0.0011388130951672792,\n",
       "    -0.01426150742918253,\n",
       "    0.05014992132782936,\n",
       "    0.010118888691067696,\n",
       "    0.002162081655114889,\n",
       "    -0.020532269030809402,\n",
       "    -0.007595911156386137,\n",
       "    0.01185133121907711,\n",
       "    -0.03202664852142334,\n",
       "    0.01762142963707447,\n",
       "    0.06827501952648163,\n",
       "    -0.10169290006160736,\n",
       "    0.022190168499946594,\n",
       "    0.004094117321074009,\n",
       "    0.005785009823739529,\n",
       "    -0.05228949338197708,\n",
       "    0.006069527473300695,\n",
       "    -0.030134225264191628,\n",
       "    -0.05700507014989853,\n",
       "    0.0520537905395031,\n",
       "    -0.026826055720448494,\n",
       "    -0.04647799953818321,\n",
       "    -0.07170354574918747,\n",
       "    0.019075827673077583,\n",
       "    -0.009120067581534386,\n",
       "    0.004491837695240974,\n",
       "    0.07387121021747589,\n",
       "    0.024614250287413597,\n",
       "    0.0009671174921095371,\n",
       "    -2.86480371869402e-05,\n",
       "    0.016142379492521286,\n",
       "    -0.04783876985311508,\n",
       "    -0.021326443180441856,\n",
       "    0.023044057190418243,\n",
       "    -0.0072280955500900745,\n",
       "    -0.01855613850057125,\n",
       "    0.015867505222558975,\n",
       "    -0.004702341742813587,\n",
       "    -0.04197035729885101,\n",
       "    0.0688064694404602,\n",
       "    0.015320531092584133,\n",
       "    0.013604916632175446,\n",
       "    0.05987999588251114,\n",
       "    0.02267652004957199,\n",
       "    -0.028562933206558228,\n",
       "    -0.031331926584243774,\n",
       "    0.02908306196331978,\n",
       "    -0.009353908710181713,\n",
       "    0.04153399169445038,\n",
       "    0.020765071734786034,\n",
       "    0.039061110466718674,\n",
       "    0.04664807766675949,\n",
       "    0.0054764170199632645,\n",
       "    -0.08398368209600449,\n",
       "    -0.03571551293134689,\n",
       "    -0.026984361931681633,\n",
       "    0.012691691517829895,\n",
       "    -0.01853780262172222,\n",
       "    0.055936407297849655,\n",
       "    -0.0009094970300793648,\n",
       "    -0.04523822292685509,\n",
       "    0.026987656950950623,\n",
       "    -0.019494885578751564,\n",
       "    0.016316931694746017,\n",
       "    -0.010511077009141445,\n",
       "    0.0066840858198702335,\n",
       "    0.027610499411821365,\n",
       "    -0.005279218778014183,\n",
       "    0.08196146786212921,\n",
       "    0.05578026920557022,\n",
       "    -0.055152710527181625,\n",
       "    0.03798368200659752,\n",
       "    -0.022074948996305466,\n",
       "    0.02169179543852806,\n",
       "    -0.020238520577549934,\n",
       "    0.0003577028401196003,\n",
       "    0.002916306257247925,\n",
       "    0.0017585363239049911,\n",
       "    0.03834381699562073,\n",
       "    0.025865741074085236,\n",
       "    0.05202595889568329,\n",
       "    0.04108660668134689,\n",
       "    -0.04632623493671417,\n",
       "    0.01044605951756239,\n",
       "    -0.08963106572628021,\n",
       "    -0.04111179709434509,\n",
       "    -0.03489994630217552,\n",
       "    0.014223912730813026,\n",
       "    -0.005292695481330156,\n",
       "    -0.011010546237230301,\n",
       "    0.032100383192300797,\n",
       "    0.007915418595075607,\n",
       "    0.0341762937605381,\n",
       "    -0.005916154012084007,\n",
       "    -0.05532459169626236,\n",
       "    -0.007663007825613022,\n",
       "    -0.046707961708307266,\n",
       "    -0.012006045319139957,\n",
       "    -0.011218861676752567,\n",
       "    -0.010135800577700138,\n",
       "    0.041232991963624954,\n",
       "    -0.04691703990101814,\n",
       "    -0.028961313888430595,\n",
       "    -0.013283505104482174,\n",
       "    0.02483195811510086,\n",
       "    -0.01840541511774063,\n",
       "    -0.054265640676021576,\n",
       "    0.029792169108986855,\n",
       "    -0.10233340412378311,\n",
       "    -0.008403551764786243,\n",
       "    0.031114384531974792,\n",
       "    0.021008500829339027,\n",
       "    0.005986567586660385,\n",
       "    -0.02415047585964203,\n",
       "    -0.009126827120780945,\n",
       "    0.039744582027196884,\n",
       "    0.0475134514272213,\n",
       "    -0.011591749265789986,\n",
       "    -0.02307402342557907,\n",
       "    -0.07446125894784927,\n",
       "    0.005963355768471956,\n",
       "    0.013945251703262329,\n",
       "    0.04059229791164398,\n",
       "    -0.04729184880852699,\n",
       "    -0.042530134320259094,\n",
       "    0.0017716772854328156,\n",
       "    -0.023339612409472466,\n",
       "    0.05457586050033569,\n",
       "    0.04722639173269272,\n",
       "    -0.01369121391326189,\n",
       "    -0.0254917424172163,\n",
       "    -0.04648197814822197,\n",
       "    -0.005776968318969011,\n",
       "    0.016063431277871132,\n",
       "    0.0033751672599464655,\n",
       "    0.0005948350299149752,\n",
       "    0.02254137583076954,\n",
       "    0.051394570618867874,\n",
       "    -0.01333203911781311,\n",
       "    0.03692026063799858,\n",
       "    -0.014871279709041119,\n",
       "    0.022583601996302605,\n",
       "    0.01768702082335949,\n",
       "    -0.05787982791662216,\n",
       "    0.021544480696320534,\n",
       "    -0.02698727883398533,\n",
       "    0.01420401968061924,\n",
       "    -0.029947608709335327,\n",
       "    -0.03968001529574394,\n",
       "    0.024145204573869705,\n",
       "    0.036251816898584366,\n",
       "    -0.0019295180682092905,\n",
       "    -0.0760388970375061,\n",
       "    0.019813362509012222,\n",
       "    0.05926527827978134,\n",
       "    0.03810734301805496,\n",
       "    0.013582791201770306,\n",
       "    -0.03941980004310608,\n",
       "    -0.03585868328809738,\n",
       "    0.0190116073936224,\n",
       "    -0.0024888734333217144,\n",
       "    0.052324652671813965,\n",
       "    -0.0068313321098685265,\n",
       "    0.006074539851397276,\n",
       "    0.017832424491643906,\n",
       "    -0.0062890080735087395,\n",
       "    0.08737587183713913,\n",
       "    0.0923759937286377,\n",
       "    0.004471426829695702,\n",
       "    -0.02252783253788948,\n",
       "    -0.043612632900476456,\n",
       "    -0.02396148070693016,\n",
       "    -0.015572178177535534,\n",
       "    -0.04152102395892143,\n",
       "    0.018718594685196877,\n",
       "    -0.026386315003037453,\n",
       "    0.003867242718115449,\n",
       "    0.07171211391687393,\n",
       "    0.009516986086964607,\n",
       "    -0.009979409165680408,\n",
       "    -0.04296579211950302,\n",
       "    -0.016799304634332657,\n",
       "    -0.04021388292312622,\n",
       "    0.0021443055011332035,\n",
       "    -0.002892598044127226,\n",
       "    -5.508549464261586e-33,\n",
       "    0.025331396609544754,\n",
       "    -0.04188092052936554,\n",
       "    0.0904230922460556,\n",
       "    0.01448393240571022,\n",
       "    -0.00202311878092587,\n",
       "    -0.043997786939144135,\n",
       "    0.047551337629556656,\n",
       "    0.031440041959285736,\n",
       "    -0.07042771577835083,\n",
       "    -0.004568654578179121,\n",
       "    -0.002213320229202509,\n",
       "    -0.010382622480392456,\n",
       "    -0.01846189983189106,\n",
       "    0.026821650564670563,\n",
       "    -0.05216441676020622,\n",
       "    0.002523543080314994,\n",
       "    0.0214681439101696,\n",
       "    -0.007192379329353571,\n",
       "    -0.010258281603455544,\n",
       "    0.008858480490744114,\n",
       "    -0.10673446953296661,\n",
       "    -0.0023585064336657524,\n",
       "    -0.03774610906839371,\n",
       "    -0.026542993262410164,\n",
       "    0.025505919009447098,\n",
       "    -0.03273367881774902,\n",
       "    0.006160743068903685,\n",
       "    0.01022045873105526,\n",
       "    -0.029087211936712265,\n",
       "    0.022733086720108986,\n",
       "    0.028488697484135628,\n",
       "    0.006026833783835173,\n",
       "    0.02482403814792633,\n",
       "    -0.054447513073682785,\n",
       "    0.01656915992498398,\n",
       "    0.041406676173210144,\n",
       "    -0.029902726411819458,\n",
       "    0.025792276486754417,\n",
       "    0.06392854452133179,\n",
       "    0.0371154248714447,\n",
       "    -0.0009967662626877427,\n",
       "    -0.02504822425544262,\n",
       "    0.0014000646770000458,\n",
       "    -0.0208686962723732,\n",
       "    0.00997875351458788,\n",
       "    -0.012188326567411423,\n",
       "    0.021045677363872528,\n",
       "    -0.015824897214770317,\n",
       "    0.050205670297145844,\n",
       "    -0.02369454875588417,\n",
       "    0.0420810766518116,\n",
       "    -0.011991607025265694,\n",
       "    -0.06733092665672302,\n",
       "    0.03535638377070427,\n",
       "    0.015512754209339619,\n",
       "    0.08582081645727158,\n",
       "    -0.010113624855875969,\n",
       "    0.008475503884255886,\n",
       "    0.016617348417639732,\n",
       "    0.035937853157520294,\n",
       "    -0.034055132418870926,\n",
       "    -0.030856549739837646,\n",
       "    0.021509286016225815,\n",
       "    0.005343557335436344,\n",
       "    0.009163189679384232,\n",
       "    -0.030557909980416298,\n",
       "    0.04012126103043556,\n",
       "    -0.00605811970308423,\n",
       "    0.012193353846669197,\n",
       "    0.004915120080113411,\n",
       "    -0.03283918648958206,\n",
       "    0.05308857932686806,\n",
       "    -0.001370881567709148,\n",
       "    0.02460012584924698,\n",
       "    -0.027145761996507645,\n",
       "    0.01385081373155117,\n",
       "    0.03548340126872063,\n",
       "    0.025515379384160042,\n",
       "    0.038588084280490875,\n",
       "    -0.014323340728878975,\n",
       "    -0.062305402010679245,\n",
       "    0.028944946825504303,\n",
       "    0.023189719766378403,\n",
       "    -0.01581648178398609,\n",
       "    -0.04298840090632439,\n",
       "    -0.030032146722078323,\n",
       "    -0.009261779487133026,\n",
       "    0.007112881634384394,\n",
       "    -0.0013164440169930458,\n",
       "    -0.05768490210175514,\n",
       "    -0.025352677330374718,\n",
       "    0.03281859681010246,\n",
       "    0.03429580479860306,\n",
       "    -0.004339320119470358,\n",
       "    0.026853717863559723,\n",
       "    -0.045635323971509933,\n",
       "    0.015004792250692844,\n",
       "    -0.034058161079883575,\n",
       "    0.008043539710342884,\n",
       "    0.03407040983438492,\n",
       "    -0.03149409592151642,\n",
       "    0.005619252100586891,\n",
       "    0.00392533652484417,\n",
       "    -0.008053027093410492,\n",
       "    0.0068856715224683285,\n",
       "    0.07264634966850281,\n",
       "    -0.05745025724172592,\n",
       "    0.0003159716143272817,\n",
       "    0.02279866486787796,\n",
       "    -0.0012799239484593272,\n",
       "    0.0247357077896595,\n",
       "    0.03283603489398956,\n",
       "    -0.02061028592288494,\n",
       "    0.03756508231163025,\n",
       "    -0.034730464220047,\n",
       "    0.012530586682260036,\n",
       "    0.015611759386956692,\n",
       "    6.427513289963827e-05,\n",
       "    -0.023913763463497162,\n",
       "    -0.024479886516928673,\n",
       "    0.005801552906632423,\n",
       "    -0.07976669073104858,\n",
       "    -0.01636769063770771,\n",
       "    -0.014274952933192253,\n",
       "    -0.008966144174337387,\n",
       "    0.03139564022421837,\n",
       "    -0.02542710117995739,\n",
       "    0.001984640955924988,\n",
       "    0.0397377572953701,\n",
       "    0.004825994838029146,\n",
       "    -0.019187990576028824,\n",
       "    0.06138201430439949,\n",
       "    2.7359445198271715e-07,\n",
       "    -0.012346680276095867,\n",
       "    -0.0019778884015977383,\n",
       "    -0.004366565495729446,\n",
       "    0.04626493528485298,\n",
       "    0.08019692450761795,\n",
       "    0.0019862568005919456,\n",
       "    0.040779028087854385,\n",
       "    -0.03684728592634201,\n",
       "    -0.010185838676989079,\n",
       "    -0.01079829316586256,\n",
       "    -0.007846677675843239,\n",
       "    0.01479173544794321,\n",
       "    -0.009474572725594044,\n",
       "    0.0007901614881120622,\n",
       "    -0.018039681017398834,\n",
       "    -0.062452517449855804,\n",
       "    -0.01287779025733471,\n",
       "    -0.023097433149814606,\n",
       "    -0.021674735471606255,\n",
       "    0.05285794660449028,\n",
       "    0.01101074367761612,\n",
       "    0.03707215562462807,\n",
       "    -0.023268362507224083,\n",
       "    0.03190107271075249,\n",
       "    -0.01523204892873764,\n",
       "    -0.03853606432676315,\n",
       "    -0.061061397194862366,\n",
       "    0.0020334667060524225,\n",
       "    0.009585482999682426,\n",
       "    0.05093754827976227,\n",
       "    0.028743011876940727,\n",
       "    0.0011241936590522528,\n",
       "    0.054649703204631805,\n",
       "    0.023016782477498055,\n",
       "    -0.0006013992242515087,\n",
       "    -0.02457912638783455,\n",
       "    0.03417526185512543,\n",
       "    0.046849269419908524,\n",
       "    -0.022031623870134354,\n",
       "    -0.014064297080039978,\n",
       "    0.04041406512260437,\n",
       "    -0.00193231669254601,\n",
       "    -0.0010018206667155027,\n",
       "    -0.05571792274713516,\n",
       "    0.061048734933137894,\n",
       "    0.015919124707579613,\n",
       "    -0.060868654400110245,\n",
       "    -0.10823094099760056,\n",
       "    -0.0018944687908515334,\n",
       "    -0.014933979138731956,\n",
       "    -0.04121609032154083,\n",
       "    -0.005049745552241802,\n",
       "    0.035803455859422684,\n",
       "    0.017609337344765663,\n",
       "    0.026019079610705376,\n",
       "    -0.04694617539644241,\n",
       "    -0.003903936129063368,\n",
       "    0.03092704713344574,\n",
       "    0.012478234246373177,\n",
       "    -0.05392036586999893,\n",
       "    0.004628662019968033,\n",
       "    -0.048673711717128754,\n",
       "    0.0218451339751482,\n",
       "    -0.07339170575141907,\n",
       "    0.02613779902458191,\n",
       "    0.010918852873146534,\n",
       "    -0.0049633667804300785,\n",
       "    2.1578802345659166e-34,\n",
       "    0.006775692105293274,\n",
       "    0.02546815201640129,\n",
       "    -0.06239594146609306,\n",
       "    0.03471865877509117,\n",
       "    0.04963023215532303,\n",
       "    -0.06348299980163574,\n",
       "    -0.033513639122247696,\n",
       "    -0.012773043476045132,\n",
       "    -0.026853905990719795,\n",
       "    -0.00817696563899517,\n",
       "    -0.0004670124617405236]}},\n",
       " {'_index': 'ml-interview-questions',\n",
       "  '_id': 'acXylZIByzqUH6XnClgf',\n",
       "  '_score': 1.8349755,\n",
       "  '_source': {'chapter': 'CHAPTER 3',\n",
       "   'title': 'Technical Interview: Machine Learning Algorithms',\n",
       "   'section': 'Overview of the Machine Learning Algorithms Technical Interview',\n",
       "   'text': 'In Chapter 1 , you learned about the various steps you will go through as part of your ML interviews. In Chapter 2 , you looked at how to tie your experiences to roles of interest as well as how to craft a relevant resume. The goal of the previous chapters was to get you invited to interviews. In this chapter, I’ll focus on ML algorithms. As you recall, the interview process is illustrated in Figure 1-9 , and the ML algorithms interview is only one portion of the technical interviews; the rest, such as ML training and evaluation, coding, and so on, will be covered in subsequent chapters. You’re likely to be asked ML algorithm technical questions in an interview if you’re applying for any of the following jobs: • Data scientist who builds ML models • Machine learning engineer • Applied scientist • And similar roles Recall that within the common ML job titles ( Figure 1-8 ), there are some jobs that have the responsibility of training ML models in the ML lifecycle. This chapter focu‐ ses on assessing candidates for those skills; if the job you’re aiming for focuses less on training ML models, you might get a simplified version of this type of interview, or it might be skipped completely. This interview is meant to assess your understanding of ML algorithms, especially on the theoretical side. As to how you implement the algorithms with code, I cover that in the model deployment questions in Chapter 6 and the coding/programming tech‐ nical interview in Chapter 5 . The goal for you as an interviewee is for the interviewers to confirm that you understand the underlying concepts behind ML algorithms. Roles do exist where all you have to know is how to import the library with Python, but for more advanced projects, an underlying understanding can help you custom‐ ize various ML approaches and better debug and troubleshoot models. As covered in Chapter 1 , in the three pillars of ML roles, this is the pillar of ML algorithm and data intuition, which showcases your ability to adapt (refer to Figure 1-6 ). This skill is especially important in companies that have complex ML use cases and custom-made solutions, where you might modify or combine various off-the-shelf methods or cre‐ ate something from scratch. I try to mention as many common algorithms as space allows, but there are many more techniques under the sun. Be sure to check out the linked resources to extend your learning and interview preparation! It is also important to note that, in addition to understanding the ML algorithms’ inner workings and underlying statistical methods, you need to successfully commu‐ nicate that understanding to the interviewer. Yes, I know that communication skills have been brought up many times in this book, but they are what help set you apart as a successful candidate. As a rule of thumb, it’s important to be able to explain algorithms and ML concepts at two levels: on a simple “explain like I’m five years old” level and at a deeper, tech‐ nical level, one more appropriate for a college course. A second rule of thumb is to be prepared to answer follow-up questions to these ML algorithm interview questions. This is so the interviewer knows that you didn’t just memorize and then regurgitate the answer, but that you can apply it to various real-life scenarios on the job. In this chapter, I break down technical questions on the following topics so you can easily refer to a specific question if your interview focuses on that topic: • Statistical techniques • Supervised, unsupervised, and reinforcement learning • Natural language processing (NLP) • Recommender systems • Reinforcement learning • Computer vision In technical interviews that are very structured, such as the Ama‐ zon data science initial phone screen, they will ask you clearly scoped questions, such as asking for a definition of a particular algorithm. After you answer, they will generally move on without additional follow-up questions. There are companies that mix structured questions with a free-form discussion, where the inter‐ viewer might dig deeper into your answer, and the conversation might branch out from there into your past experiences.',\n",
       "   'text_vector': [0.07542776316404343,\n",
       "    -0.0504971481859684,\n",
       "    -0.054775018244981766,\n",
       "    -0.006170399487018585,\n",
       "    0.0045850398018956184,\n",
       "    0.04865339398384094,\n",
       "    -0.01349594909697771,\n",
       "    -0.03663203865289688,\n",
       "    0.01011820137500763,\n",
       "    -0.038959432393312454,\n",
       "    0.05121764913201332,\n",
       "    0.03879440203309059,\n",
       "    0.028971131891012192,\n",
       "    0.05330028384923935,\n",
       "    0.04174472391605377,\n",
       "    -0.03628920391201973,\n",
       "    0.015945224091410637,\n",
       "    -0.012673839926719666,\n",
       "    -0.013979357667267323,\n",
       "    -0.012800918892025948,\n",
       "    -0.026410618796944618,\n",
       "    0.004803183954209089,\n",
       "    0.01603126712143421,\n",
       "    0.020180832594633102,\n",
       "    -0.0036556764971464872,\n",
       "    -0.04129768908023834,\n",
       "    -0.03948451951146126,\n",
       "    0.04698176681995392,\n",
       "    -0.0431240014731884,\n",
       "    0.016378669068217278,\n",
       "    -0.03182235360145569,\n",
       "    -0.04896847531199455,\n",
       "    0.025188816711306572,\n",
       "    -0.020119786262512207,\n",
       "    2.3112379494705237e-06,\n",
       "    -0.013477062806487083,\n",
       "    -0.0028034329880028963,\n",
       "    0.03214086964726448,\n",
       "    -0.014134339988231659,\n",
       "    -0.1011340394616127,\n",
       "    0.03646975755691528,\n",
       "    0.0029031746089458466,\n",
       "    0.05681276321411133,\n",
       "    0.01780327782034874,\n",
       "    -0.014518358744680882,\n",
       "    0.027493255212903023,\n",
       "    0.06560639292001724,\n",
       "    0.007453792728483677,\n",
       "    0.037195224314928055,\n",
       "    0.05194085091352463,\n",
       "    0.016637803986668587,\n",
       "    -0.06271812319755554,\n",
       "    0.009676719084382057,\n",
       "    0.0800342708826065,\n",
       "    -0.005429441574960947,\n",
       "    0.0053891451098024845,\n",
       "    0.013718005269765854,\n",
       "    0.05025045573711395,\n",
       "    0.021595848724246025,\n",
       "    -0.06845112890005112,\n",
       "    0.026740286499261856,\n",
       "    0.01538478396832943,\n",
       "    0.026864783838391304,\n",
       "    0.031776055693626404,\n",
       "    0.08483096212148666,\n",
       "    0.024967480450868607,\n",
       "    -0.005599630530923605,\n",
       "    -0.05703326687216759,\n",
       "    -0.04679424315690994,\n",
       "    -0.03879333287477493,\n",
       "    0.03701925277709961,\n",
       "    -0.07162617146968842,\n",
       "    -0.020829636603593826,\n",
       "    0.01104100700467825,\n",
       "    0.024346981197595596,\n",
       "    -0.03347910940647125,\n",
       "    -0.024461790919303894,\n",
       "    0.0022129768040031195,\n",
       "    0.0200711190700531,\n",
       "    -0.026772573590278625,\n",
       "    -0.008142581209540367,\n",
       "    0.0050894515588879585,\n",
       "    -0.011681066825985909,\n",
       "    -0.005876462906599045,\n",
       "    -0.013488185591995716,\n",
       "    0.08124666661024094,\n",
       "    -0.024575475603342056,\n",
       "    -0.01890699192881584,\n",
       "    0.021768078207969666,\n",
       "    0.008577035740017891,\n",
       "    0.09598111361265182,\n",
       "    -0.0033396626822650433,\n",
       "    0.05207230895757675,\n",
       "    0.08070585876703262,\n",
       "    -0.02943500317633152,\n",
       "    0.002252158708870411,\n",
       "    0.006001987960189581,\n",
       "    0.04014641046524048,\n",
       "    -0.03351613134145737,\n",
       "    -0.03897755220532417,\n",
       "    -0.03276411071419716,\n",
       "    -0.02319248765707016,\n",
       "    0.03135651350021362,\n",
       "    -0.011061509139835835,\n",
       "    0.022663794457912445,\n",
       "    0.009099841117858887,\n",
       "    0.006005586124956608,\n",
       "    0.017668098211288452,\n",
       "    -0.062458474189043045,\n",
       "    -0.04156758263707161,\n",
       "    -0.03368409350514412,\n",
       "    -0.036618564277887344,\n",
       "    -0.028535904362797737,\n",
       "    0.052637241780757904,\n",
       "    0.014842371456325054,\n",
       "    0.06602267175912857,\n",
       "    0.032800011336803436,\n",
       "    0.03001617267727852,\n",
       "    0.005765656940639019,\n",
       "    -0.020338842645287514,\n",
       "    -0.04822372645139694,\n",
       "    -0.012679729610681534,\n",
       "    0.02571055479347706,\n",
       "    0.017676543444395065,\n",
       "    -0.04670216888189316,\n",
       "    -0.11400184035301208,\n",
       "    -0.0030808032024651766,\n",
       "    -0.003942514304071665,\n",
       "    -0.01584027148783207,\n",
       "    -0.030461259186267853,\n",
       "    0.039925120770931244,\n",
       "    0.023560279980301857,\n",
       "    -0.029179850593209267,\n",
       "    -0.016596471890807152,\n",
       "    0.04618768393993378,\n",
       "    0.025049079209566116,\n",
       "    0.013876726850867271,\n",
       "    0.046869125217199326,\n",
       "    -0.06252092123031616,\n",
       "    -0.018606925383210182,\n",
       "    0.030833274126052856,\n",
       "    -0.045250214636325836,\n",
       "    0.009687213227152824,\n",
       "    -0.029691118746995926,\n",
       "    0.00954529270529747,\n",
       "    0.011349808424711227,\n",
       "    -0.01655120961368084,\n",
       "    0.002896480029448867,\n",
       "    0.0015875307144597173,\n",
       "    0.019647618755698204,\n",
       "    -0.07026701420545578,\n",
       "    0.030766630545258522,\n",
       "    0.005382091738283634,\n",
       "    0.02902221493422985,\n",
       "    0.053626902401447296,\n",
       "    -0.0021125993225723505,\n",
       "    -0.01131212618201971,\n",
       "    0.05518634244799614,\n",
       "    0.006092115305364132,\n",
       "    0.026993857696652412,\n",
       "    0.006541565526276827,\n",
       "    -0.07781695574522018,\n",
       "    0.0639835000038147,\n",
       "    0.020534981042146683,\n",
       "    -0.022623421624302864,\n",
       "    -0.057435762137174606,\n",
       "    -0.01578020304441452,\n",
       "    0.08234527707099915,\n",
       "    -0.03228775039315224,\n",
       "    0.03597607836127281,\n",
       "    0.008603443391621113,\n",
       "    -0.009819106198847294,\n",
       "    -0.02141178958117962,\n",
       "    0.01871405728161335,\n",
       "    0.006372878327965736,\n",
       "    0.04332531616091728,\n",
       "    0.0016741855069994926,\n",
       "    0.0026065194979310036,\n",
       "    0.017536798492074013,\n",
       "    0.036830756813287735,\n",
       "    0.01478327065706253,\n",
       "    0.034161824733018875,\n",
       "    -0.016855018213391304,\n",
       "    0.022783787921071053,\n",
       "    -0.018628844991326332,\n",
       "    -0.04064158350229263,\n",
       "    -0.10260837525129318,\n",
       "    0.0582771934568882,\n",
       "    -0.008194961585104465,\n",
       "    -0.011995440348982811,\n",
       "    0.002773607149720192,\n",
       "    0.00711849657818675,\n",
       "    -0.0008348941337317228,\n",
       "    0.03041510283946991,\n",
       "    -0.009014233015477657,\n",
       "    -0.01505090482532978,\n",
       "    -0.05441867560148239,\n",
       "    -0.023845156654715538,\n",
       "    -0.05027526617050171,\n",
       "    -0.01881806179881096,\n",
       "    -0.059223469346761703,\n",
       "    -0.04413190484046936,\n",
       "    -0.04158271476626396,\n",
       "    0.013709481805562973,\n",
       "    0.054438427090644836,\n",
       "    -0.00475778104737401,\n",
       "    -0.005724349524825811,\n",
       "    -0.05523991584777832,\n",
       "    -0.042877454310655594,\n",
       "    -0.020024806261062622,\n",
       "    0.03688059002161026,\n",
       "    -0.029283298179507256,\n",
       "    -0.04058966785669327,\n",
       "    -0.011546267196536064,\n",
       "    -0.012083896435797215,\n",
       "    -0.0045259976759552956,\n",
       "    -0.011248723603785038,\n",
       "    -0.06070778891444206,\n",
       "    0.008508793078362942,\n",
       "    -0.015416399575769901,\n",
       "    -0.020511962473392487,\n",
       "    -0.021577391773462296,\n",
       "    -0.07979125529527664,\n",
       "    0.029977696016430855,\n",
       "    -0.021670714020729065,\n",
       "    -0.020398780703544617,\n",
       "    0.0004125105624552816,\n",
       "    0.0032628923654556274,\n",
       "    0.051809728145599365,\n",
       "    0.049387384206056595,\n",
       "    0.006893189623951912,\n",
       "    0.014516051858663559,\n",
       "    0.015095694921910763,\n",
       "    0.03524384647607803,\n",
       "    0.00255886884406209,\n",
       "    -0.00014044415729586035,\n",
       "    -0.0048080505803227425,\n",
       "    0.04120964556932449,\n",
       "    0.021392367780208588,\n",
       "    -0.035351235419511795,\n",
       "    -0.050773609429597855,\n",
       "    0.06111297756433487,\n",
       "    0.007468043360859156,\n",
       "    0.010170756839215755,\n",
       "    -0.04937737435102463,\n",
       "    -0.018143484368920326,\n",
       "    -0.01731260120868683,\n",
       "    0.007168241310864687,\n",
       "    -0.014231156557798386,\n",
       "    -0.05240270122885704,\n",
       "    0.006177364382892847,\n",
       "    0.008595412597060204,\n",
       "    9.422261064173654e-05,\n",
       "    0.06730344891548157,\n",
       "    -0.027984146028757095,\n",
       "    0.017476752400398254,\n",
       "    0.008610016666352749,\n",
       "    0.029955221340060234,\n",
       "    -0.0584750734269619,\n",
       "    0.008708711713552475,\n",
       "    0.019600871950387955,\n",
       "    -0.04663674533367157,\n",
       "    0.02755831927061081,\n",
       "    0.013033813796937466,\n",
       "    -0.007804634980857372,\n",
       "    -0.03696770593523979,\n",
       "    0.033990487456321716,\n",
       "    -0.0518503300845623,\n",
       "    -0.020836887881159782,\n",
       "    0.013644939288496971,\n",
       "    0.015064534731209278,\n",
       "    0.018556583672761917,\n",
       "    -0.05614415928721428,\n",
       "    -0.008191472850739956,\n",
       "    -0.023106075823307037,\n",
       "    -0.002452984219416976,\n",
       "    -0.010947681032121181,\n",
       "    -0.04919976741075516,\n",
       "    -0.02680627629160881,\n",
       "    0.031663935631513596,\n",
       "    0.00327371247112751,\n",
       "    -0.053360022604465485,\n",
       "    0.0321916788816452,\n",
       "    -0.006345640867948532,\n",
       "    -0.0541975200176239,\n",
       "    0.043973132967948914,\n",
       "    0.0015765078132972121,\n",
       "    -0.031421635299921036,\n",
       "    0.0018044149037450552,\n",
       "    -0.014858572743833065,\n",
       "    -0.011939133517444134,\n",
       "    -0.0008548768819309771,\n",
       "    0.008946730755269527,\n",
       "    -0.0038386033847928047,\n",
       "    0.008012404665350914,\n",
       "    0.03239915147423744,\n",
       "    -0.0354192890226841,\n",
       "    0.05868060886859894,\n",
       "    -0.014401132240891457,\n",
       "    0.0835723802447319,\n",
       "    0.01574617810547352,\n",
       "    0.0028324774466454983,\n",
       "    0.013537116348743439,\n",
       "    0.03314303234219551,\n",
       "    0.039314791560173035,\n",
       "    0.06145762279629707,\n",
       "    -0.024730173870921135,\n",
       "    0.005686201620846987,\n",
       "    0.005212463438510895,\n",
       "    -0.002738407114520669,\n",
       "    0.002949321176856756,\n",
       "    -0.03829965740442276,\n",
       "    0.02939244918525219,\n",
       "    0.061342183500528336,\n",
       "    0.03545593470335007,\n",
       "    -0.06374412775039673,\n",
       "    -0.051208753138780594,\n",
       "    0.012531126849353313,\n",
       "    -0.03397780656814575,\n",
       "    0.06330183148384094,\n",
       "    -0.014424684457480907,\n",
       "    -0.053352754563093185,\n",
       "    0.029322929680347443,\n",
       "    -0.013455108739435673,\n",
       "    -0.016099855303764343,\n",
       "    -0.10517764836549759,\n",
       "    -0.06854279339313507,\n",
       "    0.011052174493670464,\n",
       "    0.0716627687215805,\n",
       "    0.010879093781113625,\n",
       "    0.023226678371429443,\n",
       "    0.0006348710157908499,\n",
       "    0.0014106548624113202,\n",
       "    -0.008432532660663128,\n",
       "    0.0065046921372413635,\n",
       "    -0.018744422122836113,\n",
       "    -0.03715522214770317,\n",
       "    -0.06114716827869415,\n",
       "    0.004946447443217039,\n",
       "    -0.020726755261421204,\n",
       "    -0.012666323222219944,\n",
       "    0.008559847250580788,\n",
       "    -0.03617038205265999,\n",
       "    0.006459166295826435,\n",
       "    -0.014794688671827316,\n",
       "    0.011493481695652008,\n",
       "    -0.04764877259731293,\n",
       "    0.04403813183307648,\n",
       "    -0.059680741280317307,\n",
       "    0.016605591401457787,\n",
       "    0.06514383852481842,\n",
       "    -0.040178559720516205,\n",
       "    0.004299802705645561,\n",
       "    0.0034012014511972666,\n",
       "    0.011212888173758984,\n",
       "    -0.0320701040327549,\n",
       "    0.018884703516960144,\n",
       "    0.021905992180109024,\n",
       "    0.03898826986551285,\n",
       "    0.05479912459850311,\n",
       "    0.04740069434046745,\n",
       "    -0.025333760306239128,\n",
       "    0.03768295422196388,\n",
       "    0.022643502801656723,\n",
       "    -0.07027144730091095,\n",
       "    0.007518575992435217,\n",
       "    0.052403513342142105,\n",
       "    -0.08409272879362106,\n",
       "    0.019459495320916176,\n",
       "    0.055163003504276276,\n",
       "    -0.000209197067306377,\n",
       "    -0.023311903700232506,\n",
       "    0.019460242241621017,\n",
       "    0.014675780199468136,\n",
       "    0.013138995505869389,\n",
       "    -0.025844721123576164,\n",
       "    -0.015514727681875229,\n",
       "    0.004950283095240593,\n",
       "    -0.07176614552736282,\n",
       "    0.03895268961787224,\n",
       "    0.10785733908414841,\n",
       "    -0.09999886900186539,\n",
       "    0.008508860133588314,\n",
       "    0.000629862945061177,\n",
       "    0.01648777164518833,\n",
       "    -0.07028345763683319,\n",
       "    0.017409732565283775,\n",
       "    -0.008225635625422001,\n",
       "    -0.03058457002043724,\n",
       "    0.07300461828708649,\n",
       "    0.005837060045450926,\n",
       "    -0.05551796779036522,\n",
       "    -0.07892680168151855,\n",
       "    -0.05788443237543106,\n",
       "    -0.03057515248656273,\n",
       "    -0.024150123819708824,\n",
       "    0.01997319981455803,\n",
       "    0.022126829251646996,\n",
       "    0.014544906094670296,\n",
       "    0.032805342227220535,\n",
       "    0.0059072598814964294,\n",
       "    -0.010588130913674831,\n",
       "    -0.0010445083025842905,\n",
       "    0.016862312331795692,\n",
       "    0.03225364908576012,\n",
       "    -0.003270166227594018,\n",
       "    0.046056587249040604,\n",
       "    0.01746198534965515,\n",
       "    -0.050216905772686005,\n",
       "    0.09671033918857574,\n",
       "    0.0031633919570595026,\n",
       "    0.0582270473241806,\n",
       "    0.04163970425724983,\n",
       "    0.007959490641951561,\n",
       "    -0.0042951954528689384,\n",
       "    -0.019665898755192757,\n",
       "    -0.010332994163036346,\n",
       "    -0.030692454427480698,\n",
       "    0.02286931686103344,\n",
       "    -0.0534844808280468,\n",
       "    -0.009985503740608692,\n",
       "    0.060071032494306564,\n",
       "    0.04511312022805214,\n",
       "    -0.03761940822005272,\n",
       "    -0.05004114285111427,\n",
       "    -0.03841029107570648,\n",
       "    0.0246807262301445,\n",
       "    -0.00025614944752305746,\n",
       "    0.045859407633543015,\n",
       "    0.011243293061852455,\n",
       "    -0.007488315459340811,\n",
       "    0.006990340538322926,\n",
       "    -0.03444096818566322,\n",
       "    0.024999571964144707,\n",
       "    0.00972722563892603,\n",
       "    -0.019235027953982353,\n",
       "    0.021986259147524834,\n",
       "    0.009338506497442722,\n",
       "    0.056920573115348816,\n",
       "    0.05607859790325165,\n",
       "    -0.025538213551044464,\n",
       "    0.03585175424814224,\n",
       "    -0.0094629330560565,\n",
       "    0.03706864267587662,\n",
       "    0.006224735639989376,\n",
       "    0.016721637919545174,\n",
       "    -0.02391570620238781,\n",
       "    -0.05522901564836502,\n",
       "    0.01541887503117323,\n",
       "    -0.004986909218132496,\n",
       "    0.047307148575782776,\n",
       "    0.04456711560487747,\n",
       "    -0.039386700838804245,\n",
       "    0.04205664247274399,\n",
       "    -0.08988805115222931,\n",
       "    -0.032582689076662064,\n",
       "    -0.00996230635792017,\n",
       "    -0.006705261301249266,\n",
       "    0.0021503877360373735,\n",
       "    0.001375598250888288,\n",
       "    0.04429584741592407,\n",
       "    0.01661762036383152,\n",
       "    0.03769201040267944,\n",
       "    -0.03563389182090759,\n",
       "    -0.030560249462723732,\n",
       "    -0.027567973360419273,\n",
       "    -0.00115462567191571,\n",
       "    -0.041986819356679916,\n",
       "    0.00707637844607234,\n",
       "    -0.012774781323969364,\n",
       "    -0.006195960100740194,\n",
       "    -0.015278632752597332,\n",
       "    -0.03502630069851875,\n",
       "    -0.0009365389705635607,\n",
       "    0.008568000048398972,\n",
       "    -0.025486687198281288,\n",
       "    -0.05574065074324608,\n",
       "    0.016377363353967667,\n",
       "    -0.06158704310655594,\n",
       "    -0.013613666407763958,\n",
       "    0.077242910861969,\n",
       "    0.020594522356987,\n",
       "    0.0190895926207304,\n",
       "    -0.06958664208650589,\n",
       "    -0.02145909145474434,\n",
       "    0.022496158257126808,\n",
       "    0.07571477442979813,\n",
       "    -0.0005995662650093436,\n",
       "    -0.04206467047333717,\n",
       "    -0.04127419367432594,\n",
       "    0.015718013048171997,\n",
       "    0.060294367372989655,\n",
       "    0.04814155399799347,\n",
       "    -0.0750754326581955,\n",
       "    -0.0340239591896534,\n",
       "    -0.017148898914456367,\n",
       "    -0.028594525530934334,\n",
       "    0.07596433907747269,\n",
       "    0.03443526849150658,\n",
       "    0.04256958141922951,\n",
       "    -0.00536612793803215,\n",
       "    -0.059536419808864594,\n",
       "    -0.012237688526511192,\n",
       "    -0.0011009129229933023,\n",
       "    0.03680349141359329,\n",
       "    -0.008339068852365017,\n",
       "    0.005794202443212271,\n",
       "    0.04822668060660362,\n",
       "    -0.02470206283032894,\n",
       "    0.049213506281375885,\n",
       "    -0.016034670174121857,\n",
       "    -0.0032918269280344248,\n",
       "    -0.0212328489869833,\n",
       "    -0.06737961620092392,\n",
       "    -0.00947793573141098,\n",
       "    -0.0051793185994029045,\n",
       "    0.007171024102717638,\n",
       "    -0.03427313268184662,\n",
       "    -0.020688069984316826,\n",
       "    -0.004334805067628622,\n",
       "    0.027349552139639854,\n",
       "    -0.00176816841121763,\n",
       "    -0.0025961054489016533,\n",
       "    0.0027545958291739225,\n",
       "    0.04853759706020355,\n",
       "    0.001978527521714568,\n",
       "    0.023774579167366028,\n",
       "    -0.05275251716375351,\n",
       "    -0.023313794285058975,\n",
       "    0.030884526669979095,\n",
       "    -0.013372005894780159,\n",
       "    0.0237057413905859,\n",
       "    0.01702800951898098,\n",
       "    -0.05203753709793091,\n",
       "    0.030578935518860817,\n",
       "    -0.013164161704480648,\n",
       "    0.09129156917333603,\n",
       "    0.052532654255628586,\n",
       "    -0.008873440325260162,\n",
       "    -0.02676398865878582,\n",
       "    0.006003192160278559,\n",
       "    -0.04563641920685768,\n",
       "    -0.00783032737672329,\n",
       "    -0.03461470827460289,\n",
       "    0.0350581556558609,\n",
       "    -0.03878156840801239,\n",
       "    0.02601870335638523,\n",
       "    0.052678272128105164,\n",
       "    0.01102754008024931,\n",
       "    0.04366159811615944,\n",
       "    -0.027114633470773697,\n",
       "    -0.004697541240602732,\n",
       "    0.008089134469628334,\n",
       "    -0.002693821908906102,\n",
       "    0.042814161628484726,\n",
       "    -5.505592361285299e-33,\n",
       "    0.018288983032107353,\n",
       "    -0.06661982834339142,\n",
       "    0.06650696694850922,\n",
       "    0.05972690135240555,\n",
       "    -0.004413933027535677,\n",
       "    -0.047177862375974655,\n",
       "    0.029295284301042557,\n",
       "    0.023369047790765762,\n",
       "    -0.03890715539455414,\n",
       "    -0.013215428218245506,\n",
       "    0.01460530236363411,\n",
       "    0.03136492520570755,\n",
       "    0.017397457733750343,\n",
       "    0.004497995600104332,\n",
       "    -0.02029450796544552,\n",
       "    0.03385106101632118,\n",
       "    -0.02967068925499916,\n",
       "    0.002559276530519128,\n",
       "    0.02811523526906967,\n",
       "    -0.033049724996089935,\n",
       "    -0.11208158731460571,\n",
       "    0.028305504471063614,\n",
       "    -0.0185054000467062,\n",
       "    -0.04709630459547043,\n",
       "    0.021554019302129745,\n",
       "    -0.004889384843409061,\n",
       "    -0.005573929287493229,\n",
       "    -0.019854076206684113,\n",
       "    -0.052561383694410324,\n",
       "    0.04118664935231209,\n",
       "    0.012845874764025211,\n",
       "    0.003609148785471916,\n",
       "    -0.00016503187362104654,\n",
       "    -0.061894696205854416,\n",
       "    -0.009714271873235703,\n",
       "    0.033993907272815704,\n",
       "    -0.040859270840883255,\n",
       "    0.010751229710876942,\n",
       "    0.05966058373451233,\n",
       "    0.05942080169916153,\n",
       "    0.0012650219723582268,\n",
       "    -0.04438580945134163,\n",
       "    0.044069986790418625,\n",
       "    -0.009601393714547157,\n",
       "    -0.011511570774018764,\n",
       "    -0.014842435717582703,\n",
       "    0.03756392374634743,\n",
       "    -0.0343766026198864,\n",
       "    0.02699929103255272,\n",
       "    0.004272451624274254,\n",
       "    0.009896958246827126,\n",
       "    0.020156433805823326,\n",
       "    -0.06970527023077011,\n",
       "    0.024424569681286812,\n",
       "    0.05130938068032265,\n",
       "    0.03190778195858002,\n",
       "    -0.021979892626404762,\n",
       "    0.013346207328140736,\n",
       "    0.005110259633511305,\n",
       "    0.005824417807161808,\n",
       "    -0.03734159097075462,\n",
       "    -0.04096115753054619,\n",
       "    0.04484912380576134,\n",
       "    -0.0021290448494255543,\n",
       "    0.03757791966199875,\n",
       "    -0.012512976303696632,\n",
       "    0.07386698573827744,\n",
       "    -0.03247113898396492,\n",
       "    0.02725907228887081,\n",
       "    0.019932465627789497,\n",
       "    -0.019113188609480858,\n",
       "    -0.001258924719877541,\n",
       "    -0.02291848137974739,\n",
       "    0.0035186202730983496,\n",
       "    -0.01406778208911419,\n",
       "    0.009122871793806553,\n",
       "    -0.013837341219186783,\n",
       "    -0.009852360002696514,\n",
       "    0.01703958958387375,\n",
       "    -0.05882561579346657,\n",
       "    -0.015830909833312035,\n",
       "    0.02229905128479004,\n",
       "    0.020034100860357285,\n",
       "    -0.013996321707963943,\n",
       "    -0.02929898165166378,\n",
       "    -0.07269981503486633,\n",
       "    -0.009496198035776615,\n",
       "    0.01701822504401207,\n",
       "    -0.018382785841822624,\n",
       "    -0.05587100237607956,\n",
       "    -0.038129448890686035,\n",
       "    0.06870388984680176,\n",
       "    -0.03013545647263527,\n",
       "    -0.047216203063726425,\n",
       "    0.017955046147108078,\n",
       "    -0.07422354072332382,\n",
       "    -0.010460708290338516,\n",
       "    -0.018439751118421555,\n",
       "    0.036258213222026825,\n",
       "    0.07538038492202759,\n",
       "    0.003379697911441326,\n",
       "    -0.0048727793619036674,\n",
       "    -0.030529268085956573,\n",
       "    -0.025697408244013786,\n",
       "    0.05025435984134674,\n",
       "    0.04897981509566307,\n",
       "    -0.014851986430585384,\n",
       "    0.019560370594263077,\n",
       "    0.016479618847370148,\n",
       "    -0.0224889125674963,\n",
       "    -0.006879877764731646,\n",
       "    0.006698986515402794,\n",
       "    -0.010634390637278557,\n",
       "    0.07263965904712677,\n",
       "    -0.025371672585606575,\n",
       "    0.01719066873192787,\n",
       "    0.0068826074711978436,\n",
       "    0.010359392501413822,\n",
       "    -0.03298982232809067,\n",
       "    -0.008239788003265858,\n",
       "    -9.559128375258297e-05,\n",
       "    -0.07011346518993378,\n",
       "    -0.031585272401571274,\n",
       "    -0.03927869349718094,\n",
       "    -0.0070949760265648365,\n",
       "    -0.01883348636329174,\n",
       "    -0.03594687581062317,\n",
       "    0.011295204050838947,\n",
       "    0.016304487362504005,\n",
       "    0.030952073633670807,\n",
       "    -0.029272014275193214,\n",
       "    0.06444388628005981,\n",
       "    2.84987322629604e-07,\n",
       "    0.018931079655885696,\n",
       "    -0.00026767360395751894,\n",
       "    -0.014114880934357643,\n",
       "    0.0368439145386219,\n",
       "    0.022425318136811256,\n",
       "    -0.003663382725790143,\n",
       "    -0.03052208386361599,\n",
       "    -0.03364669159054756,\n",
       "    0.022837426513433456,\n",
       "    -0.02879900112748146,\n",
       "    -0.00935883354395628,\n",
       "    0.040198083966970444,\n",
       "    0.007646206300705671,\n",
       "    -0.027991024777293205,\n",
       "    -0.031706199049949646,\n",
       "    -0.07887271791696548,\n",
       "    0.009842116385698318,\n",
       "    -0.023320913314819336,\n",
       "    -0.008224843069911003,\n",
       "    0.05547225475311279,\n",
       "    0.0013594692572951317,\n",
       "    0.06043024733662605,\n",
       "    0.02011486142873764,\n",
       "    0.030001534149050713,\n",
       "    -0.014552590437233448,\n",
       "    0.002311523538082838,\n",
       "    -0.04786156117916107,\n",
       "    -0.008492144756019115,\n",
       "    0.058543555438518524,\n",
       "    0.03954048827290535,\n",
       "    0.01742538996040821,\n",
       "    -0.0032498291693627834,\n",
       "    0.045802466571331024,\n",
       "    0.027160324156284332,\n",
       "    0.009039808064699173,\n",
       "    -0.027363354340195656,\n",
       "    0.014213998802006245,\n",
       "    0.08448115736246109,\n",
       "    0.009656671434640884,\n",
       "    0.03630722314119339,\n",
       "    0.042459093034267426,\n",
       "    0.025251181796193123,\n",
       "    0.015972089022397995,\n",
       "    -0.07889700680971146,\n",
       "    0.04468676075339317,\n",
       "    0.03180193901062012,\n",
       "    -0.054048433899879456,\n",
       "    -0.08516103029251099,\n",
       "    0.005191166885197163,\n",
       "    -0.032150160521268845,\n",
       "    -0.015266402624547482,\n",
       "    -0.007980706170201302,\n",
       "    0.04627739265561104,\n",
       "    0.03936589136719704,\n",
       "    0.0044465744867920876,\n",
       "    -0.04082592576742172,\n",
       "    -0.036215294152498245,\n",
       "    0.02174079231917858,\n",
       "    -0.0135844387114048,\n",
       "    -0.046089790761470795,\n",
       "    -0.01306892465800047,\n",
       "    -0.020063117146492004,\n",
       "    0.01846790872514248,\n",
       "    -0.041088975965976715,\n",
       "    0.07275497913360596,\n",
       "    -0.018056966364383698,\n",
       "    0.027173960581421852,\n",
       "    2.5209098538870426e-34,\n",
       "    -0.013855796307325363,\n",
       "    0.020874368026852608,\n",
       "    -0.03477856144309044,\n",
       "    0.05155056342482567,\n",
       "    0.032027918845415115,\n",
       "    -0.06711357831954956,\n",
       "    -0.03397644683718681,\n",
       "    -0.010598518885672092,\n",
       "    -0.01526269968599081,\n",
       "    0.043499644845724106,\n",
       "    -0.013651829212903976]}},\n",
       " {'_index': 'ml-interview-questions',\n",
       "  '_id': 'W8XylZIByzqUH6XnCVjT',\n",
       "  '_score': 1.8303732,\n",
       "  '_source': {'chapter': 'CHAPTER 1',\n",
       "   'title': 'Machine Learning Roles and the Interview Process',\n",
       "   'section': 'Overview of This Book',\n",
       "   'text': 'In the first part of this chapter, I’ll walk through the structure of this book. Then, I’ll discuss the various job titles and roles that use ML skills in industry. 1 I’ll also clarify the responsibilities of various job titles, such as data scientist, machine learning engineer, and so on, as this is a common point of confusion for job seekers. These will be illustrated with an ML skills matrix and ML lifecycle that will be referenced throughout the book. The second part of this chapter walks through the interview process, from beginning to end. I’ve mentored candidates who appreciated this overview since online resources often focus on specific pieces of the interview but not how they all connect together and result in an offer. Especially for new graduates 2 and readers coming from different industries, this chapter helps get everyone on the same page as well as clarifies the process. The interconnecting pieces of interviews are complex, with many types of combina‐ tions depending on the ML role you’re aiming for. This overview will help set the stage, so you’ll know what to focus your time on. For example, some online resources focus on knowledge specific to “product data scientists,” but will title the course or article “data scientist interview tips” without differentiating. For a newcomer, it’s hard to tell if that is relevant to your own career interests. After this chapter, you’ll be able to tell what skills are required for each job title, and in Chapter 2 , you’ll be able to parse out that information yourself from job postings and make your resume as relevant to the job title and job posting as possible. This chapter focuses on helping you differentiate among various ML roles, and walks through the entire interview process, as illustrated in Figure 1-1 : • Job applications and resume ( Chapter 2 ) • Technical interviews — Machine learning (Chapters 3 , 4 , and 6 ) — Coding/programming ( Chapter 5 ) • Behavioral interviews ( Chapter 7 ) • Your interview roadmap ( Chapter 8 ) • Post-interview and follow-up ( Chapter 9 ) Figure 1-1. Overview of the chapters and how they tie into the ML interview process. Depending on where you are in your ML interview journey, I encourage you to focus on the chapters and sections that seem relevant to you. I’ve also planned the book to be referenced as you go along; for example, you might iterate on your resume multi‐ ple times and then flip back to Chapter 2 when needed. The same applies to the other chapters. With that overview, let’s continue. The companion site to this book, https://susanshu.substack.com , features bonus content, helper resources, and more.',\n",
       "   'text_vector': [0.05361959710717201,\n",
       "    -0.053831424564123154,\n",
       "    -0.07065847516059875,\n",
       "    -0.026697732508182526,\n",
       "    0.014686007983982563,\n",
       "    0.018319953233003616,\n",
       "    0.010219128802418709,\n",
       "    -0.043905407190322876,\n",
       "    0.01886974833905697,\n",
       "    -0.029730461537837982,\n",
       "    0.05431145429611206,\n",
       "    0.04347320646047592,\n",
       "    0.008838780224323273,\n",
       "    0.07363094389438629,\n",
       "    0.07273608446121216,\n",
       "    -0.048283275216817856,\n",
       "    0.0186614952981472,\n",
       "    0.01654110848903656,\n",
       "    -0.0035520698875188828,\n",
       "    0.002241626614704728,\n",
       "    -0.02008422277867794,\n",
       "    0.003956672269850969,\n",
       "    -0.035183511674404144,\n",
       "    0.039715856313705444,\n",
       "    -0.012344683520495892,\n",
       "    -0.04920351132750511,\n",
       "    0.01158470381051302,\n",
       "    0.04547249153256416,\n",
       "    -0.007691530045121908,\n",
       "    0.021400263532996178,\n",
       "    0.01311235036700964,\n",
       "    -0.06220462918281555,\n",
       "    0.03281429782509804,\n",
       "    -0.004543709568679333,\n",
       "    2.204827069363091e-06,\n",
       "    -0.047574982047080994,\n",
       "    -0.006367604248225689,\n",
       "    -0.003702611895278096,\n",
       "    -0.041637301445007324,\n",
       "    -0.06889709830284119,\n",
       "    0.023631777614355087,\n",
       "    0.005760800093412399,\n",
       "    0.03028561733663082,\n",
       "    0.0338287428021431,\n",
       "    -0.010191011242568493,\n",
       "    0.01399321760982275,\n",
       "    0.0740559846162796,\n",
       "    0.0054290927946567535,\n",
       "    -0.0037654214538633823,\n",
       "    0.05225309357047081,\n",
       "    -0.011895191855728626,\n",
       "    -0.015879591926932335,\n",
       "    0.013472617603838444,\n",
       "    0.02717549353837967,\n",
       "    0.017827186733484268,\n",
       "    -0.007499967701733112,\n",
       "    0.002237569773569703,\n",
       "    0.03185978904366493,\n",
       "    0.02367415279150009,\n",
       "    -0.0639217346906662,\n",
       "    0.009745026007294655,\n",
       "    0.011802860535681248,\n",
       "    -0.012691925279796124,\n",
       "    -0.015092939138412476,\n",
       "    0.06529740989208221,\n",
       "    0.02096138522028923,\n",
       "    0.01988040842115879,\n",
       "    -0.0245343130081892,\n",
       "    -0.014781299978494644,\n",
       "    -0.001919961767271161,\n",
       "    0.037427179515361786,\n",
       "    -0.011434183456003666,\n",
       "    -0.018898095935583115,\n",
       "    -0.01360112614929676,\n",
       "    0.06541205197572708,\n",
       "    0.011612567119300365,\n",
       "    -0.01799526810646057,\n",
       "    0.01281860563904047,\n",
       "    -0.016740556806325912,\n",
       "    -0.03875122591853142,\n",
       "    -0.037399012595415115,\n",
       "    0.030601613223552704,\n",
       "    -0.023950785398483276,\n",
       "    -0.0024005393497645855,\n",
       "    0.006442532874643803,\n",
       "    0.08057904988527298,\n",
       "    -0.02295106090605259,\n",
       "    -0.016572749242186546,\n",
       "    0.02760862559080124,\n",
       "    -0.024374263361096382,\n",
       "    0.09037656337022781,\n",
       "    0.008100009523332119,\n",
       "    0.023902185261249542,\n",
       "    0.04127703234553337,\n",
       "    -0.03477370738983154,\n",
       "    0.008731888607144356,\n",
       "    0.012074609287083149,\n",
       "    0.016437888145446777,\n",
       "    -0.027810199186205864,\n",
       "    -0.07016973942518234,\n",
       "    -0.03353556618094444,\n",
       "    -0.04243849217891693,\n",
       "    0.02424926869571209,\n",
       "    0.0002701464982237667,\n",
       "    0.024826422333717346,\n",
       "    0.0010748144704848528,\n",
       "    0.020829180255532265,\n",
       "    -0.023305077105760574,\n",
       "    -0.04603785648941994,\n",
       "    -0.0377148874104023,\n",
       "    -0.02345307543873787,\n",
       "    -0.040861815214157104,\n",
       "    -0.019737936556339264,\n",
       "    0.029977982863783836,\n",
       "    0.016887877136468887,\n",
       "    0.039965804666280746,\n",
       "    0.007492836564779282,\n",
       "    0.02558436617255211,\n",
       "    -0.030489230528473854,\n",
       "    -0.019788186997175217,\n",
       "    -0.040713559836149216,\n",
       "    0.021068882197141647,\n",
       "    0.024753905832767487,\n",
       "    0.047263193875551224,\n",
       "    -0.07348436117172241,\n",
       "    -0.044521696865558624,\n",
       "    -0.008377567864954472,\n",
       "    0.02543971687555313,\n",
       "    -0.014155900105834007,\n",
       "    -0.05421038717031479,\n",
       "    0.04243962839245796,\n",
       "    0.009874590672552586,\n",
       "    -0.0067993104457855225,\n",
       "    -0.021407125517725945,\n",
       "    0.05012046545743942,\n",
       "    0.062413591891527176,\n",
       "    -0.0003210176364518702,\n",
       "    0.045816171914339066,\n",
       "    -0.040688157081604004,\n",
       "    -0.015774404630064964,\n",
       "    0.04767733812332153,\n",
       "    -0.018810315057635307,\n",
       "    0.019404970109462738,\n",
       "    0.0344194732606411,\n",
       "    0.022689225152134895,\n",
       "    0.007256805896759033,\n",
       "    -0.011945812031626701,\n",
       "    -0.008095726370811462,\n",
       "    -0.004743729718029499,\n",
       "    0.03543525189161301,\n",
       "    -0.014098864048719406,\n",
       "    0.014847204089164734,\n",
       "    0.00793615821748972,\n",
       "    -0.0018615840235725045,\n",
       "    0.08193040639162064,\n",
       "    -0.00025799969444051385,\n",
       "    0.01113703940063715,\n",
       "    0.02456468716263771,\n",
       "    0.0064283390529453754,\n",
       "    0.05020115152001381,\n",
       "    0.021969318389892578,\n",
       "    -0.0721859335899353,\n",
       "    0.09093313664197922,\n",
       "    0.03402480110526085,\n",
       "    -0.029970524832606316,\n",
       "    -0.06782137602567673,\n",
       "    -0.01378462091088295,\n",
       "    0.069770947098732,\n",
       "    -0.04384070634841919,\n",
       "    0.016564229503273964,\n",
       "    0.04138613119721413,\n",
       "    -0.015894107520580292,\n",
       "    -0.022492732852697372,\n",
       "    0.04320116713643074,\n",
       "    0.04342794045805931,\n",
       "    0.034491244703531265,\n",
       "    0.027149351313710213,\n",
       "    0.012870931066572666,\n",
       "    0.01709343120455742,\n",
       "    0.08564941585063934,\n",
       "    -0.002425686689093709,\n",
       "    -0.03354981541633606,\n",
       "    -0.004032713361084461,\n",
       "    0.05615035444498062,\n",
       "    -0.00863647274672985,\n",
       "    -0.04284297302365303,\n",
       "    -0.07953301072120667,\n",
       "    0.031933024525642395,\n",
       "    -0.014471856877207756,\n",
       "    -0.005357254762202501,\n",
       "    0.017520880326628685,\n",
       "    0.021691076457500458,\n",
       "    -0.045217547565698624,\n",
       "    0.06720304489135742,\n",
       "    -0.01776697114109993,\n",
       "    -0.019282374531030655,\n",
       "    -0.05696340277791023,\n",
       "    -0.006743068806827068,\n",
       "    -0.041110504418611526,\n",
       "    -0.011214477941393852,\n",
       "    -0.054497670382261276,\n",
       "    -0.019932929426431656,\n",
       "    -0.05132652819156647,\n",
       "    0.030102139338850975,\n",
       "    0.05967451632022858,\n",
       "    -0.009253112599253654,\n",
       "    -0.04488971829414368,\n",
       "    -0.0744553655385971,\n",
       "    -0.03606308251619339,\n",
       "    0.00875797774642706,\n",
       "    0.03679654374718666,\n",
       "    -0.037448130548000336,\n",
       "    -0.02294452302157879,\n",
       "    -0.006524645257741213,\n",
       "    -0.00014711216499563307,\n",
       "    -0.03705928474664688,\n",
       "    -0.02399170584976673,\n",
       "    -0.045513808727264404,\n",
       "    -0.023135297000408173,\n",
       "    0.005903215613216162,\n",
       "    -0.01843711920082569,\n",
       "    -0.027300875633955002,\n",
       "    -0.06093081831932068,\n",
       "    0.008689137175679207,\n",
       "    -0.014520643278956413,\n",
       "    -0.046070486307144165,\n",
       "    0.018604446202516556,\n",
       "    0.0047203609719872475,\n",
       "    0.03231348842382431,\n",
       "    0.05078049376606941,\n",
       "    0.00725837517529726,\n",
       "    0.02038019709289074,\n",
       "    0.018396129831671715,\n",
       "    0.04494117572903633,\n",
       "    0.004984731320291758,\n",
       "    0.036515094339847565,\n",
       "    0.008205421268939972,\n",
       "    0.07539955526590347,\n",
       "    0.01110096462070942,\n",
       "    -0.049033261835575104,\n",
       "    -0.06023162975907326,\n",
       "    0.04878422990441322,\n",
       "    -0.02218666486442089,\n",
       "    0.011892314068973064,\n",
       "    -0.016511928290128708,\n",
       "    -0.00853157788515091,\n",
       "    -0.0289089847356081,\n",
       "    0.03323576599359512,\n",
       "    -0.0297747403383255,\n",
       "    -0.029186414554715157,\n",
       "    0.02137669362127781,\n",
       "    -0.005348695907741785,\n",
       "    0.0030445430893450975,\n",
       "    0.06335540860891342,\n",
       "    0.014092613011598587,\n",
       "    -0.0229860320687294,\n",
       "    -0.00892389565706253,\n",
       "    0.03063260205090046,\n",
       "    -0.08056722581386566,\n",
       "    0.024631697684526443,\n",
       "    0.006875278428196907,\n",
       "    -0.04589430242776871,\n",
       "    0.054683711379766464,\n",
       "    -0.0063646468333899975,\n",
       "    -0.056275881826877594,\n",
       "    -0.04562706500291824,\n",
       "    0.033023785799741745,\n",
       "    -0.016569998115301132,\n",
       "    -0.02177663892507553,\n",
       "    0.03778798133134842,\n",
       "    0.008480612188577652,\n",
       "    0.004457617178559303,\n",
       "    -0.05472996085882187,\n",
       "    -0.025419175624847412,\n",
       "    -0.028136136010289192,\n",
       "    -0.007149051874876022,\n",
       "    -0.026543108746409416,\n",
       "    0.0009743889095261693,\n",
       "    -0.05753876268863678,\n",
       "    0.03649409860372543,\n",
       "    0.0033672184217721224,\n",
       "    -0.01550847478210926,\n",
       "    0.0272055771201849,\n",
       "    -0.0035957000218331814,\n",
       "    -0.032211221754550934,\n",
       "    0.0245804525911808,\n",
       "    0.027689391747117043,\n",
       "    -0.04746439307928085,\n",
       "    0.020656703040003777,\n",
       "    -0.02499186433851719,\n",
       "    -0.020751355215907097,\n",
       "    0.003007033374160528,\n",
       "    -0.011121298186480999,\n",
       "    0.020100872963666916,\n",
       "    -0.03129500523209572,\n",
       "    0.04513849690556526,\n",
       "    -0.02994544617831707,\n",
       "    0.07390092313289642,\n",
       "    0.009910175576806068,\n",
       "    0.06935020536184311,\n",
       "    0.010021855123341084,\n",
       "    0.014041254296898842,\n",
       "    0.04440564289689064,\n",
       "    0.01873619668185711,\n",
       "    0.05697770416736603,\n",
       "    0.058589108288288116,\n",
       "    -0.029678139835596085,\n",
       "    0.01230844296514988,\n",
       "    0.022418474778532982,\n",
       "    -0.04815316200256348,\n",
       "    -0.01168041955679655,\n",
       "    -0.008433479815721512,\n",
       "    0.06708232313394547,\n",
       "    0.05973222851753235,\n",
       "    0.03553133085370064,\n",
       "    -0.04431923106312752,\n",
       "    -0.04295143112540245,\n",
       "    0.020138554275035858,\n",
       "    -0.014084545895457268,\n",
       "    0.06445034593343735,\n",
       "    -0.03669732064008713,\n",
       "    -0.03673595190048218,\n",
       "    0.02091962844133377,\n",
       "    -0.01323949545621872,\n",
       "    0.007779255043715239,\n",
       "    -0.09657318145036697,\n",
       "    -0.09691774845123291,\n",
       "    0.016666391864418983,\n",
       "    0.04734214395284653,\n",
       "    -0.026363728567957878,\n",
       "    0.023356517776846886,\n",
       "    -0.04142624884843826,\n",
       "    -0.009678876958787441,\n",
       "    -0.025728369131684303,\n",
       "    -0.00031214114278554916,\n",
       "    0.002661569509655237,\n",
       "    -0.03447897359728813,\n",
       "    -0.06016619876027107,\n",
       "    -0.028600364923477173,\n",
       "    -0.014638892374932766,\n",
       "    -0.029771368950605392,\n",
       "    0.01707232929766178,\n",
       "    -0.03190646320581436,\n",
       "    -0.006881312932819128,\n",
       "    -0.0008595973486080766,\n",
       "    0.000467784731881693,\n",
       "    0.0074604591354727745,\n",
       "    0.00880946684628725,\n",
       "    -0.0812060609459877,\n",
       "    -0.016463980078697205,\n",
       "    0.05652543529868126,\n",
       "    -0.010876529850065708,\n",
       "    0.016892636194825172,\n",
       "    -0.011033603921532631,\n",
       "    0.008751515299081802,\n",
       "    -0.01869744434952736,\n",
       "    0.04399924352765083,\n",
       "    0.05931931734085083,\n",
       "    0.028415244072675705,\n",
       "    0.011345184408128262,\n",
       "    0.004914517048746347,\n",
       "    -0.0015495548723265529,\n",
       "    0.004060415085405111,\n",
       "    -0.01967395655810833,\n",
       "    -0.03664035350084305,\n",
       "    -0.018116403371095657,\n",
       "    0.03153952211141586,\n",
       "    -0.07357292622327805,\n",
       "    0.03646345064043999,\n",
       "    0.03904825076460838,\n",
       "    -0.012914692983031273,\n",
       "    -0.011313274502754211,\n",
       "    0.03418603539466858,\n",
       "    0.025113927200436592,\n",
       "    -0.01684354431927204,\n",
       "    -0.044036269187927246,\n",
       "    -0.0054961820133030415,\n",
       "    0.00012011806393275037,\n",
       "    -0.07662419229745865,\n",
       "    0.03927832469344139,\n",
       "    0.10466449707746506,\n",
       "    -0.11605249345302582,\n",
       "    0.04603242874145508,\n",
       "    -0.018366174772381783,\n",
       "    0.04094013199210167,\n",
       "    -0.06270642578601837,\n",
       "    -0.009826060384511948,\n",
       "    -0.03247936815023422,\n",
       "    -0.05486081913113594,\n",
       "    0.06584948301315308,\n",
       "    -0.03361109644174576,\n",
       "    -0.04265999048948288,\n",
       "    -0.08282234519720078,\n",
       "    -0.04273312911391258,\n",
       "    -0.042551685124635696,\n",
       "    -0.023747481405735016,\n",
       "    0.03946966677904129,\n",
       "    -0.0025799560826271772,\n",
       "    -0.013424714095890522,\n",
       "    0.018610961735248566,\n",
       "    0.052407003939151764,\n",
       "    -0.011470730416476727,\n",
       "    -0.04451518878340721,\n",
       "    0.029212303459644318,\n",
       "    -0.022209787741303444,\n",
       "    -0.011734017170965672,\n",
       "    0.03777329623699188,\n",
       "    0.027555100619792938,\n",
       "    -0.06586010009050369,\n",
       "    0.07345513999462128,\n",
       "    -0.01484470535069704,\n",
       "    0.04455801844596863,\n",
       "    0.024480057880282402,\n",
       "    0.029900411143898964,\n",
       "    0.01457166951149702,\n",
       "    -0.04015876352787018,\n",
       "    -0.0012895422987639904,\n",
       "    -0.04662027582526207,\n",
       "    0.02351497672498226,\n",
       "    -0.014515599235892296,\n",
       "    0.019699513912200928,\n",
       "    0.07638715952634811,\n",
       "    0.035519517958164215,\n",
       "    -0.047156162559986115,\n",
       "    -0.02040129154920578,\n",
       "    -0.05699419230222702,\n",
       "    0.00526686804369092,\n",
       "    0.004818127024918795,\n",
       "    0.051262266933918,\n",
       "    -0.018436847254633904,\n",
       "    -0.04281207546591759,\n",
       "    -0.0007001038175076246,\n",
       "    -0.03766622394323349,\n",
       "    0.036104463040828705,\n",
       "    0.01348663680255413,\n",
       "    -0.019805584102869034,\n",
       "    0.017978468909859657,\n",
       "    -0.015940355136990547,\n",
       "    0.006151593290269375,\n",
       "    0.053916238248348236,\n",
       "    -0.04241976886987686,\n",
       "    0.039941173046827316,\n",
       "    0.046548888087272644,\n",
       "    0.03344668075442314,\n",
       "    0.004510487429797649,\n",
       "    -0.005926397629082203,\n",
       "    0.005086252465844154,\n",
       "    -0.08490526676177979,\n",
       "    0.02077362686395645,\n",
       "    0.0024801294784992933,\n",
       "    0.07453303784132004,\n",
       "    0.014238282106816769,\n",
       "    -0.019249871373176575,\n",
       "    0.03294796496629715,\n",
       "    -0.07998213171958923,\n",
       "    -0.0415906086564064,\n",
       "    -0.029388949275016785,\n",
       "    -0.01608511619269848,\n",
       "    0.006543195806443691,\n",
       "    -0.01535787433385849,\n",
       "    0.04981718212366104,\n",
       "    0.0224620271474123,\n",
       "    0.022486984729766846,\n",
       "    -0.0018248830456286669,\n",
       "    -0.08640242367982864,\n",
       "    -0.02855193056166172,\n",
       "    0.03215792030096054,\n",
       "    -0.06288816779851913,\n",
       "    0.026540925726294518,\n",
       "    -0.028787050396203995,\n",
       "    0.0008679447346366942,\n",
       "    -0.03726107254624367,\n",
       "    -0.029302481561899185,\n",
       "    -0.01781545951962471,\n",
       "    0.008119534701108932,\n",
       "    -0.008409394882619381,\n",
       "    -0.08589079976081848,\n",
       "    0.021514615043997765,\n",
       "    -0.061612486839294434,\n",
       "    -0.01645432598888874,\n",
       "    0.03082295134663582,\n",
       "    0.027753813192248344,\n",
       "    0.009362776763737202,\n",
       "    -0.04926672577857971,\n",
       "    -0.009018383920192719,\n",
       "    0.024931184947490692,\n",
       "    0.06310144811868668,\n",
       "    -0.01768646202981472,\n",
       "    -0.025976724922657013,\n",
       "    -0.0531926266849041,\n",
       "    0.036124832928180695,\n",
       "    0.037130385637283325,\n",
       "    0.024237891659140587,\n",
       "    -0.033032022416591644,\n",
       "    -0.02754468284547329,\n",
       "    -0.003666004864498973,\n",
       "    -0.06426715105772018,\n",
       "    0.04538903385400772,\n",
       "    0.013540263287723064,\n",
       "    0.0347166508436203,\n",
       "    -0.01989666558802128,\n",
       "    -0.047451358288526535,\n",
       "    -0.009944423101842403,\n",
       "    -0.004998987074941397,\n",
       "    0.002527728909626603,\n",
       "    0.00574401393532753,\n",
       "    0.010532109998166561,\n",
       "    0.022268397733569145,\n",
       "    -0.011463948525488377,\n",
       "    0.046125441789627075,\n",
       "    0.009482878260314465,\n",
       "    0.0011835407931357622,\n",
       "    -0.03328300639986992,\n",
       "    -0.07588958740234375,\n",
       "    -0.0007252652430906892,\n",
       "    -0.005998760461807251,\n",
       "    0.023308362811803818,\n",
       "    -0.05456045642495155,\n",
       "    0.005636801011860371,\n",
       "    0.003967095632106066,\n",
       "    0.033693037927150726,\n",
       "    -0.011802753433585167,\n",
       "    -0.018618756905198097,\n",
       "    0.005548468325287104,\n",
       "    0.03687803074717522,\n",
       "    0.03936385735869408,\n",
       "    0.013749375939369202,\n",
       "    -0.03495120257139206,\n",
       "    -0.00660120090469718,\n",
       "    0.029673676937818527,\n",
       "    -0.011077551171183586,\n",
       "    0.020645204931497574,\n",
       "    -0.007371795829385519,\n",
       "    -0.035960663110017776,\n",
       "    0.007805395405739546,\n",
       "    -0.013200115412473679,\n",
       "    0.06559932976961136,\n",
       "    0.05470121279358864,\n",
       "    0.03338934853672981,\n",
       "    -0.006064132321625948,\n",
       "    -0.039700739085674286,\n",
       "    -0.04308469593524933,\n",
       "    -0.015716347843408585,\n",
       "    -0.020288189873099327,\n",
       "    0.026356985792517662,\n",
       "    -0.05057236924767494,\n",
       "    -0.014010332524776459,\n",
       "    0.07269476354122162,\n",
       "    0.05204583331942558,\n",
       "    0.004097494762390852,\n",
       "    -0.03887111693620682,\n",
       "    0.004729034379124641,\n",
       "    -0.005792808718979359,\n",
       "    0.006925120484083891,\n",
       "    0.014352870173752308,\n",
       "    -5.3198550703057614e-33,\n",
       "    0.025530193001031876,\n",
       "    -0.07919244468212128,\n",
       "    0.06097247079014778,\n",
       "    0.060325268656015396,\n",
       "    0.013618449680507183,\n",
       "    -0.025629863142967224,\n",
       "    0.0497102290391922,\n",
       "    0.021151207387447357,\n",
       "    -0.03878750652074814,\n",
       "    -0.014924533665180206,\n",
       "    -0.02400919981300831,\n",
       "    0.03716260939836502,\n",
       "    0.019705738872289658,\n",
       "    0.029770156368613243,\n",
       "    -0.0006264020339585841,\n",
       "    0.0341629721224308,\n",
       "    0.016615092754364014,\n",
       "    0.029951054602861404,\n",
       "    0.023681361228227615,\n",
       "    -0.0038951828610152006,\n",
       "    -0.09843778610229492,\n",
       "    0.01765751838684082,\n",
       "    -0.016754552721977234,\n",
       "    -0.06287919729948044,\n",
       "    0.028076838701963425,\n",
       "    -0.03526687249541283,\n",
       "    -0.011285034939646721,\n",
       "    -0.026161128655076027,\n",
       "    -0.012685693800449371,\n",
       "    0.004370775539427996,\n",
       "    0.04101645573973656,\n",
       "    -0.0014377441257238388,\n",
       "    0.02820240519940853,\n",
       "    -0.042511969804763794,\n",
       "    -0.020208565518260002,\n",
       "    0.010140366852283478,\n",
       "    -0.05550669878721237,\n",
       "    0.01824307069182396,\n",
       "    0.041660334914922714,\n",
       "    0.04836851730942726,\n",
       "    0.020840607583522797,\n",
       "    -0.04682648181915283,\n",
       "    0.028480658307671547,\n",
       "    -0.03325286880135536,\n",
       "    -0.013319986872375011,\n",
       "    -0.0015235170722007751,\n",
       "    0.07806528359651566,\n",
       "    -0.012080162763595581,\n",
       "    0.024663042277097702,\n",
       "    -0.026209305971860886,\n",
       "    0.009259282611310482,\n",
       "    0.006611401680856943,\n",
       "    -0.1009388342499733,\n",
       "    0.039385803043842316,\n",
       "    0.036385685205459595,\n",
       "    0.07236000150442123,\n",
       "    0.009395097382366657,\n",
       "    -0.0009411489591002464,\n",
       "    -0.05403721705079079,\n",
       "    0.05254344642162323,\n",
       "    -0.03720244765281677,\n",
       "    -0.016376571729779243,\n",
       "    0.06771925091743469,\n",
       "    0.02573997527360916,\n",
       "    0.028230350464582443,\n",
       "    0.004054411314427853,\n",
       "    0.07816790789365768,\n",
       "    0.010872437618672848,\n",
       "    0.03495856001973152,\n",
       "    0.0033203635830432177,\n",
       "    -0.03387122601270676,\n",
       "    0.05948396399617195,\n",
       "    -0.04303181171417236,\n",
       "    0.01164382603019476,\n",
       "    0.001115840976126492,\n",
       "    -0.031104303896427155,\n",
       "    0.0006784420693293214,\n",
       "    0.021075265482068062,\n",
       "    0.06509792804718018,\n",
       "    -0.05571305379271507,\n",
       "    -0.04849149286746979,\n",
       "    0.03052739053964615,\n",
       "    0.012612320482730865,\n",
       "    -0.0317223034799099,\n",
       "    -0.0056676664389669895,\n",
       "    -0.057305704802274704,\n",
       "    0.01150877121835947,\n",
       "    0.004387973807752132,\n",
       "    0.003893491579219699,\n",
       "    -0.06360672414302826,\n",
       "    -0.0048067569732666016,\n",
       "    0.0924004539847374,\n",
       "    -0.015648433938622475,\n",
       "    -0.011290234513580799,\n",
       "    -0.008098522201180458,\n",
       "    -0.04078374803066254,\n",
       "    -0.0038660902064293623,\n",
       "    0.00902548898011446,\n",
       "    0.020339926704764366,\n",
       "    0.01891466975212097,\n",
       "    -0.025141431018710136,\n",
       "    -0.012926463037729263,\n",
       "    -0.026274744421243668,\n",
       "    0.014654025435447693,\n",
       "    0.05455981194972992,\n",
       "    0.04599389806389809,\n",
       "    -0.03889373689889908,\n",
       "    0.005773245822638273,\n",
       "    0.00011719930625986308,\n",
       "    -0.016887741163372993,\n",
       "    0.002397171687334776,\n",
       "    0.009346839971840382,\n",
       "    -0.031448978930711746,\n",
       "    0.047726716846227646,\n",
       "    -0.013839301653206348,\n",
       "    0.01159453485161066,\n",
       "    0.024414828047156334,\n",
       "    0.0038938617799431086,\n",
       "    -0.03073112852871418,\n",
       "    -0.01208746712654829,\n",
       "    0.02208280749619007,\n",
       "    -0.048050299286842346,\n",
       "    -0.03413677588105202,\n",
       "    -0.025296472012996674,\n",
       "    -0.012902842834591866,\n",
       "    -0.006800326053053141,\n",
       "    -0.01426014956086874,\n",
       "    0.009253083728253841,\n",
       "    0.028225449845194817,\n",
       "    -0.03029428981244564,\n",
       "    -0.0008805467514321208,\n",
       "    0.0032710619270801544,\n",
       "    2.696182264116942e-07,\n",
       "    0.01006595604121685,\n",
       "    0.01750737428665161,\n",
       "    -0.0026380878407508135,\n",
       "    0.025775745511054993,\n",
       "    0.032807622104883194,\n",
       "    -0.008457052521407604,\n",
       "    -0.00807629432529211,\n",
       "    -0.000824505346827209,\n",
       "    0.008693741634488106,\n",
       "    0.033425942063331604,\n",
       "    -0.011317513883113861,\n",
       "    -0.0010884524090215564,\n",
       "    -0.00816332083195448,\n",
       "    -0.04109366238117218,\n",
       "    -0.03798969089984894,\n",
       "    -0.09356921911239624,\n",
       "    -0.01005097571760416,\n",
       "    -0.021934906020760536,\n",
       "    -0.05539681017398834,\n",
       "    0.05970139801502228,\n",
       "    0.015465094707906246,\n",
       "    0.0482700876891613,\n",
       "    0.006164062302559614,\n",
       "    -0.010109290480613708,\n",
       "    -0.022536057978868484,\n",
       "    -0.026238661259412766,\n",
       "    -0.05290750041604042,\n",
       "    -0.024671228602528572,\n",
       "    0.05332507938146591,\n",
       "    0.055149201303720474,\n",
       "    0.07019603997468948,\n",
       "    0.012043501250445843,\n",
       "    0.025113314390182495,\n",
       "    0.03851528838276863,\n",
       "    -0.006961533799767494,\n",
       "    -0.025078926235437393,\n",
       "    0.017024939879775047,\n",
       "    0.0765799880027771,\n",
       "    -0.0075508845038712025,\n",
       "    0.0256908368319273,\n",
       "    0.03908095508813858,\n",
       "    0.017166631296277046,\n",
       "    -0.0043221754021942616,\n",
       "    -0.050889406353235245,\n",
       "    0.061744172126054764,\n",
       "    0.015518664382398129,\n",
       "    -0.06536046415567398,\n",
       "    -0.065017931163311,\n",
       "    -0.03136415779590607,\n",
       "    -0.015442021191120148,\n",
       "    -0.004054023884236813,\n",
       "    -0.010215743444859982,\n",
       "    0.023907670751214027,\n",
       "    0.03158268705010414,\n",
       "    0.007299425080418587,\n",
       "    -0.07457439601421356,\n",
       "    -0.042027220129966736,\n",
       "    0.02183554321527481,\n",
       "    0.011352422647178173,\n",
       "    0.00607758155092597,\n",
       "    -0.012970400042831898,\n",
       "    -0.045652952045202255,\n",
       "    0.026344401761889458,\n",
       "    -0.04547524079680443,\n",
       "    0.09917163848876953,\n",
       "    0.001964098773896694,\n",
       "    0.006482806988060474,\n",
       "    2.082653647065579e-34,\n",
       "    -0.009409044869244099,\n",
       "    0.03142071142792702,\n",
       "    -0.023186590522527695,\n",
       "    0.01753874123096466,\n",
       "    0.04366634413599968,\n",
       "    -0.045325037091970444,\n",
       "    -0.007411911152303219,\n",
       "    0.0004422649508342147,\n",
       "    -0.016187544912099838,\n",
       "    0.047714777290821075,\n",
       "    -0.019077884033322334]}},\n",
       " {'_index': 'ml-interview-questions',\n",
       "  '_id': 'dMXylZIByzqUH6XnClhj',\n",
       "  '_score': 1.8205687,\n",
       "  '_source': {'chapter': 'CHAPTER 5',\n",
       "   'title': 'Technical Interview: Coding',\n",
       "   'section': 'Starting from Scratch: Learning Roadmap If You Don’t Know Python',\n",
       "   'text': 'In the previous chapters, I walked through the ML interview process and the ML algorithms and model training portions that are part of the technical interview. Tech‐ nical interviews can ask for much more of candidates beyond ML algorithms, stats knowledge, and model training, though. This chapter will cover one of those pieces, which is the coding interview. For jobs in machine learning, the kinds of coding that could be asked for will differ among companies and even among teams in the company. For example, when I was interviewing for data scientist and MLE roles, I got the following types of coding questions and tasks: • Company 1: Python questions related to data manipulation in pandas • Company 2: Python brainteaser questions (“LeetCode style”) only • Company 3: Data-related coding questions in SQL and Python Pandas • Company 4: Take-home coding exercise with a real-life scenario to code …and so on. There is a large variance in what companies might ask in a coding round of interviews. From what I’ve seen personally and heard from colleagues working as software engineers and hiring managers of software engineers, the ML coding inter‐ views are less standardized than the technical interviews for software engineering roles. On the bright side, interviewers for some ML roles don’t always ask candidates the most difficult “LeetCode style” questions, aka “LeetCode hards,” 1 since the candidate can be evaluated on their other skills, such as ML algorithm knowledge. This heavily depends on the role, though: for example, a candidate interviewing for an engineer title (such as MLE) at big tech gets the usual software engineering loop questions, which could include LeetCode hards. As usual, check with your recruiter. As I mentioned in “The Three Pillars of Machine Learning Roles” on page 12 , people working in ML are expected to clear both bars of programming and ML/stats. You aren’t expected to be the most skilled coder who is even more experienced than the average candidate for software engineer roles, but you need to be good enough to easily work with a team. You might encounter some teams where all you need is ML/stats knowledge, where you work with smaller scale data and never need to put things into production. Those companies may not test your coding skills during the interview process. But for this book, I tend to aim more toward ML roles where there is a blend of software engi‐ neering and ML since, without a way to distribute and serve the ML models, they don’t become part of the products that we use so much in our day to day. ML itself doesn’t make a Netflix recommender system, it’s putting that model into production and being a part of the frontend experience that customers interact with that brings in the user delight and thus revenue from ML. (For more on interviews that test your knowledge of model deployment, see Chapter 6 .) In this chapter, I’ll break down the common types of programming questions that show up in ML job interviews and walk through how to prepare: • A learning roadmap if you don’t know Python • Python questions related to data • Python brainteaser questions • SQL questions related to data Feel free to skip this section if you already know Python! The reasons I focus on knowing Python are: • Most, if not all, of the interviews for ML that I’ve been through assume you know Python to some extent. Most ML libraries you’ll be using at work have an imple‐ mentation in Python. • For interviewing, even software engineers recommend using Python in language • For interviewing, even software engineers recommend using Python in languageagnostic interviews over other languages because of the speed (of coding) and abstractions. Instead of spending valuable interview time coding out from scratch in C or C++, you could instead code just one or two lines in Python, which helps you focus on the important things in the interview. • Often in an ML job, you’ll be collaborating with other individuals and teams. Even if ML might be your main skill, the interviewers will be looking to see if you can write readable code that colleagues can read and use. Given these reasons, here’s a suggested roadmap to self-learn if you don’t know any Python or if you’re a little rusty. Find a book or course that focuses on practical code where you can immediately see the results. 2 Do the exercises in the book. Here are some resources I recommend: • Automate the Boring Stuff with Python by Al Sweigart (No Starch Press): it’s free online, and the exercises are related to day-to-day activities. It starts with the bare basics of Python, such as using it for math ( 2+2 is the first line of code in the book), so anyone can get started with it. • For those who prefer video format, Al Sweigart has a playlist on YouTube that walks through his book in 15 video lessons. • Learn Python, Data Viz, pandas and More, tutorials at Kaggle . There are many similar online coding platforms, the most common (in North Amer‐ ica) being LeetCode and HackerRank . For simplicity, I will be using these two plat‐ forms the most as examples, but there might be other equivalents in your region or locale. Two classic beginner questions are Fizz Buzz and Two Sum . Try them on your plat‐ form of choice; it’s OK if you need to look at the answers during this early stage of learning. The goal is to understand the snippet of code—if you can understand this Even trying to complete one question a day or one question every two days is helpful for a beginner. Keep a tracker in your journal or phone. Try for an hour, and if you get stuck, look at the answers and read them or find video explanations until you understand . You can redo the question after a few more days to see if the understand‐ ing stuck. Avoid using rote memorization since the interview likely won’t have the exact same question, unless you’re supremely lucky. After you become familiar with some of the basics in Python, start with these tutori‐ als for using ML: • CatBoost: “Tutorial” • NumPy: “The Absolute Basics for Beginners” • pandas: “10 Minutes to pandas” The CatBoost tutorial can be a starting point for your first ML model! After this step, you can try using your own dataset or make modifications, try out other types of models, and build your own projects. Now, I’ll go through some tips for coding questions that may be asked during ML job interviews.',\n",
       "   'text_vector': [0.05708177015185356,\n",
       "    -0.042599841952323914,\n",
       "    -0.047992072999477386,\n",
       "    -0.010543390177190304,\n",
       "    0.01210465282201767,\n",
       "    0.053173258900642395,\n",
       "    -0.016555434092879295,\n",
       "    -0.021212337538599968,\n",
       "    0.004995518364012241,\n",
       "    -0.03513846918940544,\n",
       "    0.046387627720832825,\n",
       "    0.032417744398117065,\n",
       "    0.03670348972082138,\n",
       "    0.07369086891412735,\n",
       "    0.022741736844182014,\n",
       "    -0.005903944373130798,\n",
       "    -0.006801535841077566,\n",
       "    -0.016809603199362755,\n",
       "    -0.010308695025742054,\n",
       "    -0.019147170707583427,\n",
       "    -0.035429492592811584,\n",
       "    0.011087627150118351,\n",
       "    -0.014741482213139534,\n",
       "    -0.021300440654158592,\n",
       "    0.002473601372912526,\n",
       "    -0.036017801612615585,\n",
       "    -0.03307446464896202,\n",
       "    0.026103096082806587,\n",
       "    -0.022865289822220802,\n",
       "    0.013026533648371696,\n",
       "    0.01148564089089632,\n",
       "    -0.029928624629974365,\n",
       "    0.003682612907141447,\n",
       "    -0.01679949462413788,\n",
       "    2.300775122421328e-06,\n",
       "    -0.02013636752963066,\n",
       "    0.02180297300219536,\n",
       "    0.00814453698694706,\n",
       "    -0.06144591048359871,\n",
       "    -0.08957033604383469,\n",
       "    0.0024274312891066074,\n",
       "    -0.04740283638238907,\n",
       "    0.058285828679800034,\n",
       "    0.03350693732500076,\n",
       "    -0.0478825643658638,\n",
       "    -0.026461781933903694,\n",
       "    0.10011987388134003,\n",
       "    -0.01871151477098465,\n",
       "    0.018341241404414177,\n",
       "    0.040133990347385406,\n",
       "    0.013668403960764408,\n",
       "    -0.023373331874608994,\n",
       "    0.015468677505850792,\n",
       "    0.04525576904416084,\n",
       "    0.008761503733694553,\n",
       "    0.002052171388641,\n",
       "    -0.0078558549284935,\n",
       "    0.04511360824108124,\n",
       "    -0.0016628424637019634,\n",
       "    -0.026755018159747124,\n",
       "    0.015693185850977898,\n",
       "    0.008516225032508373,\n",
       "    0.0516849122941494,\n",
       "    0.027600685134530067,\n",
       "    0.031118139624595642,\n",
       "    0.011407773941755295,\n",
       "    -0.013236494734883308,\n",
       "    -0.010427882894873619,\n",
       "    -0.03607591241598129,\n",
       "    0.003245704574510455,\n",
       "    0.06162778660655022,\n",
       "    -0.01584761030972004,\n",
       "    -0.049998074769973755,\n",
       "    -0.01564307138323784,\n",
       "    0.05214828997850418,\n",
       "    -0.019735058769583702,\n",
       "    -0.0023333835415542126,\n",
       "    0.020948277786374092,\n",
       "    -0.02191968820989132,\n",
       "    -0.054482199251651764,\n",
       "    -0.04158652573823929,\n",
       "    0.022754710167646408,\n",
       "    -0.012471419759094715,\n",
       "    -0.01595880836248398,\n",
       "    -0.018881717696785927,\n",
       "    0.0444173589348793,\n",
       "    -0.015311903320252895,\n",
       "    -0.012798769399523735,\n",
       "    0.03623946011066437,\n",
       "    0.00518700061365962,\n",
       "    0.10563602298498154,\n",
       "    0.020545974373817444,\n",
       "    -0.002465135185047984,\n",
       "    0.04036416858434677,\n",
       "    -0.03568485006690025,\n",
       "    -0.009320062585175037,\n",
       "    -0.0018022038275375962,\n",
       "    0.061246007680892944,\n",
       "    -0.03465380519628525,\n",
       "    -0.024831023067235947,\n",
       "    -0.028797419741749763,\n",
       "    -0.010470631532371044,\n",
       "    0.021828031167387962,\n",
       "    -0.007851802743971348,\n",
       "    0.02636943943798542,\n",
       "    0.014419634826481342,\n",
       "    0.02101036347448826,\n",
       "    0.0007570994785055518,\n",
       "    -0.0571305975317955,\n",
       "    -0.07455772906541824,\n",
       "    0.006569236982613802,\n",
       "    -0.018486090004444122,\n",
       "    -0.015333883464336395,\n",
       "    0.023295948281884193,\n",
       "    0.004101539496332407,\n",
       "    0.05952441319823265,\n",
       "    0.02911348268389702,\n",
       "    0.0073599484749138355,\n",
       "    -0.007299255579710007,\n",
       "    -0.0030724364332854748,\n",
       "    -0.0789894089102745,\n",
       "    0.018138829618692398,\n",
       "    0.03458163142204285,\n",
       "    0.0333201102912426,\n",
       "    -0.00026963872369378805,\n",
       "    -0.08119262009859085,\n",
       "    -0.0085907606408,\n",
       "    -0.01121094822883606,\n",
       "    0.021342391148209572,\n",
       "    -0.04431064426898956,\n",
       "    0.021848876029253006,\n",
       "    0.029998214915394783,\n",
       "    -0.010856973007321358,\n",
       "    -0.04798704385757446,\n",
       "    0.0541682168841362,\n",
       "    -0.00483567314222455,\n",
       "    -0.007966111414134502,\n",
       "    0.06342925131320953,\n",
       "    -0.016680490225553513,\n",
       "    0.0023663907777518034,\n",
       "    0.03142349421977997,\n",
       "    -0.047065623104572296,\n",
       "    0.05328769609332085,\n",
       "    0.0004850119003094733,\n",
       "    0.018055517226457596,\n",
       "    0.016664793714880943,\n",
       "    -0.013212963938713074,\n",
       "    -0.06621565669775009,\n",
       "    -0.012047655880451202,\n",
       "    0.022067055106163025,\n",
       "    -0.024180935695767403,\n",
       "    0.05680970102548599,\n",
       "    0.019960051402449608,\n",
       "    -0.01567792147397995,\n",
       "    0.0484270416200161,\n",
       "    -0.0036377636715769768,\n",
       "    -0.017506223171949387,\n",
       "    0.05129297822713852,\n",
       "    0.01520521566271782,\n",
       "    0.047235533595085144,\n",
       "    0.01907687447965145,\n",
       "    -0.10944303870201111,\n",
       "    0.06561213731765747,\n",
       "    0.03722282871603966,\n",
       "    0.0009193832520395517,\n",
       "    -0.011878889985382557,\n",
       "    -0.03674943372607231,\n",
       "    0.07626964151859283,\n",
       "    0.005594850052148104,\n",
       "    0.011837663128972054,\n",
       "    0.046261291950941086,\n",
       "    0.030181916430592537,\n",
       "    -0.013290886767208576,\n",
       "    -0.038368333131074905,\n",
       "    -0.013467106968164444,\n",
       "    0.029712414368987083,\n",
       "    0.019414599984884262,\n",
       "    -0.002616195473819971,\n",
       "    0.039178069680929184,\n",
       "    0.038618627935647964,\n",
       "    0.03024008311331272,\n",
       "    0.041384484618902206,\n",
       "    -0.013692019507288933,\n",
       "    0.051956046372652054,\n",
       "    -0.031206270679831505,\n",
       "    -0.03867103159427643,\n",
       "    -0.055424414575099945,\n",
       "    0.04419305920600891,\n",
       "    -0.04149570316076279,\n",
       "    -0.029334131628274918,\n",
       "    0.00956505723297596,\n",
       "    0.026416810229420662,\n",
       "    0.052747052162885666,\n",
       "    0.067166768014431,\n",
       "    -0.0336269810795784,\n",
       "    -0.02844439446926117,\n",
       "    -0.057765714824199677,\n",
       "    -0.004653941839933395,\n",
       "    -0.0623558945953846,\n",
       "    -0.03693748638033867,\n",
       "    -0.08250260353088379,\n",
       "    -0.04267197474837303,\n",
       "    -0.0769961029291153,\n",
       "    -0.0020838596392422915,\n",
       "    0.04800952225923538,\n",
       "    0.01921067200601101,\n",
       "    -0.044511206448078156,\n",
       "    -0.04133034124970436,\n",
       "    -0.0385386161506176,\n",
       "    -0.023432902991771698,\n",
       "    0.07757747173309326,\n",
       "    -0.03392157703638077,\n",
       "    -0.03209282085299492,\n",
       "    -0.013494051992893219,\n",
       "    -0.01245670486241579,\n",
       "    -0.02114584669470787,\n",
       "    -0.010712477378547192,\n",
       "    -0.06047890707850456,\n",
       "    0.0021766729187220335,\n",
       "    -0.02377891167998314,\n",
       "    -0.009505930356681347,\n",
       "    0.012973075732588768,\n",
       "    -0.05714752525091171,\n",
       "    0.007441928144544363,\n",
       "    -0.034490395337343216,\n",
       "    -0.019747937098145485,\n",
       "    0.009404470212757587,\n",
       "    0.004271050449460745,\n",
       "    0.04335494711995125,\n",
       "    0.032118313014507294,\n",
       "    0.00653597479686141,\n",
       "    0.024966998025774956,\n",
       "    0.0009565021609887481,\n",
       "    -0.0042351591400802135,\n",
       "    0.01462081354111433,\n",
       "    -0.015375939197838306,\n",
       "    -0.012314212508499622,\n",
       "    -0.006486301776021719,\n",
       "    0.012399687431752682,\n",
       "    -0.012375004589557648,\n",
       "    -0.06511188298463821,\n",
       "    0.0396406315267086,\n",
       "    -0.002709371969103813,\n",
       "    0.04303992912173271,\n",
       "    -0.017828304320573807,\n",
       "    -0.015007684007287025,\n",
       "    -0.02885802648961544,\n",
       "    0.022571716457605362,\n",
       "    -0.03236157447099686,\n",
       "    -0.018142297863960266,\n",
       "    0.030810384079813957,\n",
       "    0.012528392486274242,\n",
       "    -0.006130093242973089,\n",
       "    0.02969186194241047,\n",
       "    0.04521241411566734,\n",
       "    0.0032890758011490107,\n",
       "    0.03959599882364273,\n",
       "    0.036246269941329956,\n",
       "    -0.06493651121854782,\n",
       "    -0.0056515964679419994,\n",
       "    0.04280178248882294,\n",
       "    -0.032251063734292984,\n",
       "    0.028196994215250015,\n",
       "    0.017213182523846626,\n",
       "    -0.01214798167347908,\n",
       "    -0.03813714534044266,\n",
       "    0.013252952136099339,\n",
       "    -0.03914995118975639,\n",
       "    -0.013950268737971783,\n",
       "    0.001565565587952733,\n",
       "    -2.5791847292566672e-05,\n",
       "    -0.002462210366502404,\n",
       "    -0.03244590759277344,\n",
       "    -0.020848751068115234,\n",
       "    -0.014609874226152897,\n",
       "    -0.04950273409485817,\n",
       "    -0.04103420302271843,\n",
       "    -0.025681348517537117,\n",
       "    0.0077540152706205845,\n",
       "    0.008462565951049328,\n",
       "    -0.01017968449741602,\n",
       "    -0.016280008479952812,\n",
       "    0.05243442580103874,\n",
       "    -0.005288973916321993,\n",
       "    -0.029993100091814995,\n",
       "    0.04838532209396362,\n",
       "    0.01501853670924902,\n",
       "    -0.02761908620595932,\n",
       "    0.003907416947185993,\n",
       "    0.033876437693834305,\n",
       "    -0.0009199474006891251,\n",
       "    -0.013017323799431324,\n",
       "    -0.00367606314830482,\n",
       "    -0.011191138066351414,\n",
       "    0.014882772229611874,\n",
       "    0.03368920087814331,\n",
       "    0.005357766058295965,\n",
       "    0.09699862450361252,\n",
       "    -0.003667956916615367,\n",
       "    0.05034346505999565,\n",
       "    0.004965401720255613,\n",
       "    0.003965898882597685,\n",
       "    0.02921455353498459,\n",
       "    0.032674841582775116,\n",
       "    0.057173699140548706,\n",
       "    0.004953510593622923,\n",
       "    0.0006795049994252622,\n",
       "    -0.005781187210232019,\n",
       "    0.01910305954515934,\n",
       "    -0.0033186343498528004,\n",
       "    0.01214689388871193,\n",
       "    -0.024091321974992752,\n",
       "    0.00696578761562705,\n",
       "    0.07208835333585739,\n",
       "    0.04591226577758789,\n",
       "    -0.05636473745107651,\n",
       "    -0.028822509571909904,\n",
       "    0.03247072547674179,\n",
       "    -0.03174438327550888,\n",
       "    0.10177955776453018,\n",
       "    -0.023215467110276222,\n",
       "    -0.07104851305484772,\n",
       "    0.019835080951452255,\n",
       "    -0.018750593066215515,\n",
       "    -0.03855154663324356,\n",
       "    -0.07656262814998627,\n",
       "    -0.06469554454088211,\n",
       "    0.011084395460784435,\n",
       "    0.08479488641023636,\n",
       "    -0.0006345708970911801,\n",
       "    0.02701285108923912,\n",
       "    0.013366262428462505,\n",
       "    -0.0345805399119854,\n",
       "    -0.03022918663918972,\n",
       "    -0.005083493888378143,\n",
       "    -0.04252857342362404,\n",
       "    -0.07621165364980698,\n",
       "    -0.06126675382256508,\n",
       "    -0.016106238588690758,\n",
       "    -0.0176442489027977,\n",
       "    -0.043008774518966675,\n",
       "    -0.015479590743780136,\n",
       "    -0.029896965250372887,\n",
       "    0.03183813765645027,\n",
       "    0.046324990689754486,\n",
       "    0.014798332005739212,\n",
       "    0.00889794621616602,\n",
       "    0.006502046249806881,\n",
       "    0.008779930882155895,\n",
       "    -0.02414429746568203,\n",
       "    0.011526769027113914,\n",
       "    -0.02014748938381672,\n",
       "    -0.006686934269964695,\n",
       "    -0.001045038690790534,\n",
       "    0.021456481888890266,\n",
       "    -0.0021330276504158974,\n",
       "    0.02558218501508236,\n",
       "    0.046069350093603134,\n",
       "    0.006858752574771643,\n",
       "    0.03658333793282509,\n",
       "    0.029200494289398193,\n",
       "    0.014172104187309742,\n",
       "    0.02946629375219345,\n",
       "    0.029601192101836205,\n",
       "    -0.03785286098718643,\n",
       "    0.05332207679748535,\n",
       "    0.06197146698832512,\n",
       "    -0.03477172926068306,\n",
       "    0.04204126074910164,\n",
       "    0.049711573868989944,\n",
       "    0.01738862879574299,\n",
       "    -0.014968367293477058,\n",
       "    0.0230706874281168,\n",
       "    -0.0011450775200501084,\n",
       "    0.019339466467499733,\n",
       "    -0.0415174663066864,\n",
       "    -0.018395183607935905,\n",
       "    -0.011419092305004597,\n",
       "    -0.03684987500309944,\n",
       "    0.053157415241003036,\n",
       "    0.09937801957130432,\n",
       "    -0.13185329735279083,\n",
       "    -0.003153714118525386,\n",
       "    0.00655793771147728,\n",
       "    0.007438458502292633,\n",
       "    -0.06107783317565918,\n",
       "    -0.015857500955462456,\n",
       "    -0.04920665919780731,\n",
       "    -0.05232476815581322,\n",
       "    0.08220715820789337,\n",
       "    -0.0034427721984684467,\n",
       "    -0.010525761172175407,\n",
       "    -0.1231822744011879,\n",
       "    -0.0348835326731205,\n",
       "    -0.020910004153847694,\n",
       "    -0.017923545092344284,\n",
       "    0.043672915548086166,\n",
       "    0.048653148114681244,\n",
       "    0.02685435302555561,\n",
       "    0.03206900879740715,\n",
       "    0.031159894540905952,\n",
       "    -0.026492144912481308,\n",
       "    -0.02598804049193859,\n",
       "    0.030113857239484787,\n",
       "    0.02483564056456089,\n",
       "    0.007205533795058727,\n",
       "    0.03824760019779205,\n",
       "    -0.013850213028490543,\n",
       "    -0.029041649773716927,\n",
       "    0.08564355224370956,\n",
       "    -0.0033140871673822403,\n",
       "    0.008000940084457397,\n",
       "    0.02907329984009266,\n",
       "    0.011303715407848358,\n",
       "    0.01956953853368759,\n",
       "    -0.012590119615197182,\n",
       "    -0.009325078688561916,\n",
       "    0.0007392060942947865,\n",
       "    0.050997283309698105,\n",
       "    -0.03313498571515083,\n",
       "    0.03612813726067543,\n",
       "    0.03911849856376648,\n",
       "    0.01995346136391163,\n",
       "    -0.06872505694627762,\n",
       "    -0.031622257083654404,\n",
       "    -0.013025300577282906,\n",
       "    0.0022079707123339176,\n",
       "    -0.026030011475086212,\n",
       "    0.06311304122209549,\n",
       "    0.03387177735567093,\n",
       "    -0.03497572988271713,\n",
       "    0.03141535818576813,\n",
       "    -0.06191805377602577,\n",
       "    0.021794967353343964,\n",
       "    0.003410235047340393,\n",
       "    -0.04847681149840355,\n",
       "    0.02912161685526371,\n",
       "    0.02516583353281021,\n",
       "    0.028653861954808235,\n",
       "    0.03140790015459061,\n",
       "    -0.07187502086162567,\n",
       "    0.07925856858491898,\n",
       "    -0.001312135485932231,\n",
       "    0.012674286961555481,\n",
       "    -0.034205030649900436,\n",
       "    -0.01913502626121044,\n",
       "    -0.009360115975141525,\n",
       "    -0.08210017532110214,\n",
       "    0.022779718041419983,\n",
       "    0.01189272478222847,\n",
       "    0.03301113098859787,\n",
       "    0.022594884037971497,\n",
       "    -0.0422678180038929,\n",
       "    0.0303316842764616,\n",
       "    -0.0901808887720108,\n",
       "    -0.0261527169495821,\n",
       "    -0.023372502997517586,\n",
       "    -0.019401028752326965,\n",
       "    -0.0029254944529384375,\n",
       "    -0.013830617070198059,\n",
       "    0.024476759135723114,\n",
       "    0.013023758307099342,\n",
       "    0.021571775898337364,\n",
       "    -0.017155852168798447,\n",
       "    -0.04451069235801697,\n",
       "    -0.03315448760986328,\n",
       "    0.010980683378875256,\n",
       "    -0.058011606335639954,\n",
       "    0.020018314942717552,\n",
       "    -0.0125951012596488,\n",
       "    0.014887833967804909,\n",
       "    -0.01948215626180172,\n",
       "    -0.037267424166202545,\n",
       "    -0.0067139132879674435,\n",
       "    -0.0092623857781291,\n",
       "    -0.014142526313662529,\n",
       "    -0.06075999513268471,\n",
       "    0.004350265022367239,\n",
       "    -0.06253079324960709,\n",
       "    0.025285858660936356,\n",
       "    0.05801079049706459,\n",
       "    -0.005765673704445362,\n",
       "    0.04209626466035843,\n",
       "    -0.10409218072891235,\n",
       "    -0.011403877288103104,\n",
       "    0.021728957071900368,\n",
       "    0.06533005088567734,\n",
       "    -0.03120265156030655,\n",
       "    -0.04096762463450432,\n",
       "    -0.06132141873240471,\n",
       "    0.00708727678284049,\n",
       "    0.01662101410329342,\n",
       "    0.028279578313231468,\n",
       "    -0.034556251019239426,\n",
       "    -0.03485811874270439,\n",
       "    -0.010292179882526398,\n",
       "    -0.04698892682790756,\n",
       "    0.05932565778493881,\n",
       "    0.013552547432482243,\n",
       "    0.05996323004364967,\n",
       "    0.00703726802021265,\n",
       "    -0.042440272867679596,\n",
       "    0.002535145729780197,\n",
       "    -0.014240878634154797,\n",
       "    0.014941917732357979,\n",
       "    0.016596326604485512,\n",
       "    0.023282252252101898,\n",
       "    0.06401882320642471,\n",
       "    -0.013872339390218258,\n",
       "    0.05542443320155144,\n",
       "    -0.0024395522195845842,\n",
       "    0.010969910770654678,\n",
       "    -0.005210732109844685,\n",
       "    -0.08649025857448578,\n",
       "    0.020095473155379295,\n",
       "    -0.048053111881017685,\n",
       "    -0.01569773629307747,\n",
       "    -0.020304057747125626,\n",
       "    -0.05848192051053047,\n",
       "    0.026785189285874367,\n",
       "    0.04411955550312996,\n",
       "    -0.005929287523031235,\n",
       "    -0.05070124939084053,\n",
       "    0.02372194081544876,\n",
       "    0.033869873732328415,\n",
       "    0.045159656554460526,\n",
       "    0.015325994230806828,\n",
       "    -0.03743472695350647,\n",
       "    -0.03679615259170532,\n",
       "    0.06461046636104584,\n",
       "    0.024369286373257637,\n",
       "    0.0616937093436718,\n",
       "    -0.02510637603700161,\n",
       "    -0.01951761730015278,\n",
       "    0.010908452793955803,\n",
       "    -0.01938188262283802,\n",
       "    0.09383291006088257,\n",
       "    0.030521512031555176,\n",
       "    0.02606924995779991,\n",
       "    -0.012813079170882702,\n",
       "    -0.011757859960198402,\n",
       "    -0.07363173365592957,\n",
       "    -0.0239702258259058,\n",
       "    -0.04713656008243561,\n",
       "    0.02648007869720459,\n",
       "    -0.026051411405205727,\n",
       "    0.003508609952405095,\n",
       "    0.08542915433645248,\n",
       "    -0.016819484531879425,\n",
       "    0.021596526727080345,\n",
       "    -0.03794604912400246,\n",
       "    -0.007367839105427265,\n",
       "    0.048195332288742065,\n",
       "    -0.034134551882743835,\n",
       "    -0.003037211950868368,\n",
       "    -5.756379672929418e-33,\n",
       "    0.02282583899796009,\n",
       "    -0.06903506815433502,\n",
       "    0.0843302309513092,\n",
       "    0.033662132918834686,\n",
       "    0.006916026119142771,\n",
       "    -0.023714108392596245,\n",
       "    0.022224199026823044,\n",
       "    0.0402902290225029,\n",
       "    -0.0767301544547081,\n",
       "    -0.014128033071756363,\n",
       "    -0.010521848686039448,\n",
       "    0.02599281258881092,\n",
       "    0.00047865515807643533,\n",
       "    0.016314098611474037,\n",
       "    -0.016040099784731865,\n",
       "    -0.010734203271567822,\n",
       "    -0.016730528324842453,\n",
       "    -0.013231090269982815,\n",
       "    -0.0007014175062067807,\n",
       "    -0.011557461693882942,\n",
       "    -0.10292753577232361,\n",
       "    0.002008809708058834,\n",
       "    -0.022674547508358955,\n",
       "    -0.04387855902314186,\n",
       "    0.048980407416820526,\n",
       "    0.0013967828126624227,\n",
       "    0.01431545615196228,\n",
       "    0.006044451147317886,\n",
       "    -0.009650270454585552,\n",
       "    -0.0012694669421762228,\n",
       "    0.006746658124029636,\n",
       "    -0.008165268227458,\n",
       "    0.022129088640213013,\n",
       "    -0.08780642598867416,\n",
       "    -0.003331043291836977,\n",
       "    0.012982155196368694,\n",
       "    -0.03589789196848869,\n",
       "    -0.008243642747402191,\n",
       "    0.047521479427814484,\n",
       "    0.032032936811447144,\n",
       "    0.007122208829969168,\n",
       "    -0.048526883125305176,\n",
       "    0.07559788972139359,\n",
       "    -0.013978827744722366,\n",
       "    -0.012424317188560963,\n",
       "    -0.02446085959672928,\n",
       "    0.004286063369363546,\n",
       "    -0.017065905034542084,\n",
       "    0.027435319498181343,\n",
       "    -0.007753369398415089,\n",
       "    0.04566164314746857,\n",
       "    0.014058446511626244,\n",
       "    -0.09526677429676056,\n",
       "    0.002307000569999218,\n",
       "    0.03443591296672821,\n",
       "    0.05601667985320091,\n",
       "    -0.036528583616018295,\n",
       "    0.04472219571471214,\n",
       "    -0.011648517102003098,\n",
       "    0.048502370715141296,\n",
       "    -0.016842087730765343,\n",
       "    -0.002537295687943697,\n",
       "    0.037334248423576355,\n",
       "    -0.037105973809957504,\n",
       "    0.012588012963533401,\n",
       "    -0.011825937777757645,\n",
       "    0.08599713444709778,\n",
       "    -0.04818323999643326,\n",
       "    0.021966490894556046,\n",
       "    0.01832517795264721,\n",
       "    -0.02315063402056694,\n",
       "    0.026933899149298668,\n",
       "    -0.005416500847786665,\n",
       "    0.025100622326135635,\n",
       "    0.03522199019789696,\n",
       "    -0.012710237875580788,\n",
       "    0.006335387472063303,\n",
       "    0.013264917768537998,\n",
       "    0.013762488029897213,\n",
       "    -0.05814399942755699,\n",
       "    -0.040686190128326416,\n",
       "    0.008845883421599865,\n",
       "    -0.020027389749884605,\n",
       "    -0.05885236710309982,\n",
       "    -0.027096496894955635,\n",
       "    -0.008722991682589054,\n",
       "    -0.002037151949480176,\n",
       "    -0.0032491672318428755,\n",
       "    -0.004881937988102436,\n",
       "    -0.05754658579826355,\n",
       "    -0.02417413890361786,\n",
       "    0.05021338537335396,\n",
       "    0.015437339432537556,\n",
       "    -0.008917329832911491,\n",
       "    0.04450098052620888,\n",
       "    -0.005306422710418701,\n",
       "    -0.003691179445013404,\n",
       "    0.008645735681056976,\n",
       "    0.01044574473053217,\n",
       "    0.02860131300985813,\n",
       "    -0.0205079335719347,\n",
       "    0.016782542690634727,\n",
       "    0.010525894351303577,\n",
       "    -0.025148076936602592,\n",
       "    0.056766021996736526,\n",
       "    0.05028409883379936,\n",
       "    -0.028748057782649994,\n",
       "    0.023981306701898575,\n",
       "    -0.019337505102157593,\n",
       "    -0.03373196721076965,\n",
       "    -0.013685360550880432,\n",
       "    0.010751184076070786,\n",
       "    0.004238894674926996,\n",
       "    0.0831390991806984,\n",
       "    -0.022363439202308655,\n",
       "    0.023575622588396072,\n",
       "    0.021878903731703758,\n",
       "    -0.0032165786251425743,\n",
       "    -0.037413857877254486,\n",
       "    -0.03056752309203148,\n",
       "    -0.03253379091620445,\n",
       "    -0.06991488486528397,\n",
       "    -0.025412749499082565,\n",
       "    -0.009998788125813007,\n",
       "    -0.027315741404891014,\n",
       "    -0.00865200161933899,\n",
       "    -0.01078130304813385,\n",
       "    0.006660726852715015,\n",
       "    0.009028549306094646,\n",
       "    0.020171044394373894,\n",
       "    -0.024732746183872223,\n",
       "    0.05327734723687172,\n",
       "    2.863887686999078e-07,\n",
       "    0.009058717638254166,\n",
       "    0.002932659350335598,\n",
       "    0.002703205682337284,\n",
       "    0.04720969498157501,\n",
       "    0.03490786626935005,\n",
       "    0.025330159813165665,\n",
       "    0.007729498203843832,\n",
       "    -0.04271876439452171,\n",
       "    -0.0010114917531609535,\n",
       "    -0.03810345008969307,\n",
       "    -0.014254778623580933,\n",
       "    0.04250046983361244,\n",
       "    0.007916503585875034,\n",
       "    -0.010706881061196327,\n",
       "    -0.018326740711927414,\n",
       "    -0.03698335587978363,\n",
       "    0.011811726726591587,\n",
       "    -0.012491556815803051,\n",
       "    -0.000642073224298656,\n",
       "    0.04250190407037735,\n",
       "    -0.0004230922204442322,\n",
       "    0.03128829598426819,\n",
       "    -0.011640484444797039,\n",
       "    0.01866581104695797,\n",
       "    0.005773140117526054,\n",
       "    -0.023515457287430763,\n",
       "    -0.03533490002155304,\n",
       "    -0.021772006526589394,\n",
       "    0.02998933754861355,\n",
       "    0.059102337807416916,\n",
       "    -0.008576348423957825,\n",
       "    -0.02887425385415554,\n",
       "    0.03896281123161316,\n",
       "    0.06281699240207672,\n",
       "    -0.011768609285354614,\n",
       "    -0.05148930847644806,\n",
       "    0.02542315609753132,\n",
       "    0.03605091571807861,\n",
       "    -0.019139109179377556,\n",
       "    0.01927167922258377,\n",
       "    0.03814467787742615,\n",
       "    0.005832372698932886,\n",
       "    -0.007859153673052788,\n",
       "    -0.019358539953827858,\n",
       "    0.06413964182138443,\n",
       "    0.058654945343732834,\n",
       "    -0.07814619690179825,\n",
       "    -0.05667126178741455,\n",
       "    0.014437471516430378,\n",
       "    -0.028039969503879547,\n",
       "    -0.021134357899427414,\n",
       "    -0.005600911099463701,\n",
       "    0.039414577186107635,\n",
       "    0.024826770648360252,\n",
       "    0.01448382344096899,\n",
       "    -0.028274424374103546,\n",
       "    -0.0190742127597332,\n",
       "    0.02676798403263092,\n",
       "    -0.006750041618943214,\n",
       "    -0.055464353412389755,\n",
       "    0.013310123234987259,\n",
       "    -0.0626835823059082,\n",
       "    0.0063589406199753284,\n",
       "    -0.04755038768053055,\n",
       "    0.07038799673318863,\n",
       "    -0.0036971645895391703,\n",
       "    0.019928012043237686,\n",
       "    2.7314562745649208e-34,\n",
       "    0.01958429254591465,\n",
       "    0.01972018927335739,\n",
       "    -0.034387700259685516,\n",
       "    0.05193456634879112,\n",
       "    0.054042160511016846,\n",
       "    -0.06432298570871353,\n",
       "    -0.02117815427482128,\n",
       "    -0.009648696519434452,\n",
       "    -0.03656609356403351,\n",
       "    0.031363341957330704,\n",
       "    -0.007992294616997242]}}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = es_client.search(\n",
    "    index=index_name,\n",
    "    body={\n",
    "        \"size\": 5,\n",
    "        \"knn\": {\n",
    "            \"field\": \"text_vector\",\n",
    "            \"query_vector\": vector_search_term,\n",
    "            \"k\": 5,\n",
    "            \"num_candidates\": 10000\n",
    "        },\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "response['hits']['hits']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score == 2.72. When semanticsearch with elasticsearch can be more than 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interview_preparation_bot-ZQDkHgpI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
