{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run \n",
    "\n",
    "sudo docker run -it \\\n",
    "    --rm \\\n",
    "    --name elasticsearch \\\n",
    "    -m 4GB \\\n",
    "    -p 9200:9200 \\\n",
    "    -p 9300:9300 \\\n",
    "    -e \"discovery.type=single-node\" \\\n",
    "    -e \"xpack.security.enabled=false\" \\\n",
    "    docker.elastic.co/elasticsearch/elasticsearch:8.4.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "len(model.encode(\"This is a simple sentence\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/parsed_book.json', 'rt') as f_in:\n",
    "    book_raw = json.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "for doc in book_raw: \n",
    "    for section in doc['content']:\n",
    "        if 'text' in section:\n",
    "            section[\"text_vector\"] = model.encode(section[\"text\"]).tolist()\n",
    "        section['chapter'] = doc['chapter']\n",
    "        section['title'] = doc['title']\n",
    "        documents.append(section)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'In the first part of this chapter, I’ll walk through the structure of this book. Then, I’ll discuss the various job titles and roles that use ML skills in industry. 1 I’ll also clarify the responsibilities of various job titles, such as data scientist, machine learning engineer, and so on, as this is a common point of confusion for job seekers. These will be illustrated with an ML skills matrix and ML lifecycle that will be referenced throughout the book. The second part of this chapter walks through the interview process, from beginning to end. I’ve mentored candidates who appreciated this overview since online resources often focus on specific pieces of the interview but not how they all connect together and result in an offer. Especially for new graduates 2 and readers coming from different industries, this chapter helps get everyone on the same page as well as clarifies the process. The interconnecting pieces of interviews are complex, with many types of combina‐ tions depending on the ML role you’re aiming for. This overview will help set the stage, so you’ll know what to focus your time on. For example, some online resources focus on knowledge specific to “product data scientists,” but will title the course or article “data scientist interview tips” without differentiating. For a newcomer, it’s hard to tell if that is relevant to your own career interests. After this chapter, you’ll be able to tell what skills are required for each job title, and in Chapter 2 , you’ll be able to parse out that information yourself from job postings and make your resume as relevant to the job title and job posting as possible.',\n",
       "  'text_vector': [0.050185490399599075,\n",
       "   -0.04620453342795372,\n",
       "   -0.06787369400262833,\n",
       "   -0.030100882053375244,\n",
       "   0.020755629986524582,\n",
       "   0.02044174261391163,\n",
       "   0.014008884318172932,\n",
       "   -0.039854951202869415,\n",
       "   0.025338638573884964,\n",
       "   -0.02799830585718155,\n",
       "   0.05076625570654869,\n",
       "   0.04565666243433952,\n",
       "   0.010629191063344479,\n",
       "   0.07527502626180649,\n",
       "   0.06988721340894699,\n",
       "   -0.04521840810775757,\n",
       "   0.017519349232316017,\n",
       "   0.015717754140496254,\n",
       "   -0.0036672374699264765,\n",
       "   0.005232647527009249,\n",
       "   -0.019051460549235344,\n",
       "   0.004596404265612364,\n",
       "   -0.04056693613529205,\n",
       "   0.03566720709204674,\n",
       "   -0.016653062775731087,\n",
       "   -0.05011069402098656,\n",
       "   0.01264555286616087,\n",
       "   0.04413944482803345,\n",
       "   -0.004009637050330639,\n",
       "   0.019715363159775734,\n",
       "   0.019577566534280777,\n",
       "   -0.05812559276819229,\n",
       "   0.03103826567530632,\n",
       "   -0.010270373895764351,\n",
       "   2.1992773326928727e-06,\n",
       "   -0.0457351990044117,\n",
       "   -0.008197905495762825,\n",
       "   -0.009753294289112091,\n",
       "   -0.04821748659014702,\n",
       "   -0.0641702190041542,\n",
       "   0.022989554330706596,\n",
       "   0.01073820237070322,\n",
       "   0.029533838853240013,\n",
       "   0.029734162613749504,\n",
       "   -0.011210046708583832,\n",
       "   0.0061891814693808556,\n",
       "   0.06837346404790878,\n",
       "   0.0004888686817139387,\n",
       "   -0.0017041530227288604,\n",
       "   0.051050107926130295,\n",
       "   -0.007515179459005594,\n",
       "   -0.011717083863914013,\n",
       "   0.021045202389359474,\n",
       "   0.022053547203540802,\n",
       "   0.012346502393484116,\n",
       "   -0.010838501155376434,\n",
       "   0.004725790116935968,\n",
       "   0.03603799268603325,\n",
       "   0.021914316341280937,\n",
       "   -0.06220080330967903,\n",
       "   0.010495035909116268,\n",
       "   0.006095561198890209,\n",
       "   -0.016372641548514366,\n",
       "   -0.019862813875079155,\n",
       "   0.0634395107626915,\n",
       "   0.013629579916596413,\n",
       "   0.017875248566269875,\n",
       "   -0.026802130043506622,\n",
       "   -0.01284407265484333,\n",
       "   -0.0017475677886977792,\n",
       "   0.0363171361386776,\n",
       "   -0.013021662831306458,\n",
       "   -0.022996678948402405,\n",
       "   -0.01663937419652939,\n",
       "   0.07022479176521301,\n",
       "   0.008871633559465408,\n",
       "   -0.021723859012126923,\n",
       "   0.007483295630663633,\n",
       "   -0.017614873126149178,\n",
       "   -0.04087422043085098,\n",
       "   -0.03936714306473732,\n",
       "   0.026202477514743805,\n",
       "   -0.02720753662288189,\n",
       "   -0.002020531566813588,\n",
       "   0.018956657499074936,\n",
       "   0.08102618157863617,\n",
       "   -0.02209247089922428,\n",
       "   -0.016687259078025818,\n",
       "   0.03312544524669647,\n",
       "   -0.025736147537827492,\n",
       "   0.08613210916519165,\n",
       "   0.01121519971638918,\n",
       "   0.021488545462489128,\n",
       "   0.03887948766350746,\n",
       "   -0.03154875710606575,\n",
       "   0.011827908456325531,\n",
       "   0.015314917080104351,\n",
       "   0.019236357882618904,\n",
       "   -0.02965906634926796,\n",
       "   -0.07224174588918686,\n",
       "   -0.04103943333029747,\n",
       "   -0.04259666055440903,\n",
       "   0.029921000823378563,\n",
       "   -0.0017927732551470399,\n",
       "   0.02729489654302597,\n",
       "   0.0020180719438940287,\n",
       "   0.030271872878074646,\n",
       "   -0.029737288132309914,\n",
       "   -0.046853478997945786,\n",
       "   -0.03721874952316284,\n",
       "   -0.021420812234282494,\n",
       "   -0.038097087293863297,\n",
       "   -0.025090457871556282,\n",
       "   0.02805968001484871,\n",
       "   0.016639521345496178,\n",
       "   0.03959362953901291,\n",
       "   0.009358234703540802,\n",
       "   0.018901942297816277,\n",
       "   -0.026322120800614357,\n",
       "   -0.018310099840164185,\n",
       "   -0.04756826162338257,\n",
       "   0.02486632578074932,\n",
       "   0.02953820303082466,\n",
       "   0.05298396944999695,\n",
       "   -0.06838150322437286,\n",
       "   -0.039738934487104416,\n",
       "   -0.0018677660264074802,\n",
       "   0.023598214611411095,\n",
       "   -0.010045938193798065,\n",
       "   -0.05450292304158211,\n",
       "   0.04061827063560486,\n",
       "   0.009426248259842396,\n",
       "   -0.006437683943659067,\n",
       "   -0.019375305622816086,\n",
       "   0.0512227863073349,\n",
       "   0.06112057343125343,\n",
       "   -0.005696731619536877,\n",
       "   0.04378033056855202,\n",
       "   -0.04289541393518448,\n",
       "   -0.01821266859769821,\n",
       "   0.047547079622745514,\n",
       "   -0.00857364572584629,\n",
       "   0.024418935179710388,\n",
       "   0.03353611007332802,\n",
       "   0.01862415485084057,\n",
       "   0.0036735781468451023,\n",
       "   -0.014078108593821526,\n",
       "   -0.010786980390548706,\n",
       "   0.001070470898412168,\n",
       "   0.038677576929330826,\n",
       "   -0.012351796962320805,\n",
       "   0.01325149554759264,\n",
       "   0.0023383183870464563,\n",
       "   -0.0060983882285654545,\n",
       "   0.07699709385633469,\n",
       "   0.0051714289002120495,\n",
       "   0.023815320804715157,\n",
       "   0.01837237738072872,\n",
       "   0.007031407207250595,\n",
       "   0.05091754347085953,\n",
       "   0.02365117147564888,\n",
       "   -0.07295770198106766,\n",
       "   0.09799236059188843,\n",
       "   0.03575871139764786,\n",
       "   -0.024223167449235916,\n",
       "   -0.05882030725479126,\n",
       "   -0.008404736407101154,\n",
       "   0.07170933485031128,\n",
       "   -0.03869469836354256,\n",
       "   0.011209535412490368,\n",
       "   0.04698118194937706,\n",
       "   -0.025448139756917953,\n",
       "   -0.026949776336550713,\n",
       "   0.04907826706767082,\n",
       "   0.04274297133088112,\n",
       "   0.03135695308446884,\n",
       "   0.03055381029844284,\n",
       "   0.016832124441862106,\n",
       "   0.013626757077872753,\n",
       "   0.08666329830884933,\n",
       "   -0.00952710397541523,\n",
       "   -0.044747065752744675,\n",
       "   -0.0032049657311290503,\n",
       "   0.06075779348611832,\n",
       "   -4.578003427013755e-05,\n",
       "   -0.03453105688095093,\n",
       "   -0.08530092239379883,\n",
       "   0.033683568239212036,\n",
       "   -0.012631617486476898,\n",
       "   -0.005910114850848913,\n",
       "   0.016146596521139145,\n",
       "   0.02683013491332531,\n",
       "   -0.052617885172367096,\n",
       "   0.07786780595779419,\n",
       "   -0.02599398046731949,\n",
       "   -0.016789887100458145,\n",
       "   -0.053237900137901306,\n",
       "   0.0007575377821922302,\n",
       "   -0.038006942719221115,\n",
       "   -0.014738923870027065,\n",
       "   -0.051866140216588974,\n",
       "   -0.015294885262846947,\n",
       "   -0.05274957790970802,\n",
       "   0.029427314177155495,\n",
       "   0.05997477471828461,\n",
       "   -0.0006944634369574487,\n",
       "   -0.053448595106601715,\n",
       "   -0.07790665328502655,\n",
       "   -0.029939930886030197,\n",
       "   0.008771976456046104,\n",
       "   0.03534383699297905,\n",
       "   -0.046514593064785004,\n",
       "   -0.020995929837226868,\n",
       "   -0.006688192021101713,\n",
       "   -0.0008412201423197985,\n",
       "   -0.03975221514701843,\n",
       "   -0.029033776372671127,\n",
       "   -0.04267509654164314,\n",
       "   -0.026658743619918823,\n",
       "   0.0038811496924608946,\n",
       "   -0.01623091660439968,\n",
       "   -0.030128903687000275,\n",
       "   -0.05654343217611313,\n",
       "   0.010821687057614326,\n",
       "   -0.014205528423190117,\n",
       "   -0.051824189722537994,\n",
       "   0.017422711476683617,\n",
       "   0.003282052930444479,\n",
       "   0.031329285353422165,\n",
       "   0.0514734722673893,\n",
       "   0.00866732932627201,\n",
       "   0.017036907374858856,\n",
       "   0.018485581502318382,\n",
       "   0.04983457550406456,\n",
       "   0.0037939478643238544,\n",
       "   0.03474143147468567,\n",
       "   0.01018605288118124,\n",
       "   0.07447073608636856,\n",
       "   0.012075314298272133,\n",
       "   -0.048957835882902145,\n",
       "   -0.059853050857782364,\n",
       "   0.04766061156988144,\n",
       "   -0.020596452057361603,\n",
       "   0.015066350810229778,\n",
       "   -0.014540825970470905,\n",
       "   -0.002533533377572894,\n",
       "   -0.025466270744800568,\n",
       "   0.034700676798820496,\n",
       "   -0.03524678200483322,\n",
       "   -0.029718538746237755,\n",
       "   0.023548969998955727,\n",
       "   -0.003780586877837777,\n",
       "   0.000965850951615721,\n",
       "   0.0623854324221611,\n",
       "   0.021049996837973595,\n",
       "   -0.02685682103037834,\n",
       "   -0.008953813463449478,\n",
       "   0.03744499757885933,\n",
       "   -0.08309156447649002,\n",
       "   0.028313705697655678,\n",
       "   -0.0006167364772409201,\n",
       "   -0.04335420951247215,\n",
       "   0.057281915098428726,\n",
       "   -0.003913732245564461,\n",
       "   -0.04961550235748291,\n",
       "   -0.046834010630846024,\n",
       "   0.03724036365747452,\n",
       "   -0.016520557925105095,\n",
       "   -0.02128966897726059,\n",
       "   0.04204009845852852,\n",
       "   0.007056616712361574,\n",
       "   0.005421643611043692,\n",
       "   -0.05540645867586136,\n",
       "   -0.02915477380156517,\n",
       "   -0.026748543605208397,\n",
       "   -0.00938344094902277,\n",
       "   -0.03434984013438225,\n",
       "   0.0002636352146510035,\n",
       "   -0.060512788593769073,\n",
       "   0.03303590044379234,\n",
       "   0.002465968020260334,\n",
       "   -0.01443470735102892,\n",
       "   0.02359611541032791,\n",
       "   -0.00445232167840004,\n",
       "   -0.028932273387908936,\n",
       "   0.024526376277208328,\n",
       "   0.029922233894467354,\n",
       "   -0.04201997444033623,\n",
       "   0.025361694395542145,\n",
       "   -0.027260204777121544,\n",
       "   -0.022128012031316757,\n",
       "   2.1991418179823086e-05,\n",
       "   -0.01420553307980299,\n",
       "   0.0178036130964756,\n",
       "   -0.0311509408056736,\n",
       "   0.05074753239750862,\n",
       "   -0.03174386918544769,\n",
       "   0.07096519321203232,\n",
       "   0.01096083503216505,\n",
       "   0.05550406500697136,\n",
       "   0.008785100653767586,\n",
       "   0.0047767264768481255,\n",
       "   0.04502542316913605,\n",
       "   0.019086580723524094,\n",
       "   0.054891880601644516,\n",
       "   0.05831317603588104,\n",
       "   -0.025829171761870384,\n",
       "   0.01638861373066902,\n",
       "   0.020231029018759727,\n",
       "   -0.05296126753091812,\n",
       "   -0.00932143721729517,\n",
       "   -0.006369320675730705,\n",
       "   0.07140407711267471,\n",
       "   0.05397684872150421,\n",
       "   0.036242399364709854,\n",
       "   -0.040195222944021225,\n",
       "   -0.04556379094719887,\n",
       "   0.023225849494338036,\n",
       "   -0.011518176645040512,\n",
       "   0.06520813703536987,\n",
       "   -0.039365965873003006,\n",
       "   -0.03871498256921768,\n",
       "   0.015020609833300114,\n",
       "   -0.014408652670681477,\n",
       "   0.0074487244710326195,\n",
       "   -0.09562240540981293,\n",
       "   -0.0983710065484047,\n",
       "   0.01584668830037117,\n",
       "   0.0438142791390419,\n",
       "   -0.031348422169685364,\n",
       "   0.01821991801261902,\n",
       "   -0.049965061247348785,\n",
       "   -0.008410325273871422,\n",
       "   -0.028308700770139694,\n",
       "   -0.000585355912335217,\n",
       "   -0.003095899010077119,\n",
       "   -0.03314194455742836,\n",
       "   -0.06368747353553772,\n",
       "   -0.029590846970677376,\n",
       "   -0.013372347690165043,\n",
       "   -0.027458740398287773,\n",
       "   0.017218399792909622,\n",
       "   -0.03299546241760254,\n",
       "   -0.006130559369921684,\n",
       "   -0.0006167514948174357,\n",
       "   0.00034847413189709187,\n",
       "   0.012723272666335106,\n",
       "   0.006270792335271835,\n",
       "   -0.07298507541418076,\n",
       "   -0.01563631184399128,\n",
       "   0.05813392624258995,\n",
       "   -0.008296783082187176,\n",
       "   0.01641571894288063,\n",
       "   -0.009352185763418674,\n",
       "   0.014437181875109673,\n",
       "   -0.015065763145685196,\n",
       "   0.04606565088033676,\n",
       "   0.05844515934586525,\n",
       "   0.03224442899227142,\n",
       "   0.002726237755268812,\n",
       "   0.003703637747094035,\n",
       "   -0.004164700396358967,\n",
       "   0.002887500450015068,\n",
       "   -0.01778576709330082,\n",
       "   -0.03590046986937523,\n",
       "   -0.017738208174705505,\n",
       "   0.026133861392736435,\n",
       "   -0.06300978362560272,\n",
       "   0.03936256840825081,\n",
       "   0.035660773515701294,\n",
       "   -0.014051437377929688,\n",
       "   -0.01335067767649889,\n",
       "   0.029381120577454567,\n",
       "   0.02133382111787796,\n",
       "   -0.017907466739416122,\n",
       "   -0.036057937890291214,\n",
       "   -0.007505715824663639,\n",
       "   -0.002089116722345352,\n",
       "   -0.07516524940729141,\n",
       "   0.04357459768652916,\n",
       "   0.10554882884025574,\n",
       "   -0.11832581460475922,\n",
       "   0.04379653185606003,\n",
       "   -0.02376653626561165,\n",
       "   0.048764415085315704,\n",
       "   -0.060704305768013,\n",
       "   -0.009863212704658508,\n",
       "   -0.032751817256212234,\n",
       "   -0.054697439074516296,\n",
       "   0.06839565932750702,\n",
       "   -0.03395967185497284,\n",
       "   -0.03719198331236839,\n",
       "   -0.078712098300457,\n",
       "   -0.041490860283374786,\n",
       "   -0.039020370692014694,\n",
       "   -0.024285968393087387,\n",
       "   0.0432678684592247,\n",
       "   0.005432845093309879,\n",
       "   -0.007524748332798481,\n",
       "   0.022905133664608,\n",
       "   0.056036196649074554,\n",
       "   -0.006406095810234547,\n",
       "   -0.04152682051062584,\n",
       "   0.03131433576345444,\n",
       "   -0.02241460420191288,\n",
       "   -0.011294841766357422,\n",
       "   0.03702953830361366,\n",
       "   0.028370359912514687,\n",
       "   -0.06032010540366173,\n",
       "   0.07261455804109573,\n",
       "   -0.009535009041428566,\n",
       "   0.03521224111318588,\n",
       "   0.019598282873630524,\n",
       "   0.03661046549677849,\n",
       "   0.014130961149930954,\n",
       "   -0.04011620208621025,\n",
       "   0.002117633353918791,\n",
       "   -0.04757359251379967,\n",
       "   0.02976769395172596,\n",
       "   -0.023295462131500244,\n",
       "   0.019573023542761803,\n",
       "   0.07566379755735397,\n",
       "   0.03171521797776222,\n",
       "   -0.0459027998149395,\n",
       "   -0.01625934988260269,\n",
       "   -0.057716336101293564,\n",
       "   0.006352995056658983,\n",
       "   0.00134411477483809,\n",
       "   0.04955056309700012,\n",
       "   -0.01810726337134838,\n",
       "   -0.04242619872093201,\n",
       "   0.004223616328090429,\n",
       "   -0.03578544780611992,\n",
       "   0.036081720143556595,\n",
       "   0.019250577315688133,\n",
       "   -0.021051842719316483,\n",
       "   0.02300228737294674,\n",
       "   -0.014684262685477734,\n",
       "   0.005825463682413101,\n",
       "   0.051278553903102875,\n",
       "   -0.044783152639865875,\n",
       "   0.04503491148352623,\n",
       "   0.054857026785612106,\n",
       "   0.024967698380351067,\n",
       "   0.001314179738983512,\n",
       "   -0.00825503095984459,\n",
       "   0.003245758591219783,\n",
       "   -0.08366278558969498,\n",
       "   0.02347387932240963,\n",
       "   0.0038939048536121845,\n",
       "   0.07656394690275192,\n",
       "   0.011161272414028645,\n",
       "   -0.017011191695928574,\n",
       "   0.03643398731946945,\n",
       "   -0.08215140551328659,\n",
       "   -0.04348902404308319,\n",
       "   -0.02190244011580944,\n",
       "   -0.01953807659447193,\n",
       "   0.010331032797694206,\n",
       "   -0.01922803558409214,\n",
       "   0.048944320529699326,\n",
       "   0.02111886441707611,\n",
       "   0.020352261140942574,\n",
       "   0.0006227008416317403,\n",
       "   -0.08540508896112442,\n",
       "   -0.02847202494740486,\n",
       "   0.03897502273321152,\n",
       "   -0.06292867660522461,\n",
       "   0.03160443902015686,\n",
       "   -0.03483085706830025,\n",
       "   0.012276889756321907,\n",
       "   -0.03248640149831772,\n",
       "   -0.030415385961532593,\n",
       "   -0.014814862981438637,\n",
       "   0.005287405103445053,\n",
       "   -0.0013735635438933969,\n",
       "   -0.08549800515174866,\n",
       "   0.019205288961529732,\n",
       "   -0.060862947255373,\n",
       "   -0.01215317752212286,\n",
       "   0.0255812369287014,\n",
       "   0.029981166124343872,\n",
       "   0.009864785708487034,\n",
       "   -0.04831646382808685,\n",
       "   -0.009133727289736271,\n",
       "   0.027048425748944283,\n",
       "   0.0665721446275711,\n",
       "   -0.021528344601392746,\n",
       "   -0.02769247628748417,\n",
       "   -0.04969537630677223,\n",
       "   0.033010777086019516,\n",
       "   0.03839254379272461,\n",
       "   0.020872430875897408,\n",
       "   -0.029440563172101974,\n",
       "   -0.02829584665596485,\n",
       "   -0.00313732516951859,\n",
       "   -0.06508032232522964,\n",
       "   0.04299899935722351,\n",
       "   0.012616806663572788,\n",
       "   0.02732451818883419,\n",
       "   -0.02144739218056202,\n",
       "   -0.050857603549957275,\n",
       "   -0.010278207249939442,\n",
       "   -0.004350490402430296,\n",
       "   -0.006718341261148453,\n",
       "   0.005969957914203405,\n",
       "   0.013645454309880733,\n",
       "   0.02118665911257267,\n",
       "   -0.00759458402171731,\n",
       "   0.04220862686634064,\n",
       "   0.01836150325834751,\n",
       "   -0.006208023522049189,\n",
       "   -0.03665627911686897,\n",
       "   -0.0800928920507431,\n",
       "   -0.004384192172437906,\n",
       "   -0.005515779368579388,\n",
       "   0.016225261613726616,\n",
       "   -0.050242096185684204,\n",
       "   0.002278448548167944,\n",
       "   0.0021754817571491003,\n",
       "   0.03648259863257408,\n",
       "   -0.009830640628933907,\n",
       "   -0.02217722497880459,\n",
       "   0.0022053616121411324,\n",
       "   0.03187982365489006,\n",
       "   0.04940587282180786,\n",
       "   0.014091760851442814,\n",
       "   -0.03886835649609566,\n",
       "   -0.008015720173716545,\n",
       "   0.021200235933065414,\n",
       "   -0.011911535635590553,\n",
       "   0.025453070178627968,\n",
       "   -0.004874437116086483,\n",
       "   -0.03430845960974693,\n",
       "   0.005197679158300161,\n",
       "   -0.015076147392392159,\n",
       "   0.06551100313663483,\n",
       "   0.05524253845214844,\n",
       "   0.03536037728190422,\n",
       "   0.00036901497514918447,\n",
       "   -0.0387585274875164,\n",
       "   -0.048967450857162476,\n",
       "   -0.02425633929669857,\n",
       "   -0.015464330092072487,\n",
       "   0.026657279580831528,\n",
       "   -0.04794333502650261,\n",
       "   -0.015479414723813534,\n",
       "   0.06685661524534225,\n",
       "   0.04715492203831673,\n",
       "   0.0010446535889059305,\n",
       "   -0.034319669008255005,\n",
       "   0.002223232528194785,\n",
       "   -0.00354644563049078,\n",
       "   0.002739398507401347,\n",
       "   0.009855532087385654,\n",
       "   -5.332645183526677e-33,\n",
       "   0.02454383485019207,\n",
       "   -0.07734416425228119,\n",
       "   0.06236301735043526,\n",
       "   0.05857798829674721,\n",
       "   0.015301433391869068,\n",
       "   -0.02381237782537937,\n",
       "   0.048916045576334,\n",
       "   0.02607167325913906,\n",
       "   -0.03621763736009598,\n",
       "   -0.012620403431355953,\n",
       "   -0.02555954083800316,\n",
       "   0.04108063131570816,\n",
       "   0.015099226497113705,\n",
       "   0.027847591787576675,\n",
       "   -0.0030295224860310555,\n",
       "   0.03331228718161583,\n",
       "   0.015507311560213566,\n",
       "   0.027986688539385796,\n",
       "   0.024744370952248573,\n",
       "   -0.0045754751190543175,\n",
       "   -0.09582123905420303,\n",
       "   0.01839594356715679,\n",
       "   -0.018430525436997414,\n",
       "   -0.05625072494149208,\n",
       "   0.020555701106786728,\n",
       "   -0.0356677770614624,\n",
       "   -0.01403586845844984,\n",
       "   -0.0235054399818182,\n",
       "   -0.0034085677471011877,\n",
       "   0.004301399923861027,\n",
       "   0.04368981346487999,\n",
       "   -0.005371712613850832,\n",
       "   0.02601027302443981,\n",
       "   -0.040707219392061234,\n",
       "   -0.020444387570023537,\n",
       "   0.00435307901352644,\n",
       "   -0.05760522186756134,\n",
       "   0.019634030759334564,\n",
       "   0.03887531906366348,\n",
       "   0.048167310655117035,\n",
       "   0.03010309487581253,\n",
       "   -0.0363885834813118,\n",
       "   0.035297941416502,\n",
       "   -0.03456679359078407,\n",
       "   -0.010713942348957062,\n",
       "   -0.0003557167074177414,\n",
       "   0.0769549086689949,\n",
       "   -0.013851354829967022,\n",
       "   0.027205228805541992,\n",
       "   -0.03332515060901642,\n",
       "   0.010575206018984318,\n",
       "   0.006449698470532894,\n",
       "   -0.10131038725376129,\n",
       "   0.03592503070831299,\n",
       "   0.032341018319129944,\n",
       "   0.06913989782333374,\n",
       "   0.008729372173547745,\n",
       "   -0.011301686055958271,\n",
       "   -0.052584338933229446,\n",
       "   0.05292317643761635,\n",
       "   -0.030238425359129906,\n",
       "   -0.013452300801873207,\n",
       "   0.07036443799734116,\n",
       "   0.027123844251036644,\n",
       "   0.025876225903630257,\n",
       "   0.0006298556691035628,\n",
       "   0.07329579442739487,\n",
       "   0.010890192352235317,\n",
       "   0.03931531310081482,\n",
       "   0.0047310953959822655,\n",
       "   -0.028521517291665077,\n",
       "   0.05561337620019913,\n",
       "   -0.043893709778785706,\n",
       "   0.018505552783608437,\n",
       "   -0.0020857094787061214,\n",
       "   -0.0364881232380867,\n",
       "   -0.00033578340662643313,\n",
       "   0.020405584946274757,\n",
       "   0.06670891493558884,\n",
       "   -0.06173640489578247,\n",
       "   -0.053193237632513046,\n",
       "   0.031240766867995262,\n",
       "   0.00986067857593298,\n",
       "   -0.03476466238498688,\n",
       "   -0.007464431691914797,\n",
       "   -0.049866240471601486,\n",
       "   0.011520197615027428,\n",
       "   0.004743126221001148,\n",
       "   0.003923836164176464,\n",
       "   -0.06426079571247101,\n",
       "   -0.001903311349451542,\n",
       "   0.09016688168048859,\n",
       "   -0.019025711342692375,\n",
       "   -0.006439023185521364,\n",
       "   -0.010695748031139374,\n",
       "   -0.03925516456365585,\n",
       "   -0.011995771899819374,\n",
       "   0.01223307941108942,\n",
       "   0.016180893406271935,\n",
       "   0.01658271998167038,\n",
       "   -0.033956971019506454,\n",
       "   -0.004326659720391035,\n",
       "   -0.026927897706627846,\n",
       "   0.013290627859532833,\n",
       "   0.05407242849469185,\n",
       "   0.04711369052529335,\n",
       "   -0.0396081879734993,\n",
       "   0.0015189789701253176,\n",
       "   -0.0029927415307611227,\n",
       "   -0.018904443830251694,\n",
       "   0.0037900893948972225,\n",
       "   0.01672251522541046,\n",
       "   -0.02888385020196438,\n",
       "   0.04633554816246033,\n",
       "   -0.011473099701106548,\n",
       "   0.013659166172146797,\n",
       "   0.025504834949970245,\n",
       "   0.008822576142847538,\n",
       "   -0.034735579043626785,\n",
       "   -0.011498022824525833,\n",
       "   0.024195298552513123,\n",
       "   -0.04018480330705643,\n",
       "   -0.0344596728682518,\n",
       "   -0.026447391137480736,\n",
       "   -0.01708810403943062,\n",
       "   -0.01070722658187151,\n",
       "   -0.007532862480729818,\n",
       "   0.00011732796701835468,\n",
       "   0.027551203966140747,\n",
       "   -0.03643591329455376,\n",
       "   0.002747755730524659,\n",
       "   0.00286462833173573,\n",
       "   2.6954742793350306e-07,\n",
       "   0.004516103304922581,\n",
       "   0.021874327212572098,\n",
       "   -0.007275216281414032,\n",
       "   0.026141485199332237,\n",
       "   0.027681075036525726,\n",
       "   -0.008655260317027569,\n",
       "   -0.00614273501560092,\n",
       "   0.0031383049208670855,\n",
       "   0.004419796168804169,\n",
       "   0.02722235955297947,\n",
       "   -0.013842301443219185,\n",
       "   -0.0070479935966432095,\n",
       "   -0.008300314657390118,\n",
       "   -0.03892127797007561,\n",
       "   -0.04127322509884834,\n",
       "   -0.09202369302511215,\n",
       "   -0.003814672352746129,\n",
       "   -0.030254703015089035,\n",
       "   -0.062161803245544434,\n",
       "   0.058456458151340485,\n",
       "   0.00593182910233736,\n",
       "   0.05114268884062767,\n",
       "   0.007547308690845966,\n",
       "   -0.011293266899883747,\n",
       "   -0.01983441784977913,\n",
       "   -0.024240514263510704,\n",
       "   -0.052997246384620667,\n",
       "   -0.02276643179357052,\n",
       "   0.04799913242459297,\n",
       "   0.05786909908056259,\n",
       "   0.06286801397800446,\n",
       "   0.01665354147553444,\n",
       "   0.018061568960547447,\n",
       "   0.04228046163916588,\n",
       "   -0.009616289287805557,\n",
       "   -0.02246616967022419,\n",
       "   0.01754770614206791,\n",
       "   0.07402527332305908,\n",
       "   -0.008817410096526146,\n",
       "   0.026519525796175003,\n",
       "   0.04610144719481468,\n",
       "   0.017917409539222717,\n",
       "   -0.008229349739849567,\n",
       "   -0.04833143204450607,\n",
       "   0.057967402040958405,\n",
       "   0.014223978854715824,\n",
       "   -0.06364507973194122,\n",
       "   -0.0638623759150505,\n",
       "   -0.031662169843912125,\n",
       "   -0.016245272010564804,\n",
       "   -0.010045766830444336,\n",
       "   -0.00863697100430727,\n",
       "   0.023339368402957916,\n",
       "   0.02953464351594448,\n",
       "   0.008399201557040215,\n",
       "   -0.07750958204269409,\n",
       "   -0.03926356881856918,\n",
       "   0.02490755170583725,\n",
       "   0.017904886975884438,\n",
       "   0.016221025958657265,\n",
       "   -0.00795397162437439,\n",
       "   -0.05490266904234886,\n",
       "   0.02468402497470379,\n",
       "   -0.04390516132116318,\n",
       "   0.09967026859521866,\n",
       "   0.002109908265992999,\n",
       "   0.008560382761061192,\n",
       "   2.1149558644841907e-34,\n",
       "   -0.007553906179964542,\n",
       "   0.02875625528395176,\n",
       "   -0.02107066847383976,\n",
       "   0.010674535296857357,\n",
       "   0.04657669737935066,\n",
       "   -0.04764719679951668,\n",
       "   0.0015843880828469992,\n",
       "   0.000557278108317405,\n",
       "   -0.007551687769591808,\n",
       "   0.047041889280080795,\n",
       "   -0.021928569301962852],\n",
       "  'chapter': 'CHAPTER 1',\n",
       "  'title': 'Machine Learning Roles and the Interview Process'},\n",
       " {'section': 'Overview of This Book',\n",
       "  'chapter': 'CHAPTER 1',\n",
       "  'title': 'Machine Learning Roles and the Interview Process'},\n",
       " {'text': 'This chapter focuses on helping you differentiate among various ML roles, and walks through the entire interview process, as illustrated in Figure 1-1 : • Job applications and resume ( Chapter 2 ) • Technical interviews — Machine learning (Chapters 3 , 4 , and 6 ) — Coding/programming ( Chapter 5 ) • Behavioral interviews ( Chapter 7 ) • Your interview roadmap ( Chapter 8 ) • Post-interview and follow-up ( Chapter 9 ) Figure 1-1. Overview of the chapters and how they tie into the ML interview process. Depending on where you are in your ML interview journey, I encourage you to focus on the chapters and sections that seem relevant to you. I’ve also planned the book to be referenced as you go along; for example, you might iterate on your resume multi‐ ple times and then flip back to Chapter 2 when needed. The same applies to the other chapters. With that overview, let’s continue. The companion site to this book, https://susanshu.substack.com , features bonus content, helper resources, and more.',\n",
       "  'text_vector': [0.06150469183921814,\n",
       "   -0.02986118756234646,\n",
       "   -0.0677529126405716,\n",
       "   -0.005004839971661568,\n",
       "   -0.004746549297124147,\n",
       "   0.011573880910873413,\n",
       "   -0.01126045547425747,\n",
       "   -0.069349505007267,\n",
       "   0.00781652145087719,\n",
       "   -0.037126898765563965,\n",
       "   0.07198570668697357,\n",
       "   0.05393322557210922,\n",
       "   0.0019189731683582067,\n",
       "   0.04936626926064491,\n",
       "   0.051216498017311096,\n",
       "   -0.060517262667417526,\n",
       "   0.021293003112077713,\n",
       "   0.0016773371025919914,\n",
       "   -0.023980222642421722,\n",
       "   -0.0021804957650601864,\n",
       "   -0.01773860491812229,\n",
       "   0.004696739837527275,\n",
       "   -0.019184526056051254,\n",
       "   0.04604174569249153,\n",
       "   -0.008013682439923286,\n",
       "   -0.050932805985212326,\n",
       "   0.008092028088867664,\n",
       "   0.02306441403925419,\n",
       "   -0.0014129971386864781,\n",
       "   0.02194467931985855,\n",
       "   -0.00695400033146143,\n",
       "   -0.06683598458766937,\n",
       "   0.0299978069961071,\n",
       "   0.013847176916897297,\n",
       "   2.1935968561592745e-06,\n",
       "   -0.05755004286766052,\n",
       "   -0.006559413857758045,\n",
       "   -0.003044109558686614,\n",
       "   -0.05019314959645271,\n",
       "   -0.06830710172653198,\n",
       "   0.008325736969709396,\n",
       "   2.2309224732453004e-05,\n",
       "   0.02849365584552288,\n",
       "   0.02363518252968788,\n",
       "   -0.0007074734312482178,\n",
       "   0.014933103695511818,\n",
       "   0.08136292546987534,\n",
       "   0.02241629734635353,\n",
       "   -0.008828084915876389,\n",
       "   0.044468559324741364,\n",
       "   -0.017764216288924217,\n",
       "   -0.026601774618029594,\n",
       "   -0.01177271082997322,\n",
       "   0.039188385009765625,\n",
       "   0.007736884523183107,\n",
       "   -0.029721686616539955,\n",
       "   0.0012870238861069083,\n",
       "   0.007589801214635372,\n",
       "   0.018166908994317055,\n",
       "   -0.07438559085130692,\n",
       "   0.0025201488751918077,\n",
       "   0.010967407375574112,\n",
       "   0.004372245166450739,\n",
       "   -0.00047966575948521495,\n",
       "   0.05022294074296951,\n",
       "   0.016183942556381226,\n",
       "   0.024815674871206284,\n",
       "   -0.017719512805342674,\n",
       "   -0.026529503986239433,\n",
       "   0.01066331472247839,\n",
       "   0.05688004940748215,\n",
       "   0.0020831353031098843,\n",
       "   -0.0135304294526577,\n",
       "   -0.004584255162626505,\n",
       "   0.06979668140411377,\n",
       "   0.015240532346069813,\n",
       "   -0.0052250102162361145,\n",
       "   0.006807356607168913,\n",
       "   -0.007604304701089859,\n",
       "   -0.032721035182476044,\n",
       "   -0.05325939133763313,\n",
       "   0.05030728876590729,\n",
       "   -0.00889844074845314,\n",
       "   -0.0035742945037782192,\n",
       "   -0.03343523293733597,\n",
       "   0.07717965543270111,\n",
       "   -0.02250296249985695,\n",
       "   -0.010265891440212727,\n",
       "   0.026953991502523422,\n",
       "   -0.01792740263044834,\n",
       "   0.0904177576303482,\n",
       "   0.0022580476943403482,\n",
       "   0.012032794766128063,\n",
       "   0.049887195229530334,\n",
       "   -0.03603057190775871,\n",
       "   0.0019947539549320936,\n",
       "   0.025166725739836693,\n",
       "   0.020955713465809822,\n",
       "   -0.030279159545898438,\n",
       "   -0.03914424404501915,\n",
       "   -0.022109629586338997,\n",
       "   -0.03478965908288956,\n",
       "   0.0019791286904364824,\n",
       "   -0.007285818457603455,\n",
       "   0.041732799261808395,\n",
       "   0.024274488911032677,\n",
       "   0.0018128706142306328,\n",
       "   -0.03834624961018562,\n",
       "   -0.0451698899269104,\n",
       "   -0.02658463828265667,\n",
       "   -0.0274348184466362,\n",
       "   -0.024413801729679108,\n",
       "   -0.001296195900067687,\n",
       "   0.027083542197942734,\n",
       "   0.024458421394228935,\n",
       "   0.03414464369416237,\n",
       "   0.004499212838709354,\n",
       "   0.018285181373357773,\n",
       "   -0.04192161187529564,\n",
       "   -0.013165737502276897,\n",
       "   -0.048398975282907486,\n",
       "   0.013819779269397259,\n",
       "   0.027298027649521828,\n",
       "   0.04924720525741577,\n",
       "   -0.04512534290552139,\n",
       "   -0.056272298097610474,\n",
       "   -0.016020795330405235,\n",
       "   0.02117701806128025,\n",
       "   -0.018423113971948624,\n",
       "   -0.04994562640786171,\n",
       "   0.04624117538332939,\n",
       "   0.008896573446691036,\n",
       "   -0.019102830439805984,\n",
       "   -0.037678610533475876,\n",
       "   0.035866472870111465,\n",
       "   0.05852362513542175,\n",
       "   0.01375842560082674,\n",
       "   0.03469443321228027,\n",
       "   -0.04750771075487137,\n",
       "   -0.013860005885362625,\n",
       "   0.038864001631736755,\n",
       "   -0.03511115536093712,\n",
       "   0.008872492238879204,\n",
       "   0.005878036841750145,\n",
       "   0.006529714446514845,\n",
       "   0.012954648584127426,\n",
       "   -0.002242017537355423,\n",
       "   -0.013188036158680916,\n",
       "   -0.029847223311662674,\n",
       "   0.016018519178032875,\n",
       "   0.003912783693522215,\n",
       "   0.03365636616945267,\n",
       "   0.03033767081797123,\n",
       "   0.013398936949670315,\n",
       "   0.07200361043214798,\n",
       "   0.009008262306451797,\n",
       "   0.006037066224962473,\n",
       "   0.0555000863969326,\n",
       "   0.008371228352189064,\n",
       "   0.03865163400769234,\n",
       "   0.031021596863865852,\n",
       "   -0.08583550155162811,\n",
       "   0.0672721192240715,\n",
       "   0.04514702036976814,\n",
       "   -0.03460623696446419,\n",
       "   -0.06122905761003494,\n",
       "   0.001043800264596939,\n",
       "   0.061646539717912674,\n",
       "   -0.0279292780905962,\n",
       "   0.012388186529278755,\n",
       "   0.026454275473952293,\n",
       "   0.00016049217083491385,\n",
       "   -0.017495281994342804,\n",
       "   0.020495552569627762,\n",
       "   0.048160914331674576,\n",
       "   0.056499287486076355,\n",
       "   0.0007631052285432816,\n",
       "   -0.01631062477827072,\n",
       "   0.0393202044069767,\n",
       "   0.057958632707595825,\n",
       "   0.008210998959839344,\n",
       "   -0.012358554638922215,\n",
       "   -0.009509452618658543,\n",
       "   0.0350954569876194,\n",
       "   -0.009513880126178265,\n",
       "   -0.05148490518331528,\n",
       "   -0.09334951639175415,\n",
       "   0.034223176538944244,\n",
       "   -0.01519556250423193,\n",
       "   -0.016892127692699432,\n",
       "   0.009591838344931602,\n",
       "   0.006949049420654774,\n",
       "   -0.02230725809931755,\n",
       "   0.06903382390737534,\n",
       "   -0.0050008250400424,\n",
       "   -0.03316335752606392,\n",
       "   -0.05881199613213539,\n",
       "   -0.0154364462941885,\n",
       "   -0.062081653624773026,\n",
       "   -0.022348089143633842,\n",
       "   -0.04951011762022972,\n",
       "   -0.04994771257042885,\n",
       "   -0.04731522500514984,\n",
       "   0.015478807501494884,\n",
       "   0.03939245641231537,\n",
       "   -0.02946518175303936,\n",
       "   -0.007117219734936953,\n",
       "   -0.06671696156263351,\n",
       "   -0.03492823243141174,\n",
       "   0.007751408498734236,\n",
       "   0.05460989475250244,\n",
       "   -0.023877685889601707,\n",
       "   -0.016978537663817406,\n",
       "   0.005198774393647909,\n",
       "   -0.009090516716241837,\n",
       "   -0.028518743813037872,\n",
       "   -0.013554390519857407,\n",
       "   -0.03609377145767212,\n",
       "   -0.025247516110539436,\n",
       "   0.02191169559955597,\n",
       "   -0.0160826425999403,\n",
       "   -0.027141207829117775,\n",
       "   -0.06993865221738815,\n",
       "   0.015293353237211704,\n",
       "   -0.004471439402550459,\n",
       "   -0.037131618708372116,\n",
       "   0.020403455942869186,\n",
       "   0.017718330025672913,\n",
       "   0.03959709405899048,\n",
       "   0.04663681611418724,\n",
       "   0.0008575885440222919,\n",
       "   0.03163359314203262,\n",
       "   0.026444533839821815,\n",
       "   0.02974180318415165,\n",
       "   0.012509583495557308,\n",
       "   0.06154783442616463,\n",
       "   0.007546473294496536,\n",
       "   0.056166939437389374,\n",
       "   0.02257833629846573,\n",
       "   -0.04421386122703552,\n",
       "   -0.05304266884922981,\n",
       "   0.05969729647040367,\n",
       "   -0.02048557437956333,\n",
       "   0.014482380822300911,\n",
       "   -0.01630697026848793,\n",
       "   -0.009781145490705967,\n",
       "   -0.016362469643354416,\n",
       "   0.030343040823936462,\n",
       "   -0.004977591335773468,\n",
       "   -0.03037836030125618,\n",
       "   0.01742655411362648,\n",
       "   -0.009670708328485489,\n",
       "   0.0010538767091929913,\n",
       "   0.04181500896811485,\n",
       "   -0.017810968682169914,\n",
       "   0.005939334165304899,\n",
       "   -0.010900643654167652,\n",
       "   0.03349785879254341,\n",
       "   -0.06015390902757645,\n",
       "   0.027886375784873962,\n",
       "   0.018335413187742233,\n",
       "   -0.04027681052684784,\n",
       "   0.03502190485596657,\n",
       "   0.005085855722427368,\n",
       "   -0.05359910801053047,\n",
       "   -0.04673324152827263,\n",
       "   0.02483253739774227,\n",
       "   -0.03166912868618965,\n",
       "   -0.030251141637563705,\n",
       "   0.015489171259105206,\n",
       "   -0.004534920211881399,\n",
       "   -0.026611534878611565,\n",
       "   -0.04237234592437744,\n",
       "   -0.017566001042723656,\n",
       "   -0.01977934129536152,\n",
       "   -0.0016368709038943052,\n",
       "   -0.02494443766772747,\n",
       "   0.0035739666782319546,\n",
       "   -0.023711342364549637,\n",
       "   0.029488347470760345,\n",
       "   -0.00603802315890789,\n",
       "   -0.011953224427998066,\n",
       "   0.01951363869011402,\n",
       "   0.010951663367450237,\n",
       "   -0.031100409105420113,\n",
       "   0.04731431230902672,\n",
       "   0.027284331619739532,\n",
       "   -0.057353947311639786,\n",
       "   0.01238525751978159,\n",
       "   -0.014512892812490463,\n",
       "   -0.019921913743019104,\n",
       "   -0.0021503721363842487,\n",
       "   -0.021614039316773415,\n",
       "   0.019223205745220184,\n",
       "   -0.02220516838133335,\n",
       "   0.03928050771355629,\n",
       "   -0.022813744843006134,\n",
       "   0.060604728758335114,\n",
       "   -0.017262235283851624,\n",
       "   0.09027295559644699,\n",
       "   0.01052737794816494,\n",
       "   0.005404589232057333,\n",
       "   0.038346901535987854,\n",
       "   0.011017114855349064,\n",
       "   0.0744422972202301,\n",
       "   0.03531540557742119,\n",
       "   -0.02989981696009636,\n",
       "   0.006308931857347488,\n",
       "   0.021371543407440186,\n",
       "   -0.031666673719882965,\n",
       "   -0.005461871158331633,\n",
       "   -0.010384750552475452,\n",
       "   0.055887024849653244,\n",
       "   0.05493555963039398,\n",
       "   0.0205534715205431,\n",
       "   -0.03973317891359329,\n",
       "   -0.04652896896004677,\n",
       "   -0.005092771723866463,\n",
       "   -0.0232295673340559,\n",
       "   0.09327144920825958,\n",
       "   -0.03699812665581703,\n",
       "   -0.03996850922703743,\n",
       "   0.029676370322704315,\n",
       "   0.010604851879179478,\n",
       "   0.01571393758058548,\n",
       "   -0.091934934258461,\n",
       "   -0.07440705597400665,\n",
       "   0.007551638409495354,\n",
       "   0.05692346766591072,\n",
       "   -0.02390563301742077,\n",
       "   0.024583805352449417,\n",
       "   -0.030726637691259384,\n",
       "   -0.00850586872547865,\n",
       "   -0.018314378336071968,\n",
       "   -0.0006685946136713028,\n",
       "   -0.0022869345266371965,\n",
       "   -0.03784783557057381,\n",
       "   -0.028888748958706856,\n",
       "   -0.02385270968079567,\n",
       "   -0.013409794308245182,\n",
       "   -0.028669921681284904,\n",
       "   -0.0129094822332263,\n",
       "   -0.04232531785964966,\n",
       "   -0.00500165019184351,\n",
       "   -0.009286636486649513,\n",
       "   0.009072915650904179,\n",
       "   -0.009319992735981941,\n",
       "   0.02801702730357647,\n",
       "   -0.09776997566223145,\n",
       "   -0.019127216190099716,\n",
       "   0.03752392530441284,\n",
       "   -0.005320013966411352,\n",
       "   0.03211032226681709,\n",
       "   -0.022103136405348778,\n",
       "   0.006215732078999281,\n",
       "   -0.04749658703804016,\n",
       "   0.03252628818154335,\n",
       "   0.06463693827390671,\n",
       "   0.05045095831155777,\n",
       "   0.003704702714458108,\n",
       "   0.017132466658949852,\n",
       "   -0.010052449069917202,\n",
       "   0.037101712077856064,\n",
       "   -0.014495491050183773,\n",
       "   -0.02038327232003212,\n",
       "   -0.011428087018430233,\n",
       "   0.049970731139183044,\n",
       "   -0.05648364871740341,\n",
       "   0.028084734454751015,\n",
       "   0.058044154196977615,\n",
       "   0.003600886557251215,\n",
       "   -0.004835398402065039,\n",
       "   0.03057062067091465,\n",
       "   0.04311671853065491,\n",
       "   -0.014851897023618221,\n",
       "   -0.050677843391895294,\n",
       "   0.012774263508617878,\n",
       "   -0.022842297330498695,\n",
       "   -0.05301102250814438,\n",
       "   0.03548642247915268,\n",
       "   0.09178446978330612,\n",
       "   -0.09309795498847961,\n",
       "   0.03971150144934654,\n",
       "   -0.012614278122782707,\n",
       "   0.031021855771541595,\n",
       "   -0.0775495246052742,\n",
       "   -0.002144063822925091,\n",
       "   -0.018868980929255486,\n",
       "   -0.06318877637386322,\n",
       "   0.07914240658283234,\n",
       "   -0.010649514384567738,\n",
       "   -0.06371164321899414,\n",
       "   -0.08251233398914337,\n",
       "   -0.03896645829081535,\n",
       "   -0.03147120773792267,\n",
       "   -0.037135325372219086,\n",
       "   0.028953569009900093,\n",
       "   -0.00391533225774765,\n",
       "   -0.01775090955197811,\n",
       "   0.005765616428107023,\n",
       "   0.0361911840736866,\n",
       "   -0.028875837102532387,\n",
       "   -0.029798170551657677,\n",
       "   0.020799795165657997,\n",
       "   -0.025340277701616287,\n",
       "   -0.03856281936168671,\n",
       "   0.05191182717680931,\n",
       "   0.026930520310997963,\n",
       "   -0.07022695243358612,\n",
       "   0.06908374279737473,\n",
       "   -0.03475872054696083,\n",
       "   0.04963918402791023,\n",
       "   0.021819500252604485,\n",
       "   0.03377288952469826,\n",
       "   0.01736098900437355,\n",
       "   -0.027380893006920815,\n",
       "   -0.008806836791336536,\n",
       "   -0.03990485519170761,\n",
       "   0.0194484181702137,\n",
       "   -0.022549496963620186,\n",
       "   0.009639578871428967,\n",
       "   0.08861284703016281,\n",
       "   0.06263778358697891,\n",
       "   -0.04098135232925415,\n",
       "   -0.02496619150042534,\n",
       "   -0.05061658099293709,\n",
       "   0.006491630803793669,\n",
       "   0.0061104376800358295,\n",
       "   0.057223785668611526,\n",
       "   0.009496135637164116,\n",
       "   -0.04260103777050972,\n",
       "   -0.0043558585457503796,\n",
       "   -0.02638089284300804,\n",
       "   0.031164124608039856,\n",
       "   0.012303565628826618,\n",
       "   -0.010823051445186138,\n",
       "   0.013221518136560917,\n",
       "   -0.0047757565043866634,\n",
       "   0.013162354938685894,\n",
       "   0.05642301216721535,\n",
       "   -0.04692328721284866,\n",
       "   0.03542199358344078,\n",
       "   0.041565507650375366,\n",
       "   0.05513747036457062,\n",
       "   0.017553342506289482,\n",
       "   -0.013366574421525002,\n",
       "   -0.002107695210725069,\n",
       "   -0.07944436371326447,\n",
       "   0.0073665836825966835,\n",
       "   0.009892799891531467,\n",
       "   0.0883617103099823,\n",
       "   0.02437024936079979,\n",
       "   -0.015847092494368553,\n",
       "   0.02942025661468506,\n",
       "   -0.06959472596645355,\n",
       "   -0.018253356218338013,\n",
       "   -0.025738565251231194,\n",
       "   -0.016601871699094772,\n",
       "   -0.014527781866490841,\n",
       "   -0.01278355810791254,\n",
       "   0.060198962688446045,\n",
       "   0.023717939853668213,\n",
       "   0.009246952831745148,\n",
       "   0.019082384184002876,\n",
       "   -0.07174842059612274,\n",
       "   -0.019689319655299187,\n",
       "   0.028906017541885376,\n",
       "   -0.0417344868183136,\n",
       "   0.03523920103907585,\n",
       "   0.0038672485388815403,\n",
       "   -0.004136720672249794,\n",
       "   -0.03408786654472351,\n",
       "   -0.026763878762722015,\n",
       "   -0.024051332846283913,\n",
       "   -0.013660662807524204,\n",
       "   -0.016717730090022087,\n",
       "   -0.08707905560731888,\n",
       "   0.018659815192222595,\n",
       "   -0.06481939554214478,\n",
       "   -0.015677684918045998,\n",
       "   0.03635651618242264,\n",
       "   0.032044392079114914,\n",
       "   0.00032767519587650895,\n",
       "   -0.04714611917734146,\n",
       "   -0.01434820331633091,\n",
       "   0.005122743546962738,\n",
       "   0.04343106597661972,\n",
       "   -0.026806263253092766,\n",
       "   -0.027359936386346817,\n",
       "   -0.04887072369456291,\n",
       "   0.03304457291960716,\n",
       "   0.05904344469308853,\n",
       "   0.021205518394708633,\n",
       "   -0.03911035507917404,\n",
       "   -0.024613406509160995,\n",
       "   0.004800621885806322,\n",
       "   -0.06290975958108902,\n",
       "   0.046479493379592896,\n",
       "   0.01820574514567852,\n",
       "   0.03812261298298836,\n",
       "   -0.016256840899586678,\n",
       "   -0.0375051386654377,\n",
       "   -0.018011832609772682,\n",
       "   0.004573486279696226,\n",
       "   0.023628778755664825,\n",
       "   0.010610430501401424,\n",
       "   0.027885938063263893,\n",
       "   0.0380733385682106,\n",
       "   0.0027638787869364023,\n",
       "   0.06786342710256577,\n",
       "   -0.01273559220135212,\n",
       "   -0.008724280633032322,\n",
       "   -0.022556426003575325,\n",
       "   -0.07782409340143204,\n",
       "   0.01682966947555542,\n",
       "   -0.00556029099971056,\n",
       "   0.024276914075016975,\n",
       "   -0.05600801855325699,\n",
       "   0.027441764250397682,\n",
       "   0.004376913886517286,\n",
       "   0.02797512151300907,\n",
       "   -0.014647053554654121,\n",
       "   -0.020624399185180664,\n",
       "   0.009567651897668839,\n",
       "   0.04647456482052803,\n",
       "   0.02949059195816517,\n",
       "   0.017577674239873886,\n",
       "   -0.035421352833509445,\n",
       "   -0.003742415923625231,\n",
       "   0.040565378963947296,\n",
       "   -0.014826791360974312,\n",
       "   0.04539201408624649,\n",
       "   -0.032964371144771576,\n",
       "   -0.036510612815618515,\n",
       "   0.012840336188673973,\n",
       "   -0.025205254554748535,\n",
       "   0.0727033019065857,\n",
       "   0.056651338934898376,\n",
       "   0.025317097082734108,\n",
       "   -0.005838995799422264,\n",
       "   -0.05942046642303467,\n",
       "   -0.03996808081865311,\n",
       "   0.0012013688683509827,\n",
       "   -0.028023526072502136,\n",
       "   0.03248954936861992,\n",
       "   -0.03594371676445007,\n",
       "   -0.009175090119242668,\n",
       "   0.07564442604780197,\n",
       "   0.06186062842607498,\n",
       "   -0.0017088776221498847,\n",
       "   -0.0507180318236351,\n",
       "   -0.010602721944451332,\n",
       "   -0.0183254424482584,\n",
       "   -0.00400714622810483,\n",
       "   0.019947368651628494,\n",
       "   -5.186260872018778e-33,\n",
       "   0.023953383788466454,\n",
       "   -0.0803983062505722,\n",
       "   0.03905242308974266,\n",
       "   0.05129772424697876,\n",
       "   -0.025285987183451653,\n",
       "   -0.034174881875514984,\n",
       "   0.039938610047101974,\n",
       "   0.017303744331002235,\n",
       "   -0.04531153663992882,\n",
       "   -0.004276337567716837,\n",
       "   -0.013970051892101765,\n",
       "   0.022973602637648582,\n",
       "   0.017748920246958733,\n",
       "   0.02182912267744541,\n",
       "   0.0050146630965173244,\n",
       "   0.039873819798231125,\n",
       "   0.01296907663345337,\n",
       "   0.02421901561319828,\n",
       "   0.014359804801642895,\n",
       "   -0.015012301504611969,\n",
       "   -0.09058327972888947,\n",
       "   0.011845441535115242,\n",
       "   -0.03126461058855057,\n",
       "   -0.0810643807053566,\n",
       "   0.039984576404094696,\n",
       "   -0.02332058735191822,\n",
       "   -0.0036102605517953634,\n",
       "   -0.008223467506468296,\n",
       "   -0.05402123183012009,\n",
       "   0.02897733822464943,\n",
       "   0.019307762384414673,\n",
       "   0.012770825065672398,\n",
       "   0.03213731199502945,\n",
       "   -0.025136902928352356,\n",
       "   -0.008544761687517166,\n",
       "   0.02341931127011776,\n",
       "   -0.02801448293030262,\n",
       "   0.009109961800277233,\n",
       "   0.0499471016228199,\n",
       "   0.0456770621240139,\n",
       "   0.007456810213625431,\n",
       "   -0.05861564725637436,\n",
       "   0.02263263612985611,\n",
       "   -0.04492925480008125,\n",
       "   -0.01576993614435196,\n",
       "   -0.009398458525538445,\n",
       "   0.08270809799432755,\n",
       "   -0.024539118632674217,\n",
       "   0.020090701058506966,\n",
       "   -0.01686345413327217,\n",
       "   -0.02563626691699028,\n",
       "   0.002688058651983738,\n",
       "   -0.12085483223199844,\n",
       "   0.03754136711359024,\n",
       "   0.04714888334274292,\n",
       "   0.0838056206703186,\n",
       "   -0.0016889420803636312,\n",
       "   -0.010533694177865982,\n",
       "   -0.025351470336318016,\n",
       "   0.029242316260933876,\n",
       "   -0.034248486161231995,\n",
       "   -0.03985171020030975,\n",
       "   0.06312275677919388,\n",
       "   0.05658945441246033,\n",
       "   0.02564694918692112,\n",
       "   0.015026123262941837,\n",
       "   0.0780845433473587,\n",
       "   0.0007009481196291745,\n",
       "   0.010817833244800568,\n",
       "   -0.007918095216155052,\n",
       "   -0.03871836140751839,\n",
       "   0.06681124866008759,\n",
       "   -0.016225961968302727,\n",
       "   0.010435939766466618,\n",
       "   -0.021786706522107124,\n",
       "   -0.015210646204650402,\n",
       "   -0.017995115369558334,\n",
       "   0.01639448292553425,\n",
       "   0.06635988503694534,\n",
       "   -0.06460767984390259,\n",
       "   -0.05817091837525368,\n",
       "   0.026552405208349228,\n",
       "   -0.00295640854164958,\n",
       "   -0.03661781921982765,\n",
       "   -0.02665877155959606,\n",
       "   -0.06685741990804672,\n",
       "   0.002843976253643632,\n",
       "   0.00488547096028924,\n",
       "   0.01957189477980137,\n",
       "   -0.08651582151651382,\n",
       "   -0.024871189147233963,\n",
       "   0.09339847415685654,\n",
       "   0.01192248985171318,\n",
       "   -0.013846904039382935,\n",
       "   -0.012156653217971325,\n",
       "   -0.039002981036901474,\n",
       "   0.004079916514456272,\n",
       "   -0.004831444937735796,\n",
       "   0.02141500636935234,\n",
       "   0.02564980462193489,\n",
       "   -0.02696484886109829,\n",
       "   -0.011353326961398125,\n",
       "   -0.04062480479478836,\n",
       "   0.02457592636346817,\n",
       "   0.05166170746088028,\n",
       "   0.04828568547964096,\n",
       "   -0.03095051646232605,\n",
       "   0.009102283045649529,\n",
       "   0.019298991188406944,\n",
       "   -0.011065486818552017,\n",
       "   -0.001679538399912417,\n",
       "   -0.003974368330091238,\n",
       "   -0.01273619756102562,\n",
       "   0.04897423833608627,\n",
       "   -0.010656744241714478,\n",
       "   -0.0019386127823963761,\n",
       "   0.02153070643544197,\n",
       "   0.020578576251864433,\n",
       "   -0.007373173255473375,\n",
       "   -0.04627309367060661,\n",
       "   0.013228067196905613,\n",
       "   -0.041431114077568054,\n",
       "   -0.03323819115757942,\n",
       "   -0.030182726681232452,\n",
       "   -0.021678106859326363,\n",
       "   -0.015312508679926395,\n",
       "   -0.022399229928851128,\n",
       "   0.032320741564035416,\n",
       "   0.027760902419686317,\n",
       "   -0.027496660128235817,\n",
       "   -0.012027601711452007,\n",
       "   0.0019021466141566634,\n",
       "   2.647588530635403e-07,\n",
       "   0.011262779124081135,\n",
       "   -0.005335524212568998,\n",
       "   -0.0024820619728416204,\n",
       "   0.002383535262197256,\n",
       "   0.047141533344984055,\n",
       "   -0.007429585792124271,\n",
       "   -0.01841721683740616,\n",
       "   -0.004819404799491167,\n",
       "   0.007716841530054808,\n",
       "   0.04125004634261131,\n",
       "   -0.008572198450565338,\n",
       "   0.014359520748257637,\n",
       "   0.017129801213741302,\n",
       "   -0.04391207918524742,\n",
       "   -0.017227159813046455,\n",
       "   -0.07369689643383026,\n",
       "   -0.021423688158392906,\n",
       "   -0.018278997391462326,\n",
       "   -0.04557633772492409,\n",
       "   0.06286872178316116,\n",
       "   0.02548733726143837,\n",
       "   0.024414120241999626,\n",
       "   0.014281335286796093,\n",
       "   -0.0032019554637372494,\n",
       "   -0.013874165713787079,\n",
       "   -0.013330481946468353,\n",
       "   -0.041460685431957245,\n",
       "   -0.030268779024481773,\n",
       "   0.06691598147153854,\n",
       "   0.050109438598155975,\n",
       "   0.0683194026350975,\n",
       "   0.004049795214086771,\n",
       "   0.045994654297828674,\n",
       "   0.014373881742358208,\n",
       "   0.011870389804244041,\n",
       "   -0.02775079943239689,\n",
       "   0.02142532728612423,\n",
       "   0.0733359307050705,\n",
       "   -0.011956511996686459,\n",
       "   0.022112490609288216,\n",
       "   0.01798412576317787,\n",
       "   0.019513098523020744,\n",
       "   -0.0035081948153674603,\n",
       "   -0.06002660095691681,\n",
       "   0.0776355043053627,\n",
       "   1.846203849709127e-05,\n",
       "   -0.07520409673452377,\n",
       "   -0.06427866220474243,\n",
       "   -0.02064378932118416,\n",
       "   -0.036013565957546234,\n",
       "   0.03211090341210365,\n",
       "   -0.009124895557761192,\n",
       "   0.013430302031338215,\n",
       "   0.05146940425038338,\n",
       "   -0.007395401131361723,\n",
       "   -0.06604277342557907,\n",
       "   -0.04523385688662529,\n",
       "   0.015605196356773376,\n",
       "   0.007484272588044405,\n",
       "   -0.037858959287405014,\n",
       "   -0.004240340553224087,\n",
       "   -0.0218354444950819,\n",
       "   0.029015684500336647,\n",
       "   -0.035133302211761475,\n",
       "   0.08101723343133926,\n",
       "   -0.002637856872752309,\n",
       "   0.011823884211480618,\n",
       "   2.060540577945696e-34,\n",
       "   -0.006109119392931461,\n",
       "   0.024517254903912544,\n",
       "   -0.019964473322033882,\n",
       "   0.030486278235912323,\n",
       "   0.052110835909843445,\n",
       "   -0.05133482813835144,\n",
       "   -0.048142533749341965,\n",
       "   0.008732233196496964,\n",
       "   -0.009312549605965614,\n",
       "   0.06267710775136948,\n",
       "   -0.01333244051784277],\n",
       "  'chapter': 'CHAPTER 1',\n",
       "  'title': 'Machine Learning Roles and the Interview Process'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create embeddings using Pretrained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup ElasticSearch connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': '810a7a347b3b',\n",
       " 'cluster_name': 'docker-cluster',\n",
       " 'cluster_uuid': 'b66KeqxXTB2vOEKVbJx8WQ',\n",
       " 'version': {'number': '8.4.3',\n",
       "  'build_flavor': 'default',\n",
       "  'build_type': 'docker',\n",
       "  'build_hash': '42f05b9372a9a4a470db3b52817899b99a76ee73',\n",
       "  'build_date': '2022-10-04T07:17:24.662462378Z',\n",
       "  'build_snapshot': False,\n",
       "  'lucene_version': '9.3.0',\n",
       "  'minimum_wire_compatibility_version': '7.17.0',\n",
       "  'minimum_index_compatibility_version': '7.0.0'},\n",
       " 'tagline': 'You Know, for Search'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es_client = Elasticsearch('http://localhost:9200') \n",
    "\n",
    "es_client.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0\n",
    "    },\n",
    "    \"mappings\": {\n",
    "    \"properties\": {\n",
    "        \"text\": {\n",
    "            \"type\": \"text\",\n",
    "        },\n",
    "        \"section\": {\n",
    "            \"type\": \"text\",\n",
    "        },\n",
    "        \"chapter\": {\n",
    "            \"type\": \"text\",\n",
    "        },\n",
    "        \"title\": {\n",
    "            \"type\": \"keyword\",\n",
    "            \"null_value\": \"Untitled\"\n",
    "        },\n",
    "        \"text_vector\": {\n",
    "            \"type\": \"dense_vector\",\n",
    "            \"dims\": 768,\n",
    "            \"index\": True,\n",
    "            \"similarity\": \"cosine\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True,\n",
       " 'shards_acknowledged': True,\n",
       " 'index': 'ml-interview-questions'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_name = \"ml-interview-questions\"\n",
    "\n",
    "es_client.indices.delete(index=index_name, ignore_unavailable=True)\n",
    "es_client.indices.create(index=index_name, body=index_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in documents:\n",
    "    try:\n",
    "        es_client.index(index=index_name, body=doc)\n",
    "    except Exception as e:\n",
    "        print(f\"Error when indexing the document: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create end user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term = \"what are the steps of ML interviews?\"\n",
    "vector_search_term = model.encode(search_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_index': 'ml-interview-questions',\n",
       "  '_id': 'Ibez-ZEBwj93mx26wpoG',\n",
       "  '_score': 0.8769342,\n",
       "  '_source': {'chapter': 'CHAPTER 8',\n",
       "   'text': 'Follow this checklist to create a plan for your interview process. Refer back to rele‐ vant content or past exercises in this book to help you complete the checklist: • Write down the parts of the ML lifecycle that you’re interested in doing at work. See Figure 1-5 in Chapter 1 for a reminder of the ML lifecycle. • Write down the skills that are required for that role and run the self-assessment of them in Chapter 2 . • Determine what types of interviews could be relevant for that role. Review the overview of the interview process in Chapter 1 . • Make sure your resume is tidied up, with bullet points relevant to the role you picked. Refer to Chapter 2 for more resume tips. • Write down a time frame during which you’re aiming to prepare for interviews and start applying. For example: I aim to prepare for interviews for three months and then start applying. Now that you have these components in place, it’s time to construct your roadmap.',\n",
       "   'title': 'Tying It All Together: Your Interview Roadmap'}},\n",
       " {'_index': 'ml-interview-questions',\n",
       "  '_id': 'Ebez-ZEBwj93mx26wJpv',\n",
       "  '_score': 0.8674362,\n",
       "  '_source': {'chapter': 'CHAPTER 6',\n",
       "   'text': 'As you saw in Figure 1-1 , there are additional interview types. Usually, they are more advanced components, assessing candidates on various combinations of ML, coding, training, and deployment (content that has been covered so far in Chapters 3 , 4 , and 5 and this chapter). Additional types that you might commonly hear about are: • Machine learning systems design interview • Technical deep-dive interview • Take-home exercises • Product sense I will briefly go through each of these interview types, so you are aware of how to prepare for them. I personally didn’t need to prepare for these types of interviews when I was looking for my entry-level role, as ML theory and coding were sufficient. However, I have encountered more and more advanced interviews as I progressed to senior and staff+ roles. Each company may ask for only some of these or none at all, so what you encounter in your interviews will differ. For example, Meta asks MLE candidates systems design questions, not just candidates at the “senior” level. ML systems design interviews and questions ask you to design something in an often hypothetical scenario. This could include asking you to design a brand-new system from scratch or how you’d hypothetically design a known system. Examples include: • “Imagine you are part of an ecommerce company’s ML team. It is aiming to use ML to increase customer retention. Walk through your initial approaches and how you would accomplish this.” • “How would you introduce ML-powered restaurant recommendations on Goo‐ gle Maps?” • “The online game our company is developing uses reinforcement learning to improve player experience. How would you design such a system?” ML systems design questions are often open ended, with plenty of back-and-forth with the interviewer asking follow-up questions that they find interesting. ML sys‐ tems design questions can be quite challenging for a couple of reasons: They likely don’t have a 100% correct answer. Since the questions are often about hypothetical scenarios, the questions them‐ selves might also change on the fly. For example, I (as the candidate) might ask the interviewer, “How many daily users are we expecting for this ML system?” Given the same question, by design the interviewer might not have defined all the parameters of the scenario and makes up a plausible number on the spot. Lots of what you do during the ML systems design is merely estimation and back-of-the-envelope math, and there is often no correct tool to choose (e.g., for some scenarios, you could use either XGBoost or CatBoost). ML systems design questions have high variance between each company, team, and interviewer. Much of your performance depends not only on your initial design but also on how you can respond to open-ended questions that could go in any direction. The interviewer could be curious about how you’d deal with the speed of the ML inference, and you could spend another five minutes on that topic. Or, by chance, they might ask instead about how you’d ensure that data quality is high before training the models. Treat it like improv and be able to adjust to how the conversation is flowing between you and your interviewer. Do yourself a favor and check the job posting to see what aspects you should focus on. Even in a scenario where the systems design questions ask you to design an end to-end ML project, you can spend more time focusing on the core competencies of the position during the job interview. If you’re interviewing for a data scientist posi‐ tion that trains and evaluates ML models, then elaborate more on that and less on the deployment. Don’t ignore other aspects of the ML system if it’s an end-to-end system question, though. If you’re interviewing for an MLE position that focuses on deploy‐ ment, spend a little more time on that instead of getting stuck in a rabbit hole about data engineering. If in doubt, ask your interviewer if you’re focusing on the right thing and if they’d like to dive deeper on any topic. I won’t provide further examples here, since they would build on and combine the information from the ML algorithm, ML evaluation, ML deployment, and coding interviews that we’ve already discussed in this book. For entry-level roles, if there are systems design questions, they will focus on skills that have been covered in the pre‐ vious chapters. The most advanced systems design questions are mostly reserved for more senior and staff+ roles. For greater depth on this subject, I recommend the following resources: • “ML Systems Design Interview Guide” by Patrick Halina • Machine Learning System Design Interview by Ali Aminian and Alex Xu (ByteByteGo) • Search YouTube videos on example system design interviews for ML; this is a good example: “Harmful Content Removal: Machine Learning (System Design) Staff Level Mentorship” by Interviewing.io. (This question is aimed at the L7 staff position.) At Meta, example questions include “design a personalized news ranking system,” “design a product recommendation system,” and so on. As you can see from the fol‐ lowing breakdown, Meta is looking for an accumulation of the skills discussed in this book, not just a narrow subsection of them. The signals the interviewer is looking for from candidates include: Problem navigation Seeing if the candidate can organize the entire problem. Meta’s interview prep guide highlights that candidates should connect the problem to the business con‐ text. (Refer to Chapter 4 and the section “Product Sense” on page 214 .) Training data How would you collect training data and evaluate the risks? (Refer to Chapter 4 .) Feature engineering How would you come up with relevant features for the ML task? (Refer to Chapter 4 .) Modeling How do you justify choosing a specific model? Explain the training process, and explain the risks and how you’d mitigate them. (Refer to Chapters 3 and 4 .) Evaluation and deployment How do you evaluate and deploy the model? How do you justify which metrics to monitor? (Refer to Chapters 4 and 6 .) You can read Meta’s official resource in its software engineer ML full loop interview prep guide , which can be found on its careers site . In Meta’s interview prep guide, you’ll see repeated mentions of expecting candidates to come up with potential risks and mitigations for the ML designs they propose. This is a useful pattern of thinking for all ML interviews and a sign of a more effective and thoughtful ML practitioner. A useful and important way to improve your discus‐ sion of possible risks is to read about AI biases because they are a big part of risks. Research from Timnit Gebru and Joy Buolamwini are good resources; for example, they investigate ML algorithms’ accuracy disparities on gender and race (via skin type). 19 Meta’s own blog on progress and learnings in AI fairness and transparency also mentions various risks and mitigations. 20 Meta’s efforts include creating more datasets to “help researchers evaluate their computer vision and audio models for accuracy across a diverse set of ages, genders, skin tones, and ambient lighting conditions.” Technical deep-dive questions allow you to walk through something you’ve designed and built from scratch in the past, discussing the trade-offs and challenges you encountered along the way and how you addressed them. I have frequently seen this type of question grouped under behavioral questions that are related to past projects; for example, Shopify places a large emphasis on technical deep dives in its technical There are many companies that do this type of interview, and I’ve heard it called many names: case study interviews (different from the consulting type of case studies), reverse systems design, retro‐ spective systems design, and whatnot. I’m borrowing Shopify’s term, technical  deep dive , to refer to this kind of interview and interview questions in this book. Depending on the interview phase and the interviewer, answering this type of ques‐ tion can require more technical explanation and a deeper dive than the usual behav‐ ioral interview, and it has the depth and volleying that systems design has. However, it differs from the regular systems design questions with plenty of preparation mate‐ rial online since those questions focus on hypothetical situations instead of some‐ thing you actually built in a former job or project. Anecdotally, the more senior I get, the more questions I have gotten that are of the technical deep-dive variant. Here are important things to keep in mind while answering technical deep-dive questions: Justify and understand trade-offs in the system. Why did you choose BERT-cased over BERT-uncased? Why did you choose a basic Q-learning algorithm over a deep Q-network (DQN), or the other way around? Bring up analyses or benchmarks you ran during the time and be pre‐ pared to use them to justify your past choices. Deeply understand the main components you were responsible for. If you were responsible for training the model, be prepared to answer questions on the inner workings or even the mathematical foundations of those algorithms. For example, I’ve been asked how matrix factorization works, stemming from my project that used matrix factorization in the context of collaborative filtering. If you were responsible for the deployment infrastructure, such as in an MLE role, be prepared to answer more details on the Ops side. Sometimes, companies will provide an exercise or assessment for the candidate to do at home. These might be graded automatically, and a candidate would be passed or failed. There are also open-ended take-home exercises where the goal isn’t to pass or fail the candidate by the exercise alone but to combine it with an interview discussion where the candidate walks the interviewers through their solutions. The tips for ML algorithms and coding from previous chapters all apply: • Make sure you can not only explain the algorithms but also the trade-offs as well as why and how you decided on your approach. • Explain your thought process clearly to interviewers with docstrings in the code as well as verbally during the interviews. • Write tests! In data science and ML interviews, especially in big tech, a hidden requirement is that candidates possess some “product sense.” This is an umbrella term some companies use to describe whether a job candidate has practical knowledge on how ML benefits the company’s products. This can be shown when speaking about ML products and when you research the company’s products. It’s important to understand common product objectives for ML, such as: • Increase user convenience • Decrease user churn • Improve onboarding experience This is becoming more well-known nowadays; if you look up “data science product sense” on a search engine, some guides will show up. However, many candidates don’t think about preparing for this unless it’s explicitly mentioned by the recruiter or during the hiring process. As an ML candidate, you can integrate product sense into your behavioral interviews, systems design interviews, technical deep dives, and so on. The way you would prepare is to borrow from product manager interviews. From an interviewer’s perspective, I think of it this way. Does the candidate care only about model accuracy metrics, or do they also care about monthly average users of the product? Do they connect the ML they’re building to the product? Don’t underestimate this—when I was starting out in the ML field, much more expe‐ rienced people and successful peers recommended that I learn more about and understand the business side. This is one way that mentorship benefited my career: there is a lot of information that’s not shared in question-bank-type interview guides. In turn, I have included as much of that latent information as possible in this book. Here are some resources to get started: • “The Ultimate Guide to Cracking Product Case Interviews for Data Scientists” (Part 1) by Emma Ding • Exponent videos on product sense, such as this “Meta/Facebook Product Man‐ ager Mock Interview” • Cracking the PM Interview by Gayle Laakmann McDowell and Jackie Bavaro (CareerCup) Here are some interview questions I’ve used to interview MLOps engineers and MLEs who work on infrastructure. These interview questions include sample answers to help provide inspiration for your own potential responses. I want to point out that these questions mostly hinge on asking about your experience; most likely, the MLOps engineer and MLE will share the core coding interview loop ( Chapter 5 ) with other roles, and then the resume walk-through and technical deep-dive questions will include questions such as the ones I’ve given here. As I mentioned in Chapter 5 , those roles that are more focused on operations might also have more specialized coding questions that are similar to questions asked of DevOps engineers. At the risk of repeating myself too much, it is best that you double-check the job posting and with your recruiter and hiring manager, if possible, on the focus and expectations of the interview. For this chapter, it’s especially important to note that your answers will differ depending on your own experience; these answers are only high-level, relatively generic examples to show you what an answer might look like. Do not use them as a real answer in your interviews unless you’ve done the tasks/projects mentioned in the example answers. Example answer Using scaling on Kubernetes helped; for example, horizontal scaling helped dis‐ tribute the same workload across more instances. In cases when the heavy load came from request volume, I used load balancing with the Google Kubernetes Engine. In the past, I’ve used autoscaling features in cloud platforms, such as when I was working with GCP. Example answer For machine learning, I’ve learned that what’s different between monitoring an ML application in production as opposed to an application without ML is the data and model-related monitoring. This includes monitoring for data drift, model accuracy and drifts, and so on. For this, I use tools such as Great Expecta‐ tions or Alibi Detect . In particular, at my previous company we used Great Expectations to check for sudden large amounts of missing values or distribution shifts. In addition, using those monitoring tools, I can create alerts and have recurring anomaly-detection jobs on those platforms to report errors or drifts. On the ser‐ vice availability side (more general, less ML specific), tools such as Grafana, ELK Stack (Elasticsearch, Logstash, and Kibana, aka Elastic Stack), and Prometheus are commonly used. Example answer I started by automating the steps involved in the ML pipeline, such as tidying up the scripts for data preprocessing, model training, and evaluation. Then I inte‐ grated those steps into a CI/CD pipeline with Jenkins , triggering a pipeline run when there are changes to the code on our GitHub repo. This pipeline includes spinning up the environment, code linting, and testing, followed by automatic model deployment to a staging environment for further testing. Upon successful validation, the model is copied over to the production environment. These steps automated the deployment process, saving on manual deployment time and also allowing for quality control.',\n",
       "   'title': 'Technical Interview: Model Deployment and End-to-End ML'}},\n",
       " {'_index': 'ml-interview-questions',\n",
       "  '_id': '27ez-ZEBwj93mx26upkH',\n",
       "  '_score': 0.8662212,\n",
       "  '_source': {'chapter': 'CHAPTER 2',\n",
       "   'text': 'This chapter focused on the ML job application step. This happens before you get interviews and is the key to getting interviews. You saw where to find jobs online and some ways to increase your chances of getting interviews via networking and refer‐ rals. You also read through some resume best practices, and hopefully, you have cre‐ ated an initial version of your resume. If you have it and feel ready, I encourage you to start applying for ML jobs even if you feel that your skills or your resume aren’t 100% perfect. Next, in the following chapters I’ll go through the various types of interviews, span‐ ning technical and behavioral interviews. First up are ML algorithms and theory, which are part of the technical interview.',\n",
       "   'title': 'Machine Learning Job Application and Resume'}},\n",
       " {'_index': 'ml-interview-questions',\n",
       "  '_id': '3Lez-ZEBwj93mx26upki',\n",
       "  '_score': 0.8554759,\n",
       "  '_source': {'chapter': 'CHAPTER 3',\n",
       "   'text': 'In Chapter 1 , you learned about the various steps you will go through as part of your ML interviews. In Chapter 2 , you looked at how to tie your experiences to roles of interest as well as how to craft a relevant resume. The goal of the previous chapters was to get you invited to interviews. In this chapter, I’ll focus on ML algorithms. As you recall, the interview process is illustrated in Figure 1-9 , and the ML algorithms interview is only one portion of the technical interviews; the rest, such as ML training and evaluation, coding, and so on, will be covered in subsequent chapters.',\n",
       "   'title': 'Technical Interview: Machine Learning Algorithms'}},\n",
       " {'_index': 'ml-interview-questions',\n",
       "  '_id': 'H7ez-ZEBwj93mx26wZrO',\n",
       "  '_score': 0.8550886,\n",
       "  '_source': {'chapter': 'CHAPTER 8',\n",
       "   'text': 'Now that you’ve gone through the entire ML interview process, it’s time to create a plan. In Chapters 1 and 2 , you learned about the many types of ML jobs and did a self-assessment of which one(s) might be more suitable for you. Based on that, you also learned about the skills you are expected to be stronger in. In the subsequent chapters, you learned about what types of questions are commonly asked in inter‐ views. Are there any types that you need to prepare more for? The goal of this book is for you to start bridging the gap, not just read about bridging the gap. To succeed in interviews and land the job, taking action will help you—not just thinking about taking action.',\n",
       "   'title': 'Tying It All Together: Your Interview Roadmap'}}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = es_client.search(\n",
    "    index=\"ml-interview-questions\",\n",
    "    body={\n",
    "        \"size\": 5,  \n",
    "        \"knn\": {\n",
    "            \"field\": \"text_vector\",  \n",
    "            \"query_vector\": vector_search_term,\n",
    "            \"k\": 5,  \n",
    "            \"num_candidates\": 1000  \n",
    "        },\n",
    "        \"_source\": [\"text\", \"section\", \"title\", \"chapter\"]\n",
    "    }\n",
    ")\n",
    "res['hits']['hits']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_query = {\n",
    "    \"field\": \"text_vector\",\n",
    "    \"query_vector\": vector_search_term,\n",
    "    \"k\": 5,\n",
    "    \"num_candidates\": 10000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_index': 'ml-interview-questions',\n",
       "  '_id': '3Lez-ZEBwj93mx26upki',\n",
       "  '_score': 2.817597,\n",
       "  '_source': {'text': 'In Chapter 1 , you learned about the various steps you will go through as part of your ML interviews. In Chapter 2 , you looked at how to tie your experiences to roles of interest as well as how to craft a relevant resume. The goal of the previous chapters was to get you invited to interviews. In this chapter, I’ll focus on ML algorithms. As you recall, the interview process is illustrated in Figure 1-9 , and the ML algorithms interview is only one portion of the technical interviews; the rest, such as ML training and evaluation, coding, and so on, will be covered in subsequent chapters.',\n",
       "   'text_vector': [0.0555436834692955,\n",
       "    -0.057862937450408936,\n",
       "    -0.06579995900392532,\n",
       "    -0.018207866698503494,\n",
       "    0.008833812549710274,\n",
       "    0.02567276917397976,\n",
       "    -0.01176581159234047,\n",
       "    -0.03683503344655037,\n",
       "    0.007987787015736103,\n",
       "    -0.03394889086484909,\n",
       "    0.06602350622415543,\n",
       "    0.05767723545432091,\n",
       "    0.033435020595788956,\n",
       "    0.03225232660770416,\n",
       "    0.04612836241722107,\n",
       "    -0.05221432074904442,\n",
       "    0.026441171765327454,\n",
       "    -0.0014632202219218016,\n",
       "    -0.02367429994046688,\n",
       "    -0.02824978344142437,\n",
       "    -0.01720539666712284,\n",
       "    0.011182794347405434,\n",
       "    -0.02285507321357727,\n",
       "    0.01757640205323696,\n",
       "    -0.0022415067069232464,\n",
       "    -0.04095837473869324,\n",
       "    -0.02818242833018303,\n",
       "    0.02927485667169094,\n",
       "    -0.024174313992261887,\n",
       "    0.01460934802889824,\n",
       "    -0.033344026654958725,\n",
       "    -0.06602809578180313,\n",
       "    0.01739729754626751,\n",
       "    -0.00852904748171568,\n",
       "    2.1192950043769088e-06,\n",
       "    -0.04168311879038811,\n",
       "    0.008932244032621384,\n",
       "    0.0050083850510418415,\n",
       "    -0.038519881665706635,\n",
       "    -0.07101845741271973,\n",
       "    0.014262642711400986,\n",
       "    0.023514946922659874,\n",
       "    0.03278544545173645,\n",
       "    0.006159830838441849,\n",
       "    0.004875018261373043,\n",
       "    0.02702215686440468,\n",
       "    0.06059115752577782,\n",
       "    0.03233695030212402,\n",
       "    0.02308649942278862,\n",
       "    0.05835425853729248,\n",
       "    0.0018108042422682047,\n",
       "    -0.054530736058950424,\n",
       "    0.017236042767763138,\n",
       "    0.05804227665066719,\n",
       "    0.009896809235215187,\n",
       "    0.0024993084371089935,\n",
       "    -0.007473664823919535,\n",
       "    -0.0018575682770460844,\n",
       "    0.028342463076114655,\n",
       "    -0.08948442339897156,\n",
       "    0.006773480214178562,\n",
       "    0.027347367256879807,\n",
       "    0.013087188825011253,\n",
       "    0.011941199190914631,\n",
       "    0.07401155680418015,\n",
       "    0.03871525451540947,\n",
       "    -0.015987496823072433,\n",
       "    -0.04244381561875343,\n",
       "    -0.034510288387537,\n",
       "    -0.022553689777851105,\n",
       "    0.05420053377747536,\n",
       "    -0.02955695241689682,\n",
       "    -0.010906168259680271,\n",
       "    0.0023433237802237272,\n",
       "    0.04894989728927612,\n",
       "    -0.026037678122520447,\n",
       "    -0.0288933627307415,\n",
       "    -0.0016128385905176401,\n",
       "    -0.0076993368566036224,\n",
       "    -0.026812460273504257,\n",
       "    -0.04198426008224487,\n",
       "    0.024094486609101295,\n",
       "    -0.005777856335043907,\n",
       "    -0.010915958322584629,\n",
       "    0.0020754162687808275,\n",
       "    0.08892589807510376,\n",
       "    -0.011866339482367039,\n",
       "    -0.004000314977020025,\n",
       "    0.0317656509578228,\n",
       "    -0.003056977177038789,\n",
       "    0.10577969253063202,\n",
       "    0.009065842255949974,\n",
       "    0.04349280893802643,\n",
       "    0.05554656311869621,\n",
       "    -0.02227569930255413,\n",
       "    -0.009005554020404816,\n",
       "    0.010012597776949406,\n",
       "    0.04513302445411682,\n",
       "    -0.03996916115283966,\n",
       "    -0.05106068775057793,\n",
       "    -0.03335968777537346,\n",
       "    -0.03976384177803993,\n",
       "    0.0300985649228096,\n",
       "    -0.009320302866399288,\n",
       "    0.007749408017843962,\n",
       "    0.0040068961679935455,\n",
       "    0.0126752695068717,\n",
       "    -0.007286017760634422,\n",
       "    -0.06379349529743195,\n",
       "    -0.02335672825574875,\n",
       "    -0.024715466424822807,\n",
       "    -0.015609756112098694,\n",
       "    -0.04227031394839287,\n",
       "    0.06827794760465622,\n",
       "    0.016676589846611023,\n",
       "    0.047177065163850784,\n",
       "    -0.0035761441104114056,\n",
       "    0.02046915702521801,\n",
       "    -0.0030867676250636578,\n",
       "    -0.01624840497970581,\n",
       "    -0.05002843216061592,\n",
       "    -0.0049226293340325356,\n",
       "    0.025981562212109566,\n",
       "    0.014169106259942055,\n",
       "    -0.06140174716711044,\n",
       "    -0.11447747051715851,\n",
       "    -0.01888675056397915,\n",
       "    0.011734779924154282,\n",
       "    -0.02456224150955677,\n",
       "    -0.03797826170921326,\n",
       "    0.04024959355592728,\n",
       "    0.010643023997545242,\n",
       "    -0.03059883788228035,\n",
       "    -0.038278792053461075,\n",
       "    0.021988922730088234,\n",
       "    0.037600889801979065,\n",
       "    0.005798473488539457,\n",
       "    0.059164609760046005,\n",
       "    -0.05642351880669594,\n",
       "    -0.00628235936164856,\n",
       "    0.05029341205954552,\n",
       "    -0.009426411241292953,\n",
       "    0.007925529032945633,\n",
       "    -0.014529744163155556,\n",
       "    0.02758382260799408,\n",
       "    0.0011672679102048278,\n",
       "    0.008453315123915672,\n",
       "    0.02347537688910961,\n",
       "    -0.006724216043949127,\n",
       "    0.012913526967167854,\n",
       "    -0.05142621323466301,\n",
       "    0.040453679859638214,\n",
       "    0.014146007597446442,\n",
       "    0.02165142260491848,\n",
       "    0.06602109968662262,\n",
       "    -0.024761414155364037,\n",
       "    0.004136813338845968,\n",
       "    0.03475387021899223,\n",
       "    0.004976203199476004,\n",
       "    0.04137200489640236,\n",
       "    0.030602457001805305,\n",
       "    -0.0865999311208725,\n",
       "    0.06299915164709091,\n",
       "    0.04042217135429382,\n",
       "    -0.019092941656708717,\n",
       "    -0.06676444411277771,\n",
       "    -0.010988704860210419,\n",
       "    0.06927398592233658,\n",
       "    -0.03410648927092552,\n",
       "    0.05176632106304169,\n",
       "    0.00696374149993062,\n",
       "    0.000979853211902082,\n",
       "    -0.03008519671857357,\n",
       "    -0.002183991251513362,\n",
       "    0.020917464047670364,\n",
       "    0.05570581182837486,\n",
       "    -0.0036588606890290976,\n",
       "    0.011243538931012154,\n",
       "    0.012576507404446602,\n",
       "    0.05081085488200188,\n",
       "    -0.003095656866207719,\n",
       "    0.012759125791490078,\n",
       "    -0.022207455709576607,\n",
       "    0.02410558983683586,\n",
       "    -0.03610306605696678,\n",
       "    -0.04237627610564232,\n",
       "    -0.09075170010328293,\n",
       "    0.04033561795949936,\n",
       "    -0.02238745242357254,\n",
       "    0.00826395396143198,\n",
       "    0.0049531483091413975,\n",
       "    0.024598030373454094,\n",
       "    -0.04518873989582062,\n",
       "    0.04118770733475685,\n",
       "    -0.030843881890177727,\n",
       "    -0.017632674425840378,\n",
       "    -0.05936505272984505,\n",
       "    -0.03088628686964512,\n",
       "    -0.04211476817727089,\n",
       "    -0.004396734293550253,\n",
       "    -0.05650852620601654,\n",
       "    -0.04228554666042328,\n",
       "    -0.026843419298529625,\n",
       "    0.010540333576500416,\n",
       "    0.045067984610795975,\n",
       "    0.003109560813754797,\n",
       "    0.011042953468859196,\n",
       "    -0.07115644216537476,\n",
       "    -0.028637006878852844,\n",
       "    -0.015206752344965935,\n",
       "    0.050512779504060745,\n",
       "    -0.020359938964247704,\n",
       "    -0.00747396145015955,\n",
       "    -0.007474882062524557,\n",
       "    -0.005225108936429024,\n",
       "    -0.012213820591568947,\n",
       "    -0.016705961897969246,\n",
       "    -0.04215404391288757,\n",
       "    -0.017395291477441788,\n",
       "    0.011769152246415615,\n",
       "    -0.0037994615267962217,\n",
       "    -0.025702424347400665,\n",
       "    -0.06845331192016602,\n",
       "    0.020313778892159462,\n",
       "    0.00036658684257417917,\n",
       "    -0.02452375553548336,\n",
       "    0.029715580865740776,\n",
       "    0.006583073176443577,\n",
       "    0.0611429326236248,\n",
       "    0.051164012402296066,\n",
       "    -0.0035399645566940308,\n",
       "    0.012590887024998665,\n",
       "    0.014657041057944298,\n",
       "    0.015573440119624138,\n",
       "    0.014325344935059547,\n",
       "    0.02745431661605835,\n",
       "    0.02197115682065487,\n",
       "    0.03292655944824219,\n",
       "    0.024111110717058182,\n",
       "    -0.06751292198896408,\n",
       "    -0.05045504868030548,\n",
       "    0.06353924423456192,\n",
       "    0.00643673911690712,\n",
       "    0.010098842903971672,\n",
       "    -0.03668275475502014,\n",
       "    -0.018185745924711227,\n",
       "    0.0002117723342962563,\n",
       "    0.009329434484243393,\n",
       "    -0.008158360607922077,\n",
       "    -0.021570837125182152,\n",
       "    0.019329393282532692,\n",
       "    0.008125909604132175,\n",
       "    0.015164189040660858,\n",
       "    0.05726585164666176,\n",
       "    -0.009541784413158894,\n",
       "    0.0061468868516385555,\n",
       "    -0.004457440227270126,\n",
       "    0.023716401308774948,\n",
       "    -0.06971192359924316,\n",
       "    0.028796060010790825,\n",
       "    0.01041675079613924,\n",
       "    -0.04553953930735588,\n",
       "    0.020738620311021805,\n",
       "    0.004832092206925154,\n",
       "    -0.03150299936532974,\n",
       "    -0.04429600015282631,\n",
       "    0.013790628872811794,\n",
       "    -0.053115058690309525,\n",
       "    -0.02560560405254364,\n",
       "    0.036688730120658875,\n",
       "    0.014488043263554573,\n",
       "    -0.009228067472577095,\n",
       "    -0.06419932097196579,\n",
       "    -0.012880503199994564,\n",
       "    -0.02107907459139824,\n",
       "    1.444513054593699e-05,\n",
       "    -0.0033198660239577293,\n",
       "    -0.024905933067202568,\n",
       "    -0.03108673356473446,\n",
       "    0.033358171582221985,\n",
       "    0.0009742958354763687,\n",
       "    -0.03336518630385399,\n",
       "    0.027100173756480217,\n",
       "    -0.009219962172210217,\n",
       "    -0.03113715350627899,\n",
       "    0.04466729238629341,\n",
       "    0.008119719102978706,\n",
       "    -0.0565766766667366,\n",
       "    0.0037011869717389345,\n",
       "    -0.012338224798440933,\n",
       "    -0.03370840102434158,\n",
       "    -0.0005041065742261708,\n",
       "    0.011166509240865707,\n",
       "    0.009698199108242989,\n",
       "    -0.0012518489966169,\n",
       "    0.029880018904805183,\n",
       "    -0.050972066819667816,\n",
       "    0.07752571254968643,\n",
       "    0.0021770054008811712,\n",
       "    0.10417456179857254,\n",
       "    -0.0009557668818160892,\n",
       "    0.0067534553818404675,\n",
       "    0.00782690104097128,\n",
       "    0.02586180530488491,\n",
       "    0.045426320284605026,\n",
       "    0.06360748410224915,\n",
       "    -0.03204941377043724,\n",
       "    0.002880049403756857,\n",
       "    0.02095639519393444,\n",
       "    -0.03027138113975525,\n",
       "    0.001833139336667955,\n",
       "    -0.02031288668513298,\n",
       "    0.03506040945649147,\n",
       "    0.05600018426775932,\n",
       "    0.03160380944609642,\n",
       "    -0.05123694986104965,\n",
       "    -0.0626993402838707,\n",
       "    0.01658344827592373,\n",
       "    -0.03829720988869667,\n",
       "    0.058923933655023575,\n",
       "    -0.02719443291425705,\n",
       "    -0.037641558796167374,\n",
       "    0.02953055500984192,\n",
       "    0.0038646492175757885,\n",
       "    -0.004005240276455879,\n",
       "    -0.09462728351354599,\n",
       "    -0.0652087852358818,\n",
       "    0.025428099557757378,\n",
       "    0.051540397107601166,\n",
       "    -0.008449424058198929,\n",
       "    0.02641494944691658,\n",
       "    -0.01028856448829174,\n",
       "    -0.0008263083873316646,\n",
       "    -0.010583778843283653,\n",
       "    0.006414654664695263,\n",
       "    -0.005061961244791746,\n",
       "    -0.04029203578829765,\n",
       "    -0.061678167432546616,\n",
       "    -0.005267309490591288,\n",
       "    -0.011394286528229713,\n",
       "    -0.02568669244647026,\n",
       "    -0.006238511763513088,\n",
       "    -0.030004238709807396,\n",
       "    -0.011689789593219757,\n",
       "    -0.010035756044089794,\n",
       "    -0.012819045223295689,\n",
       "    -0.027572203427553177,\n",
       "    0.047848884016275406,\n",
       "    -0.10166989266872406,\n",
       "    0.011310838162899017,\n",
       "    0.05754116177558899,\n",
       "    -0.02797839604318142,\n",
       "    -0.007163635920733213,\n",
       "    -0.0045363702811300755,\n",
       "    0.001722582383081317,\n",
       "    -0.026574403047561646,\n",
       "    0.03393394127488136,\n",
       "    0.030201716348528862,\n",
       "    0.05449223145842552,\n",
       "    -0.0014090642798691988,\n",
       "    0.0484597384929657,\n",
       "    -0.014405087567865849,\n",
       "    0.0635632649064064,\n",
       "    0.02740427292883396,\n",
       "    -0.032986439764499664,\n",
       "    -0.00026623960002325475,\n",
       "    0.06325620412826538,\n",
       "    -0.0749872699379921,\n",
       "    0.011908404529094696,\n",
       "    0.05931319668889046,\n",
       "    -0.009721901267766953,\n",
       "    -0.0029848364647477865,\n",
       "    0.04216653108596802,\n",
       "    0.03902079537510872,\n",
       "    -0.0011361106298863888,\n",
       "    -0.047263335436582565,\n",
       "    0.0013711013598367572,\n",
       "    -0.002253495389595628,\n",
       "    -0.06365787982940674,\n",
       "    0.03780275955796242,\n",
       "    0.09864509105682373,\n",
       "    -0.0921313688158989,\n",
       "    0.0361601896584034,\n",
       "    0.0005696952575817704,\n",
       "    0.035040974617004395,\n",
       "    -0.07377096265554428,\n",
       "    -0.0032924015540629625,\n",
       "    -0.016228212043642998,\n",
       "    -0.05882954224944115,\n",
       "    0.07859773933887482,\n",
       "    0.007806798908859491,\n",
       "    -0.08102353662252426,\n",
       "    -0.06597550958395004,\n",
       "    -0.05070755258202553,\n",
       "    -0.04055945947766304,\n",
       "    -0.02871568500995636,\n",
       "    0.012567104771733284,\n",
       "    -0.007313437294214964,\n",
       "    -0.007488269824534655,\n",
       "    0.023991720750927925,\n",
       "    0.008935095742344856,\n",
       "    -0.01046465802937746,\n",
       "    0.0021425914019346237,\n",
       "    0.007641823962330818,\n",
       "    0.009950722567737103,\n",
       "    -0.010274703614413738,\n",
       "    0.017408626154065132,\n",
       "    0.014523457735776901,\n",
       "    -0.06436450779438019,\n",
       "    0.1004868596792221,\n",
       "    -0.000903633888810873,\n",
       "    0.04568679630756378,\n",
       "    0.04622214660048485,\n",
       "    0.007708179764449596,\n",
       "    -0.008373342454433441,\n",
       "    -0.03050040826201439,\n",
       "    -0.020414335653185844,\n",
       "    -0.044397175312042236,\n",
       "    0.02221738174557686,\n",
       "    -0.032325614243745804,\n",
       "    0.014936499297618866,\n",
       "    0.08872969448566437,\n",
       "    0.04268105700612068,\n",
       "    -0.046038590371608734,\n",
       "    -0.03106744959950447,\n",
       "    -0.04355143383145332,\n",
       "    0.03068115934729576,\n",
       "    -0.02723471075296402,\n",
       "    0.030294595286250114,\n",
       "    -0.0006456805276684463,\n",
       "    -0.025102095678448677,\n",
       "    0.014653416350483894,\n",
       "    -0.055196356028318405,\n",
       "    0.030849432572722435,\n",
       "    -0.004917859565466642,\n",
       "    -0.005791716277599335,\n",
       "    0.015591038390994072,\n",
       "    -0.0031345393508672714,\n",
       "    0.05132317915558815,\n",
       "    0.04705221578478813,\n",
       "    -0.04342886805534363,\n",
       "    0.04300316795706749,\n",
       "    0.046058446168899536,\n",
       "    0.048040661960840225,\n",
       "    0.012944714166224003,\n",
       "    -0.011117047630250454,\n",
       "    -0.009213698096573353,\n",
       "    -0.05809669569134712,\n",
       "    0.021689631044864655,\n",
       "    0.006982988212257624,\n",
       "    0.06197545304894447,\n",
       "    0.02837219648063183,\n",
       "    -0.018559865653514862,\n",
       "    0.018082227557897568,\n",
       "    -0.08950725197792053,\n",
       "    -0.03263150900602341,\n",
       "    0.008956968784332275,\n",
       "    -0.014526150189340115,\n",
       "    -0.005484144669026136,\n",
       "    -0.015242813155055046,\n",
       "    0.06690023094415665,\n",
       "    0.018335523083806038,\n",
       "    0.02857264317572117,\n",
       "    -0.019645391032099724,\n",
       "    -0.04296684265136719,\n",
       "    -0.021879279986023903,\n",
       "    -0.00605639535933733,\n",
       "    -0.02949342504143715,\n",
       "    0.0328107587993145,\n",
       "    -0.017099875956773758,\n",
       "    -0.014169501140713692,\n",
       "    -0.016771145164966583,\n",
       "    -0.02702675573527813,\n",
       "    -0.007903637364506721,\n",
       "    -0.0012131552211940289,\n",
       "    -0.01654183119535446,\n",
       "    -0.07500816881656647,\n",
       "    0.012747488915920258,\n",
       "    -0.07625360041856766,\n",
       "    -0.02109275385737419,\n",
       "    0.059736207127571106,\n",
       "    0.017681820318102837,\n",
       "    0.023135708644986153,\n",
       "    -0.0527171790599823,\n",
       "    -0.0079732621088624,\n",
       "    0.04027407988905907,\n",
       "    0.06080561876296997,\n",
       "    -0.00024582273908890784,\n",
       "    -0.015182348899543285,\n",
       "    -0.03382405638694763,\n",
       "    0.01959913596510887,\n",
       "    0.06458045542240143,\n",
       "    0.031074633821845055,\n",
       "    -0.027559731155633926,\n",
       "    -0.01902710273861885,\n",
       "    0.02676854096353054,\n",
       "    -0.06168573349714279,\n",
       "    0.06287561357021332,\n",
       "    0.03229837864637375,\n",
       "    0.0267706960439682,\n",
       "    -0.014071136713027954,\n",
       "    -0.04835142567753792,\n",
       "    -0.0002685262297745794,\n",
       "    0.014674643985927105,\n",
       "    0.0429278202354908,\n",
       "    0.009557894431054592,\n",
       "    0.01411944068968296,\n",
       "    0.040344785898923874,\n",
       "    -0.017512237653136253,\n",
       "    0.0697597861289978,\n",
       "    -0.01871246099472046,\n",
       "    -0.01187979243695736,\n",
       "    -0.0334937609732151,\n",
       "    -0.06489583849906921,\n",
       "    -0.005144050344824791,\n",
       "    -0.0031802502926439047,\n",
       "    0.015644492581486702,\n",
       "    -0.05590880662202835,\n",
       "    -0.0012232032604515553,\n",
       "    0.019098447635769844,\n",
       "    0.027245230972766876,\n",
       "    -0.01354304701089859,\n",
       "    0.008252816274762154,\n",
       "    -0.005761743523180485,\n",
       "    0.034632325172424316,\n",
       "    0.004386072047054768,\n",
       "    0.03024035505950451,\n",
       "    -0.03132651746273041,\n",
       "    -0.0013931026915088296,\n",
       "    0.025102458894252777,\n",
       "    -0.044591765850782394,\n",
       "    0.022047078236937523,\n",
       "    0.01993469148874283,\n",
       "    -0.027287648990750313,\n",
       "    0.01605903171002865,\n",
       "    -0.027557380497455597,\n",
       "    0.07241769134998322,\n",
       "    0.07506473362445831,\n",
       "    0.021350864320993423,\n",
       "    -0.022647012025117874,\n",
       "    -0.0016555121401324868,\n",
       "    -0.017964234575629234,\n",
       "    0.0019138501957058907,\n",
       "    -0.04209174960851669,\n",
       "    0.03402373939752579,\n",
       "    -0.04082201421260834,\n",
       "    0.022594287991523743,\n",
       "    0.05462169647216797,\n",
       "    0.030491342768073082,\n",
       "    0.02596099115908146,\n",
       "    -0.04354478418827057,\n",
       "    -0.010473662056028843,\n",
       "    -0.005429483018815517,\n",
       "    0.003918552305549383,\n",
       "    0.052850812673568726,\n",
       "    -5.303304844530153e-33,\n",
       "    0.01702840067446232,\n",
       "    -0.056560955941677094,\n",
       "    0.04573400691151619,\n",
       "    0.06508587300777435,\n",
       "    -0.04285672307014465,\n",
       "    -0.05109775438904762,\n",
       "    0.050866223871707916,\n",
       "    0.04227638989686966,\n",
       "    -0.025974459946155548,\n",
       "    -0.01610616222023964,\n",
       "    -0.009718967601656914,\n",
       "    0.032903462648391724,\n",
       "    0.025739530101418495,\n",
       "    0.004229131154716015,\n",
       "    -0.011001212522387505,\n",
       "    0.05255405977368355,\n",
       "    -0.006634087767452002,\n",
       "    -0.005283951293677092,\n",
       "    0.018961679190397263,\n",
       "    -0.03759443014860153,\n",
       "    -0.12264592200517654,\n",
       "    0.03509067744016647,\n",
       "    -0.008380809798836708,\n",
       "    -0.05518479645252228,\n",
       "    0.010505560785531998,\n",
       "    -0.009315045550465584,\n",
       "    -0.00565839558839798,\n",
       "    -0.03348743915557861,\n",
       "    -0.03098953887820244,\n",
       "    0.020658018067479134,\n",
       "    0.01880752481520176,\n",
       "    0.00940119381994009,\n",
       "    0.008798932656645775,\n",
       "    -0.06605726480484009,\n",
       "    -0.004690034314990044,\n",
       "    -0.017345082014799118,\n",
       "    -0.04276911914348602,\n",
       "    0.012706020846962929,\n",
       "    0.06526324898004532,\n",
       "    0.056300584226846695,\n",
       "    0.01834196411073208,\n",
       "    -0.05255022272467613,\n",
       "    0.010545743629336357,\n",
       "    -0.018409432843327522,\n",
       "    -0.020507916808128357,\n",
       "    -0.0015346669824793935,\n",
       "    0.050874363631010056,\n",
       "    -0.03360476344823837,\n",
       "    0.004133853130042553,\n",
       "    0.011227321811020374,\n",
       "    0.021602688357234,\n",
       "    0.017476549372076988,\n",
       "    -0.08376026898622513,\n",
       "    0.03431857377290726,\n",
       "    0.05031701177358627,\n",
       "    0.04750784859061241,\n",
       "    -0.024411320686340332,\n",
       "    -0.004706140607595444,\n",
       "    0.01233531255275011,\n",
       "    0.0298208799213171,\n",
       "    -0.035119667649269104,\n",
       "    -0.01426283735781908,\n",
       "    0.031402356922626495,\n",
       "    0.000615061610005796,\n",
       "    0.03349127620458603,\n",
       "    0.013317774049937725,\n",
       "    0.0928422063589096,\n",
       "    -0.01126711443066597,\n",
       "    0.02454427443444729,\n",
       "    0.01920291595160961,\n",
       "    -0.04626567289233208,\n",
       "    0.018365828320384026,\n",
       "    -0.02355961687862873,\n",
       "    0.0051731448620557785,\n",
       "    -0.039166178554296494,\n",
       "    -0.014305955730378628,\n",
       "    0.0003413552767597139,\n",
       "    0.013759766705334187,\n",
       "    0.05689927563071251,\n",
       "    -0.05660371482372284,\n",
       "    -0.03517749905586243,\n",
       "    0.01338745467364788,\n",
       "    0.014829626306891441,\n",
       "    -0.023855460807681084,\n",
       "    -0.03497660532593727,\n",
       "    -0.05942613631486893,\n",
       "    -0.00938485935330391,\n",
       "    0.021183079108595848,\n",
       "    -0.0039022641722112894,\n",
       "    -0.07242384552955627,\n",
       "    -0.004741104785352945,\n",
       "    0.07137104868888855,\n",
       "    -0.01994631066918373,\n",
       "    -0.02913278341293335,\n",
       "    0.00778693612664938,\n",
       "    -0.07083778083324432,\n",
       "    0.008682136423885822,\n",
       "    -0.0014370795106515288,\n",
       "    0.018057657405734062,\n",
       "    0.04102323576807976,\n",
       "    -0.010502127930521965,\n",
       "    -0.01651713252067566,\n",
       "    -0.0453028678894043,\n",
       "    0.0013727961340919137,\n",
       "    0.04811263084411621,\n",
       "    0.03806470334529877,\n",
       "    -0.03737318143248558,\n",
       "    0.007716530002653599,\n",
       "    0.010669378563761711,\n",
       "    -0.008015112951397896,\n",
       "    0.00980397593230009,\n",
       "    -0.005007595289498568,\n",
       "    -0.014020075090229511,\n",
       "    0.06020569056272507,\n",
       "    -0.02610359527170658,\n",
       "    0.012772062793374062,\n",
       "    0.020639780908823013,\n",
       "    0.025241930037736893,\n",
       "    -0.03505292162299156,\n",
       "    -0.028304923325777054,\n",
       "    0.020991824567317963,\n",
       "    -0.07555881887674332,\n",
       "    -0.027660232037305832,\n",
       "    -0.025702092796564102,\n",
       "    -0.017589827999472618,\n",
       "    -0.006780896335840225,\n",
       "    -0.03141704201698303,\n",
       "    0.01874798722565174,\n",
       "    0.009514318779110909,\n",
       "    -0.0060359216295182705,\n",
       "    -0.032264988869428635,\n",
       "    0.04182039201259613,\n",
       "    2.681613580080011e-07,\n",
       "    -0.014299983158707619,\n",
       "    0.002939284546300769,\n",
       "    0.004445292521268129,\n",
       "    0.03197871521115303,\n",
       "    0.04121676832437515,\n",
       "    -0.025570368394255638,\n",
       "    -0.019838375970721245,\n",
       "    -0.010622522793710232,\n",
       "    -0.00897264014929533,\n",
       "    -0.00025921911583282053,\n",
       "    -0.0011481561232358217,\n",
       "    0.020900851115584373,\n",
       "    0.017441032454371452,\n",
       "    -0.024796366691589355,\n",
       "    -0.05471300706267357,\n",
       "    -0.08296633511781693,\n",
       "    -0.00997112225741148,\n",
       "    -0.033534422516822815,\n",
       "    -0.035346031188964844,\n",
       "    0.05051207169890404,\n",
       "    0.010406292043626308,\n",
       "    0.06468521803617477,\n",
       "    0.01503473799675703,\n",
       "    0.020156454294919968,\n",
       "    -0.013537248596549034,\n",
       "    -0.008642966859042645,\n",
       "    -0.050673943012952805,\n",
       "    -0.01924692466855049,\n",
       "    0.030434882268309593,\n",
       "    0.03130599856376648,\n",
       "    0.056672900915145874,\n",
       "    -0.01006452925503254,\n",
       "    0.029682589694857597,\n",
       "    0.016981903463602066,\n",
       "    0.00627155089750886,\n",
       "    -0.012158429250121117,\n",
       "    0.02561526745557785,\n",
       "    0.0695880874991417,\n",
       "    0.0034745077136904,\n",
       "    0.01579027622938156,\n",
       "    0.04564111307263374,\n",
       "    0.03411541134119034,\n",
       "    -0.01547867339104414,\n",
       "    -0.05036322399973869,\n",
       "    0.07243656367063522,\n",
       "    0.041025299578905106,\n",
       "    -0.05858024209737778,\n",
       "    -0.07631809264421463,\n",
       "    -0.012783370912075043,\n",
       "    -0.01840232126414776,\n",
       "    0.008342069573700428,\n",
       "    -0.017893338575959206,\n",
       "    0.055248767137527466,\n",
       "    0.04840861260890961,\n",
       "    0.002504949923604727,\n",
       "    -0.05759318172931671,\n",
       "    -0.02218037098646164,\n",
       "    0.0234922394156456,\n",
       "    0.005701913498342037,\n",
       "    -0.04539813846349716,\n",
       "    -0.022554881870746613,\n",
       "    -0.03350740298628807,\n",
       "    0.02447611652314663,\n",
       "    -0.040239367634058,\n",
       "    0.09340112656354904,\n",
       "    0.011206032708287239,\n",
       "    -0.0007736246334388852,\n",
       "    1.7423631853593927e-34,\n",
       "    0.006638782098889351,\n",
       "    0.0063453661277890205,\n",
       "    -0.01829923689365387,\n",
       "    0.049107443541288376,\n",
       "    0.03721340000629425,\n",
       "    -0.06219286471605301,\n",
       "    -0.03059951402246952,\n",
       "    -0.013291475363075733,\n",
       "    -0.006363479886204004,\n",
       "    0.06046830117702484,\n",
       "    -0.03094950132071972],\n",
       "   'chapter': 'CHAPTER 3',\n",
       "   'title': 'Technical Interview: Machine Learning Algorithms'}},\n",
       " {'_index': 'ml-interview-questions',\n",
       "  '_id': '3bez-ZEBwj93mx26upk6',\n",
       "  '_score': 1.9621209,\n",
       "  '_source': {'section': 'Overview of the Machine Learning Algorithms Technical Interview',\n",
       "   'chapter': 'CHAPTER 3',\n",
       "   'title': 'Technical Interview: Machine Learning Algorithms'}},\n",
       " {'_index': 'ml-interview-questions',\n",
       "  '_id': '3rez-ZEBwj93mx26uplV',\n",
       "  '_score': 1.9621209,\n",
       "  '_source': {'text': 'You’re likely to be asked ML algorithm technical questions in an interview if you’re applying for any of the following jobs: • Data scientist who builds ML models • Machine learning engineer • Applied scientist • And similar roles Recall that within the common ML job titles ( Figure 1-8 ), there are some jobs that have the responsibility of training ML models in the ML lifecycle. This chapter focu‐ ses on assessing candidates for those skills; if the job you’re aiming for focuses less on training ML models, you might get a simplified version of this type of interview, or it might be skipped completely. This interview is meant to assess your understanding of ML algorithms, especially on the theoretical side. As to how you implement the algorithms with code, I cover that in the model deployment questions in Chapter 6 and the coding/programming tech‐ nical interview in Chapter 5 . The goal for you as an interviewee is for the interviewers to confirm that you understand the underlying concepts behind ML algorithms. Roles do exist where all you have to know is how to import the library with Python, but for more advanced projects, an underlying understanding can help you custom‐ ize various ML approaches and better debug and troubleshoot models. As covered in Chapter 1 , in the three pillars of ML roles, this is the pillar of ML algorithm and data intuition, which showcases your ability to adapt (refer to Figure 1-6 ). This skill is especially important in companies that have complex ML use cases and custom-made solutions, where you might modify or combine various off-the-shelf methods or cre‐ ate something from scratch. I try to mention as many common algorithms as space allows, but there are many more techniques under the sun. Be sure to check out the linked resources to extend your learning and interview preparation! It is also important to note that, in addition to understanding the ML algorithms’ inner workings and underlying statistical methods, you need to successfully commu‐ nicate that understanding to the interviewer. Yes, I know that communication skills have been brought up many times in this book, but they are what help set you apart as a successful candidate. As a rule of thumb, it’s important to be able to explain algorithms and ML concepts at two levels: on a simple “explain like I’m five years old” level and at a deeper, tech‐ nical level, one more appropriate for a college course. A second rule of thumb is to be prepared to answer follow-up questions to these ML algorithm interview questions. This is so the interviewer knows that you didn’t just memorize and then regurgitate the answer, but that you can apply it to various real-life scenarios on the job. In this chapter, I break down technical questions on the following topics so you can easily refer to a specific question if your interview focuses on that topic: • Statistical techniques • Supervised, unsupervised, and reinforcement learning • Natural language processing (NLP) • Recommender systems • Reinforcement learning • Computer vision In technical interviews that are very structured, such as the Ama‐ zon data science initial phone screen, they will ask you clearly scoped questions, such as asking for a definition of a particular algorithm. After you answer, they will generally move on without additional follow-up questions. There are companies that mix structured questions with a free-form discussion, where the inter‐ viewer might dig deeper into your answer, and the conversation might branch out from there into your past experiences.',\n",
       "   'text_vector': [0.05993577092885971,\n",
       "    -0.05391945317387581,\n",
       "    -0.04208781570196152,\n",
       "    0.017544545233249664,\n",
       "    -0.002861324232071638,\n",
       "    0.041564736515283585,\n",
       "    -0.04502599686384201,\n",
       "    -0.03485346958041191,\n",
       "    0.02959667146205902,\n",
       "    -0.04890763759613037,\n",
       "    0.04734601825475693,\n",
       "    0.04816044867038727,\n",
       "    0.020772868767380714,\n",
       "    0.059839390218257904,\n",
       "    0.02621373161673546,\n",
       "    -0.03506791219115257,\n",
       "    0.0030805773567408323,\n",
       "    -0.008104348555207253,\n",
       "    -0.02095099724829197,\n",
       "    -0.005705469287931919,\n",
       "    -0.018839184194803238,\n",
       "    0.0153201250359416,\n",
       "    0.010476820170879364,\n",
       "    -0.007886525243520737,\n",
       "    -0.0054871258325874805,\n",
       "    -0.03927190974354744,\n",
       "    -0.05143311992287636,\n",
       "    0.028323808684945107,\n",
       "    -0.027788985520601273,\n",
       "    0.020367588847875595,\n",
       "    -0.030829526484012604,\n",
       "    -0.032792653888463974,\n",
       "    0.0036351655144244432,\n",
       "    -0.014580332674086094,\n",
       "    2.306577471244964e-06,\n",
       "    -0.018277574330568314,\n",
       "    -0.0208209827542305,\n",
       "    0.03600199148058891,\n",
       "    0.007936419919133186,\n",
       "    -0.10433556139469147,\n",
       "    0.039740514010190964,\n",
       "    -0.01752186007797718,\n",
       "    0.05168933421373367,\n",
       "    0.015967221930623055,\n",
       "    -0.04609043896198273,\n",
       "    0.043641407042741776,\n",
       "    0.07432569563388824,\n",
       "    -0.017735924571752548,\n",
       "    0.05891294777393341,\n",
       "    0.041267551481723785,\n",
       "    0.016758892685174942,\n",
       "    -0.0710165724158287,\n",
       "    0.010138644836843014,\n",
       "    0.10137741267681122,\n",
       "    -0.03521575778722763,\n",
       "    0.016337063163518906,\n",
       "    0.02317001298069954,\n",
       "    0.05400419235229492,\n",
       "    0.010251354426145554,\n",
       "    -0.07366803288459778,\n",
       "    0.022489724680781364,\n",
       "    0.009480347856879234,\n",
       "    0.04316667094826698,\n",
       "    0.036297887563705444,\n",
       "    0.07358944416046143,\n",
       "    -0.0030124688055366278,\n",
       "    -0.01601889170706272,\n",
       "    -0.0353853739798069,\n",
       "    -0.04866044595837593,\n",
       "    -3.4659540233406005e-06,\n",
       "    0.030104078352451324,\n",
       "    -0.07502267509698868,\n",
       "    -0.010263746604323387,\n",
       "    0.0007292333757504821,\n",
       "    0.045224033296108246,\n",
       "    -0.02872268110513687,\n",
       "    -0.025965502485632896,\n",
       "    -0.0164458230137825,\n",
       "    0.027926500886678696,\n",
       "    -0.030829666182398796,\n",
       "    -0.01192169077694416,\n",
       "    -0.007750106509774923,\n",
       "    -0.005981812719255686,\n",
       "    -0.0013405110221356153,\n",
       "    -0.015932733193039894,\n",
       "    0.07468276470899582,\n",
       "    -0.0268633421510458,\n",
       "    -0.02454020082950592,\n",
       "    0.017150944098830223,\n",
       "    -0.017291303724050522,\n",
       "    0.08549156785011292,\n",
       "    0.009127943776547909,\n",
       "    0.053818706423044205,\n",
       "    0.0978497713804245,\n",
       "    -0.030410291627049446,\n",
       "    0.016753489151597023,\n",
       "    0.008483332581818104,\n",
       "    0.007069938350468874,\n",
       "    -0.04653380811214447,\n",
       "    -0.04721872881054878,\n",
       "    -0.019372476264834404,\n",
       "    -0.012400536797940731,\n",
       "    0.034717459231615067,\n",
       "    -0.0043710023164749146,\n",
       "    0.014921296387910843,\n",
       "    0.0026266395580023527,\n",
       "    -0.011007481254637241,\n",
       "    0.012630293145775795,\n",
       "    -0.03945392370223999,\n",
       "    -0.053298793733119965,\n",
       "    -0.02690868265926838,\n",
       "    -0.02548239938914776,\n",
       "    -0.011483878828585148,\n",
       "    0.041013166308403015,\n",
       "    0.009593886323273182,\n",
       "    0.05729550123214722,\n",
       "    0.020113874226808548,\n",
       "    0.04357387498021126,\n",
       "    -0.002971044508740306,\n",
       "    -0.01951770856976509,\n",
       "    -0.04815621301531792,\n",
       "    -0.008002993650734425,\n",
       "    0.06028402969241142,\n",
       "    0.00944421999156475,\n",
       "    -0.02250523306429386,\n",
       "    -0.1133100688457489,\n",
       "    -0.0030280649662017822,\n",
       "    -0.016901895403862,\n",
       "    -0.004208337981253862,\n",
       "    -0.0201493501663208,\n",
       "    0.05163604021072388,\n",
       "    0.0307147316634655,\n",
       "    -0.01903790980577469,\n",
       "    -0.003076596651226282,\n",
       "    0.056388143450021744,\n",
       "    0.018235811963677406,\n",
       "    0.009810620918869972,\n",
       "    0.05104219168424606,\n",
       "    -0.06376949697732925,\n",
       "    -0.02144075185060501,\n",
       "    0.028959348797798157,\n",
       "    -0.0544670969247818,\n",
       "    0.023991452530026436,\n",
       "    -0.03641391918063164,\n",
       "    0.015026185661554337,\n",
       "    0.010897352360188961,\n",
       "    -0.03244491294026375,\n",
       "    -0.01687229797244072,\n",
       "    -0.0002178361901314929,\n",
       "    0.011289150454103947,\n",
       "    -0.06343404203653336,\n",
       "    0.016163624823093414,\n",
       "    0.01032592635601759,\n",
       "    0.029909729957580566,\n",
       "    0.03334677219390869,\n",
       "    -0.0011833147145807743,\n",
       "    -0.015152734704315662,\n",
       "    0.0714680552482605,\n",
       "    0.017896126955747604,\n",
       "    0.015749327838420868,\n",
       "    0.0026956640649586916,\n",
       "    -0.08691760897636414,\n",
       "    0.028267212212085724,\n",
       "    0.019728438928723335,\n",
       "    -0.008056264370679855,\n",
       "    -0.052956853061914444,\n",
       "    -0.013408202677965164,\n",
       "    0.07618243247270584,\n",
       "    -0.029021495953202248,\n",
       "    0.029180333018302917,\n",
       "    0.00249526952393353,\n",
       "    0.00178820441942662,\n",
       "    -0.008166523650288582,\n",
       "    -0.0206541009247303,\n",
       "    -0.021456651389598846,\n",
       "    0.02576206438243389,\n",
       "    0.01771801896393299,\n",
       "    0.011412512511014938,\n",
       "    0.01657455414533615,\n",
       "    0.03267432749271393,\n",
       "    0.010272804647684097,\n",
       "    0.0319068543612957,\n",
       "    -0.008611908182501793,\n",
       "    0.04399814084172249,\n",
       "    -0.029559556394815445,\n",
       "    -0.030444510281085968,\n",
       "    -0.07995172590017319,\n",
       "    0.05623342841863632,\n",
       "    -0.003955999854952097,\n",
       "    -0.02993065118789673,\n",
       "    0.007692529819905758,\n",
       "    0.006042165216058493,\n",
       "    0.029032841324806213,\n",
       "    0.03565121442079544,\n",
       "    -0.016098299995064735,\n",
       "    -0.022455161437392235,\n",
       "    -0.04448382183909416,\n",
       "    -0.01865617372095585,\n",
       "    -0.06115516275167465,\n",
       "    0.0010106650879606605,\n",
       "    -0.06868947297334671,\n",
       "    -0.0458696112036705,\n",
       "    -0.05172445625066757,\n",
       "    0.015783187001943588,\n",
       "    0.05050439387559891,\n",
       "    -0.009645971469581127,\n",
       "    0.001623976626433432,\n",
       "    -0.0382707417011261,\n",
       "    -0.033755600452423096,\n",
       "    -0.04780231788754463,\n",
       "    0.02130255103111267,\n",
       "    -0.03061540797352791,\n",
       "    -0.06319183111190796,\n",
       "    0.0010918228654190898,\n",
       "    -0.0012680012732744217,\n",
       "    -0.007369408383965492,\n",
       "    -0.012422468513250351,\n",
       "    -0.06171559914946556,\n",
       "    0.02558796852827072,\n",
       "    -0.007650704123079777,\n",
       "    -0.011241782456636429,\n",
       "    -0.00856319535523653,\n",
       "    -0.05983719229698181,\n",
       "    0.02328561805188656,\n",
       "    -0.017166275531053543,\n",
       "    0.008013581857085228,\n",
       "    0.00467833923175931,\n",
       "    -0.005270530469715595,\n",
       "    0.04493091627955437,\n",
       "    0.05974894389510155,\n",
       "    0.02762911096215248,\n",
       "    0.021947922185063362,\n",
       "    0.009479965083301067,\n",
       "    0.04670926183462143,\n",
       "    -0.005805241875350475,\n",
       "    -0.00601569889113307,\n",
       "    0.0005478634848259389,\n",
       "    0.03909299150109291,\n",
       "    0.0354221947491169,\n",
       "    -0.031512949615716934,\n",
       "    -0.05738512799143791,\n",
       "    0.052931029349565506,\n",
       "    -0.009274779818952084,\n",
       "    0.013796319253742695,\n",
       "    -0.034763891249895096,\n",
       "    -0.024308128282427788,\n",
       "    -0.015620075166225433,\n",
       "    0.003542307298630476,\n",
       "    -0.0044745649211108685,\n",
       "    -0.049325548112392426,\n",
       "    0.009378290735185146,\n",
       "    0.012563003227114677,\n",
       "    -0.01178928092122078,\n",
       "    0.049020953476428986,\n",
       "    -0.03744195029139519,\n",
       "    0.026297423988580704,\n",
       "    0.021261204034090042,\n",
       "    0.025703465566039085,\n",
       "    -0.027882926166057587,\n",
       "    -0.009657630696892738,\n",
       "    0.026188263669610023,\n",
       "    -0.030565543100237846,\n",
       "    0.03216014802455902,\n",
       "    0.018007267266511917,\n",
       "    -0.014501531608402729,\n",
       "    -0.04228790104389191,\n",
       "    0.03544946759939194,\n",
       "    -0.03698614239692688,\n",
       "    -0.03924060985445976,\n",
       "    0.012816576287150383,\n",
       "    0.014563743025064468,\n",
       "    0.023837367072701454,\n",
       "    -0.05846910551190376,\n",
       "    -0.012787357904016972,\n",
       "    -0.02741323783993721,\n",
       "    -0.020585916936397552,\n",
       "    -0.001209129812195897,\n",
       "    -0.05252275615930557,\n",
       "    -0.01353229396045208,\n",
       "    0.02691379189491272,\n",
       "    0.0011114790104329586,\n",
       "    -0.050979144871234894,\n",
       "    0.04675965756177902,\n",
       "    -0.0029980253893882036,\n",
       "    -0.061511848121881485,\n",
       "    0.035548027604818344,\n",
       "    0.002458383096382022,\n",
       "    -0.018027696758508682,\n",
       "    -0.026821397244930267,\n",
       "    4.712527879746631e-05,\n",
       "    -0.01787043921649456,\n",
       "    -0.005689395125955343,\n",
       "    0.017084216699004173,\n",
       "    0.013610954396426678,\n",
       "    -0.0001286293991142884,\n",
       "    0.033829305320978165,\n",
       "    -0.03355007246136665,\n",
       "    0.0557778924703598,\n",
       "    -0.029652204364538193,\n",
       "    0.02846684120595455,\n",
       "    0.027290843427181244,\n",
       "    -0.005436788313090801,\n",
       "    0.019736172631382942,\n",
       "    0.032048944383859634,\n",
       "    0.04577649012207985,\n",
       "    0.05548853799700737,\n",
       "    -0.018594011664390564,\n",
       "    0.008174204267561436,\n",
       "    0.007912821136415005,\n",
       "    -0.008682255633175373,\n",
       "    -0.006157692521810532,\n",
       "    -0.03717422857880592,\n",
       "    0.024356331676244736,\n",
       "    0.06748518347740173,\n",
       "    0.04238046333193779,\n",
       "    -0.054608304053545,\n",
       "    -0.03292936831712723,\n",
       "    0.01873568817973137,\n",
       "    -0.014274615794420242,\n",
       "    0.07546140998601913,\n",
       "    0.004782046657055616,\n",
       "    -0.04863451421260834,\n",
       "    0.047969575971364975,\n",
       "    0.00390136637724936,\n",
       "    -0.03183310851454735,\n",
       "    -0.08540407568216324,\n",
       "    -0.053729984909296036,\n",
       "    0.003996640909463167,\n",
       "    0.05064919590950012,\n",
       "    0.001986056799069047,\n",
       "    0.02963203564286232,\n",
       "    0.012075965292751789,\n",
       "    0.008922475390136242,\n",
       "    -0.025153566151857376,\n",
       "    0.008491539396345615,\n",
       "    -0.02616213448345661,\n",
       "    -0.037500910460948944,\n",
       "    -0.03089454397559166,\n",
       "    0.0030680010095238686,\n",
       "    -0.0204415712505579,\n",
       "    -0.00427921861410141,\n",
       "    0.009458516724407673,\n",
       "    -0.04305867850780487,\n",
       "    0.004741364624351263,\n",
       "    -0.006667343899607658,\n",
       "    0.03224755451083183,\n",
       "    -0.04373624175786972,\n",
       "    0.026834916323423386,\n",
       "    -0.014308436773717403,\n",
       "    0.005499574821442366,\n",
       "    0.04331716522574425,\n",
       "    -0.05446040257811546,\n",
       "    0.013915828429162502,\n",
       "    -0.0008997956174425781,\n",
       "    0.020016750320792198,\n",
       "    -0.02286701090633869,\n",
       "    0.025071611627936363,\n",
       "    0.026579203084111214,\n",
       "    0.041532211005687714,\n",
       "    0.09337524324655533,\n",
       "    0.05580047518014908,\n",
       "    -0.012136666104197502,\n",
       "    0.025789368897676468,\n",
       "    -0.005617617163807154,\n",
       "    -0.09415501356124878,\n",
       "    0.007408094592392445,\n",
       "    0.034514810889959335,\n",
       "    -0.08612942695617676,\n",
       "    0.04577364772558212,\n",
       "    0.064839668571949,\n",
       "    -0.015150727704167366,\n",
       "    -0.02587071992456913,\n",
       "    0.042271438986063004,\n",
       "    -0.009953559376299381,\n",
       "    0.02992803044617176,\n",
       "    -0.0036049233749508858,\n",
       "    -0.015037626028060913,\n",
       "    -0.001692193211056292,\n",
       "    -0.04068995267152786,\n",
       "    0.03311828896403313,\n",
       "    0.11041078716516495,\n",
       "    -0.09891027957201004,\n",
       "    -0.0038298945873975754,\n",
       "    -0.003567442763596773,\n",
       "    0.0008514173678122461,\n",
       "    -0.06327812373638153,\n",
       "    -0.0019446437945589423,\n",
       "    -0.013977233320474625,\n",
       "    -0.031722359359264374,\n",
       "    0.05255626514554024,\n",
       "    0.011341225355863571,\n",
       "    -0.018325481563806534,\n",
       "    -0.09305328130722046,\n",
       "    -0.04385515674948692,\n",
       "    -0.021065441891551018,\n",
       "    -0.03270413354039192,\n",
       "    0.028510747477412224,\n",
       "    0.03319522738456726,\n",
       "    0.033509768545627594,\n",
       "    0.010342761874198914,\n",
       "    -0.01136382482945919,\n",
       "    -0.04447039216756821,\n",
       "    -0.0010092954616993666,\n",
       "    0.01844889298081398,\n",
       "    0.0675230324268341,\n",
       "    -0.027409326285123825,\n",
       "    0.04122979938983917,\n",
       "    0.008118131197988987,\n",
       "    -0.036129795014858246,\n",
       "    0.05665532499551773,\n",
       "    -0.00499502569437027,\n",
       "    0.06812010705471039,\n",
       "    0.05219460651278496,\n",
       "    -0.010073339566588402,\n",
       "    -0.014728322625160217,\n",
       "    -0.0167916938662529,\n",
       "    -0.004374856129288673,\n",
       "    -0.03333187848329544,\n",
       "    0.03307511284947395,\n",
       "    -0.03677910566329956,\n",
       "    -0.005631446838378906,\n",
       "    0.052848074585199356,\n",
       "    0.05438699945807457,\n",
       "    -0.035300251096487045,\n",
       "    -0.03587113693356514,\n",
       "    -0.03251694515347481,\n",
       "    0.005911991931498051,\n",
       "    -0.005745507311075926,\n",
       "    0.0320640429854393,\n",
       "    0.01940460316836834,\n",
       "    0.0030746629927307367,\n",
       "    0.01862635277211666,\n",
       "    -0.02828310616314411,\n",
       "    0.015341688878834248,\n",
       "    -0.0021174391731619835,\n",
       "    -0.02391892857849598,\n",
       "    0.0062637547962367535,\n",
       "    0.019581161439418793,\n",
       "    0.05508286878466606,\n",
       "    0.03221283107995987,\n",
       "    -0.023399323225021362,\n",
       "    0.0410117469727993,\n",
       "    -0.04265822097659111,\n",
       "    0.029823128134012222,\n",
       "    -0.008005606010556221,\n",
       "    0.0273310374468565,\n",
       "    -0.036690112203359604,\n",
       "    -0.054626401513814926,\n",
       "    0.01336683239787817,\n",
       "    0.0017563584260642529,\n",
       "    0.044320836663246155,\n",
       "    0.06189727038145065,\n",
       "    -0.0518164187669754,\n",
       "    0.04654927924275398,\n",
       "    -0.06478781998157501,\n",
       "    -0.023027317598462105,\n",
       "    -0.02212967723608017,\n",
       "    -0.003679978894069791,\n",
       "    -0.003016661386936903,\n",
       "    -0.01085482258349657,\n",
       "    0.03977006673812866,\n",
       "    0.002247579861432314,\n",
       "    0.04523169621825218,\n",
       "    -0.043138716369867325,\n",
       "    -0.025353819131851196,\n",
       "    -0.02551446668803692,\n",
       "    -0.012033957056701183,\n",
       "    -0.03725782036781311,\n",
       "    -0.007928349077701569,\n",
       "    0.0004430134140420705,\n",
       "    -0.0026307604275643826,\n",
       "    -0.01326088048517704,\n",
       "    -0.03453647717833519,\n",
       "    0.0013453749706968665,\n",
       "    0.006465521175414324,\n",
       "    -0.018370239064097404,\n",
       "    -0.029575781896710396,\n",
       "    0.003943406045436859,\n",
       "    -0.026192499324679375,\n",
       "    0.009102510288357735,\n",
       "    0.07205945998430252,\n",
       "    -0.0023073353804647923,\n",
       "    -0.0059700473211705685,\n",
       "    -0.0841856449842453,\n",
       "    -0.017623718827962875,\n",
       "    0.022123027592897415,\n",
       "    0.08713827282190323,\n",
       "    -0.012247614562511444,\n",
       "    -0.03296934440732002,\n",
       "    -0.021746641024947166,\n",
       "    -0.007418270222842693,\n",
       "    0.04136619344353676,\n",
       "    0.06717617064714432,\n",
       "    -0.08875633776187897,\n",
       "    -0.05366329848766327,\n",
       "    -0.0558367520570755,\n",
       "    -0.003470067633315921,\n",
       "    0.0767635777592659,\n",
       "    0.014793012291193008,\n",
       "    0.027478082105517387,\n",
       "    -0.006460086442530155,\n",
       "    -0.041930969804525375,\n",
       "    -0.002409784821793437,\n",
       "    0.0004270104691386223,\n",
       "    0.03365336358547211,\n",
       "    -0.010113412514328957,\n",
       "    0.026774859055876732,\n",
       "    0.05500324070453644,\n",
       "    -0.01112450659275055,\n",
       "    0.04466130584478378,\n",
       "    -0.02262239158153534,\n",
       "    0.017057405784726143,\n",
       "    0.006867766845971346,\n",
       "    -0.03996741399168968,\n",
       "    -0.011655066162347794,\n",
       "    -0.018929189071059227,\n",
       "    0.01034036185592413,\n",
       "    -0.0038934543263167143,\n",
       "    -0.05759424716234207,\n",
       "    -0.011592062190175056,\n",
       "    0.024933474138379097,\n",
       "    0.010807954706251621,\n",
       "    -0.0069776782765984535,\n",
       "    -0.005839897785335779,\n",
       "    0.06343044340610504,\n",
       "    0.002327922498807311,\n",
       "    0.019114816561341286,\n",
       "    -0.049885787069797516,\n",
       "    -0.02309996262192726,\n",
       "    0.01848038285970688,\n",
       "    -0.019487766548991203,\n",
       "    0.009689359925687313,\n",
       "    0.00975723285228014,\n",
       "    -0.04209696128964424,\n",
       "    0.03587847203016281,\n",
       "    -0.005190533585846424,\n",
       "    0.08579230308532715,\n",
       "    0.04956401139497757,\n",
       "    -0.020616646856069565,\n",
       "    -0.020491894334554672,\n",
       "    -9.416192915523425e-05,\n",
       "    -0.05641939118504524,\n",
       "    -0.018611639738082886,\n",
       "    -0.036973364651203156,\n",
       "    0.03063562884926796,\n",
       "    -0.024205969646573067,\n",
       "    0.03661487251520157,\n",
       "    0.05673282593488693,\n",
       "    -0.004076502285897732,\n",
       "    0.0500049814581871,\n",
       "    -0.03081602044403553,\n",
       "    -0.018356995657086372,\n",
       "    0.014984437264502048,\n",
       "    -0.026441847905516624,\n",
       "    0.02377171255648136,\n",
       "    -5.549448953488524e-33,\n",
       "    0.01239333488047123,\n",
       "    -0.057521503418684006,\n",
       "    0.06717737019062042,\n",
       "    0.0680851861834526,\n",
       "    0.02260686829686165,\n",
       "    -0.05317200720310211,\n",
       "    0.042564067989587784,\n",
       "    0.042860809713602066,\n",
       "    -0.06277427077293396,\n",
       "    -0.01589544489979744,\n",
       "    0.02844943106174469,\n",
       "    0.01964864879846573,\n",
       "    0.0002799423527903855,\n",
       "    -0.01957213319838047,\n",
       "    -0.022903716191649437,\n",
       "    0.010073741897940636,\n",
       "    -0.047704312950372696,\n",
       "    -0.004744244273751974,\n",
       "    0.014411794021725655,\n",
       "    -0.03411859646439552,\n",
       "    -0.10801413655281067,\n",
       "    0.015253081917762756,\n",
       "    -0.020603014156222343,\n",
       "    -0.057520508766174316,\n",
       "    0.043438851833343506,\n",
       "    0.0043800584971904755,\n",
       "    0.007122566923499107,\n",
       "    0.0018238702323287725,\n",
       "    -0.05229131877422333,\n",
       "    0.03859343007206917,\n",
       "    0.020271189510822296,\n",
       "    0.004761361517012119,\n",
       "    0.0030847780872136354,\n",
       "    -0.06521423906087875,\n",
       "    -0.015419901348650455,\n",
       "    0.06322918832302094,\n",
       "    -0.01671646535396576,\n",
       "    0.011693081818521023,\n",
       "    0.047780122607946396,\n",
       "    0.059144970029592514,\n",
       "    0.006453026086091995,\n",
       "    -0.031581539660692215,\n",
       "    0.05880031734704971,\n",
       "    -0.007230551913380623,\n",
       "    -0.024975255131721497,\n",
       "    -0.014322281815111637,\n",
       "    0.03447920083999634,\n",
       "    -0.025666305795311928,\n",
       "    0.03816276416182518,\n",
       "    0.010934993624687195,\n",
       "    0.024891266599297523,\n",
       "    0.028975753113627434,\n",
       "    -0.08036989718675613,\n",
       "    0.013232114724814892,\n",
       "    0.06195138767361641,\n",
       "    0.04292212426662445,\n",
       "    -0.03426282852888107,\n",
       "    0.03269559517502785,\n",
       "    0.006005424540489912,\n",
       "    0.0008267576340585947,\n",
       "    -0.01433753501623869,\n",
       "    -0.05257308483123779,\n",
       "    0.06654360890388489,\n",
       "    -0.019356420263648033,\n",
       "    0.03146854043006897,\n",
       "    -0.024668611586093903,\n",
       "    0.06244141235947609,\n",
       "    -0.043590568006038666,\n",
       "    0.028514323756098747,\n",
       "    0.02715054526925087,\n",
       "    -0.031581442803144455,\n",
       "    0.021475771442055702,\n",
       "    -0.012741453014314175,\n",
       "    -0.0030736918561160564,\n",
       "    -0.012239676900207996,\n",
       "    0.0486503504216671,\n",
       "    -0.014228986576199532,\n",
       "    -0.007760099601000547,\n",
       "    -0.009477583691477776,\n",
       "    -0.05859416350722313,\n",
       "    -0.017644720152020454,\n",
       "    0.014053200371563435,\n",
       "    0.01808910071849823,\n",
       "    -0.025319695472717285,\n",
       "    -0.01022283174097538,\n",
       "    -0.04215439409017563,\n",
       "    -0.014142959378659725,\n",
       "    1.746752423059661e-05,\n",
       "    -0.04155829921364784,\n",
       "    -0.053963955491781235,\n",
       "    -0.026281701400876045,\n",
       "    0.0662294402718544,\n",
       "    -0.025692131370306015,\n",
       "    -0.03932614251971245,\n",
       "    0.026369275525212288,\n",
       "    -0.06941965967416763,\n",
       "    -0.01149339135736227,\n",
       "    -0.024656690657138824,\n",
       "    0.034839875996112823,\n",
       "    0.09246819466352463,\n",
       "    0.007840657606720924,\n",
       "    -0.005290364846587181,\n",
       "    -0.028822703287005424,\n",
       "    -0.036222007125616074,\n",
       "    0.04496601223945618,\n",
       "    0.03117789514362812,\n",
       "    -0.018668867647647858,\n",
       "    0.0005299983313307166,\n",
       "    0.0021931210067123175,\n",
       "    -0.023787183687090874,\n",
       "    -0.005189722869545221,\n",
       "    0.008987859822809696,\n",
       "    -0.012216147035360336,\n",
       "    0.0546068511903286,\n",
       "    -0.03400413691997528,\n",
       "    0.02338911034166813,\n",
       "    -0.004806441254913807,\n",
       "    0.01140426006168127,\n",
       "    -0.0261690691113472,\n",
       "    -0.009725328534841537,\n",
       "    -0.009852258488535881,\n",
       "    -0.05725707486271858,\n",
       "    -0.03211885318160057,\n",
       "    -0.04102523997426033,\n",
       "    -0.01062001008540392,\n",
       "    -0.018177682533860207,\n",
       "    -0.010140843689441681,\n",
       "    0.013285171240568161,\n",
       "    0.009264613501727581,\n",
       "    0.054535239934921265,\n",
       "    -0.030733125284314156,\n",
       "    0.0691450908780098,\n",
       "    2.852781051387865e-07,\n",
       "    0.03187370300292969,\n",
       "    0.002965539461001754,\n",
       "    -0.036401938647031784,\n",
       "    0.024400630965828896,\n",
       "    0.03300417214632034,\n",
       "    0.0004970004083588719,\n",
       "    -0.044018372893333435,\n",
       "    -0.04952297732234001,\n",
       "    0.00491013377904892,\n",
       "    -0.043458662927150726,\n",
       "    -0.008403669111430645,\n",
       "    0.05733603611588478,\n",
       "    0.023775087669491768,\n",
       "    -0.030646173283457756,\n",
       "    -0.0407634899020195,\n",
       "    -0.06806742399930954,\n",
       "    -0.0043092467822134495,\n",
       "    -0.021598175168037415,\n",
       "    -0.0010229316540062428,\n",
       "    0.05443334951996803,\n",
       "    -0.01235464122146368,\n",
       "    0.03557674214243889,\n",
       "    0.02124745212495327,\n",
       "    0.04711419343948364,\n",
       "    -0.018970131874084473,\n",
       "    -0.00847373716533184,\n",
       "    -0.04721027612686157,\n",
       "    -0.0025783327873796225,\n",
       "    0.05383775755763054,\n",
       "    0.03721201792359352,\n",
       "    -0.01994265429675579,\n",
       "    -0.011151931248605251,\n",
       "    0.04433703050017357,\n",
       "    0.0362195260822773,\n",
       "    0.0038219457492232323,\n",
       "    -0.024197371676564217,\n",
       "    -0.0013786935014650226,\n",
       "    0.06318159401416779,\n",
       "    -0.0016053984872996807,\n",
       "    0.021409476175904274,\n",
       "    0.05994655191898346,\n",
       "    0.013800276443362236,\n",
       "    0.01648109219968319,\n",
       "    -0.07152589410543442,\n",
       "    0.02832082286477089,\n",
       "    0.03596406802535057,\n",
       "    -0.06912487745285034,\n",
       "    -0.09572704136371613,\n",
       "    0.03602512925863266,\n",
       "    -0.04952844977378845,\n",
       "    -0.019229931756854057,\n",
       "    -0.005574565380811691,\n",
       "    0.04854048788547516,\n",
       "    0.02885768935084343,\n",
       "    -0.0009208315168507397,\n",
       "    -0.03850596770644188,\n",
       "    -0.021094683557748795,\n",
       "    0.018557477742433548,\n",
       "    -0.019414110109210014,\n",
       "    -0.0554368756711483,\n",
       "    0.011423497460782528,\n",
       "    -0.04004163295030594,\n",
       "    0.005466346628963947,\n",
       "    -0.05517859384417534,\n",
       "    0.057868655771017075,\n",
       "    0.017602544277906418,\n",
       "    0.03824613243341446,\n",
       "    2.6181650218507977e-34,\n",
       "    0.0028443161863833666,\n",
       "    0.039099451154470444,\n",
       "    -0.05114372819662094,\n",
       "    0.0901920422911644,\n",
       "    0.037334851920604706,\n",
       "    -0.06057137995958328,\n",
       "    -0.030114270746707916,\n",
       "    -0.009225860238075256,\n",
       "    -0.011277182959020138,\n",
       "    0.03147172927856445,\n",
       "    0.01292212400585413],\n",
       "   'chapter': 'CHAPTER 3',\n",
       "   'title': 'Technical Interview: Machine Learning Algorithms'}},\n",
       " {'_index': 'ml-interview-questions',\n",
       "  '_id': '37ez-ZEBwj93mx26uplr',\n",
       "  '_score': 1.9621209,\n",
       "  '_source': {'section': 'Statistical and Foundational Techniques',\n",
       "   'chapter': 'CHAPTER 3',\n",
       "   'title': 'Technical Interview: Machine Learning Algorithms'}},\n",
       " {'_index': 'ml-interview-questions',\n",
       "  '_id': '4Lez-ZEBwj93mx26u5lH',\n",
       "  '_score': 1.9621209,\n",
       "  '_source': {'text': 'Statistical techniques are used in every data role, and these techniques are the foun‐ dations for ML projects. Hence, in ML interviews, you will most likely have questions that cover this topic. 1 Statistical techniques help build baseline models to compare more costly models and algorithms against or help discover if there is enough mean‐ ingful data in the first place to build ML models. For the purposes of this book, I will be placing the foundational regression tech‐ niques in this section, as well as various techniques for training and improving ML models. In short, these are (1) foundational techniques and (2) methods used during model training, such as training splits, regularization, and so on. These concepts are foundational knowledge for any type of ML algorithms that will be mentioned later as well as foundations for ML interview questions. This section covers the basics of statistical techniques for those who are unsure whether they have sufficient background knowledge in this area. Feel free to skip the subsections if you already have expertise in any of these areas. Regardless of your expertise, I’ve highlighted specific advice for ML interviews in the tip boxes to help you apply your knowledge of each ML area and excel in your interviews. To further supplement your knowledge on statistical and foundational techniques beyond the summaries I’ve provided in this book, I recommend the following resources: • The Elements of Statistical Learning by Trevor Hastie et al. • An Introduction to Statistical Learning with Applications in Python by Gareth James et al. • Courses on Coursera by DeepLearning.AI and Andrew Ng; this resource is also useful for all following subtopics of ML (I won’t repeatedly link their courses— since they sometimes change and update—in the following sections). • Introduction to Machine Learning Interviews by Chip Huyen has additional questions for overall ML interviews which can be referenced for most sections in this chapter. Refer back to this section for reference material when preparing for interviews. Now, let’s jump in. Here’s an overview of one of the foundations of ML algorithms—variables—and a simple example of fitting a model. Let’s say that you have a dataset about apples, with the weight and height of each apple. You also have a list of past sales prices of each apple. With the list of apple weights, heights, and past sales prices, you want to guess the sales price of new apples, before they are sold. For the sake of this example, ignore big grocery chains automati‐ cally calculating a price, but let’s say you’re selling as a hobby to friends and family, or maybe you’re running a farm that a grandparent left you. So you are making use of the weight and height of each single new apple to predict its price. Weight and height are fixed observations at that point in time (an apple can’t be both 100 grams and 150 grams at the same time). Now, to connect all these concepts, let’s add in some terminology. Variables refer to everything that is being taken into account in your model of how apple prices are cal‐ culated. So the variables in this case include weight, height, and price. Within these variables, you know the weight and height of each new apple, and they are fixed at that point in time. So the weight and height are independent variables. Then, you have another variable, price, that you’d like to predict for new apples prior to know‐ ing the correct answer before selling them. The predicted price depends on the height and weight of the new apple. For example, heavier and taller apples sell for more money. Thus, price is a dependent variable (shown in Table 3-1 ). Table 3-1. Examples of independent and dependent variables The concepts of independent and dependent variables are tried and true, but the ter‐ minology might not be. In different fields, you may have come across the terms listed in Table 3-2 , although they may differ from industry to industry or from textbook to textbook. During your interview, make sure you and the interviewer aren’t crossing wires due to terminology, and double-check with your interviewer if you sense that you’re using a different term to refer to the same thing. Knowing the most common terms can help you use them appropriately in your interviews in various fields. Table 3-2. Synonyms of independent and dependent variables Models are a way of using past data points to describe “the way the world works,” or, in other words, a way of finding patterns and connections with past information. The apples example from the previous section uses a model that describes the way pricing works. The model is something that knows the “truth”—even if it’s not the full truth but rather our best attempt to approximate the truth. Thus, the model can be used to predict our best approximation of future data points. This applies for all “models” in ML models. Recommender-system models seek to predict what a user will like or click on when visiting a website. Convolutional neural networks (CNNs) for image recognition “learn” a model of what various pixels represent: is this cluster and layout of pixels a cat or a dog? Just as for independent and dependent variables, it is important to have a shared defi‐ nition for “model” to prevent miscommunication during an interview, such as con‐ fusing algorithms with models. 2 The model is the outcome of having run and fit an ML algorithm. I wanted to make sure I included regression models. I’m glad that I learned the detailed ins and outs of linear and logistic regression, even calculating them by hand (a requirement for the second-year statistics course I was taking as part of an eco‐ nomics major at university). This knowledge has compounded and helped me under‐ stand the new ML algorithms I’ve encountered as well as how to apply them in practice. All of my learning stemmed from understanding these entry-level concepts, so I highly recommend not shying away from learning the mathematics of regression models. Again, feel free to skip this section if you already have expertise in this area. Let’s use the apple example from an earlier section in a graph. For simplicity and to squeeze it onto a two-dimensional graph, let’s use just one independent variable, weight , to predict the dependent variable, price . Each dot on the graph in Figure 3-1 represents a data point from past sales, so you already know the sale prices for them. For example, the dot with a callout on the graph weighs 80 grams (its intersect on the x-axis) and sold for $1 (its intersect on the y-axis). Note that this is a simple example; most usage of linear regression will have multiple independent variables (“multivaria‐ ble”) and if visualized, will be a line in an N -dimensional space, where N = number of variables + 1 (when there is one output variable). In addition, this example has one dependent variable; when there are multiple dependent/output variables, the regres‐ sion task is referred to as being multivariate . Note that multivariate is separate from the “multivariable” concept mentioned earlier. Figure 3-1. Data points to be used for linear regression. The next step in linear regression is fitting the titular “line” to the data points. Behind the scenes, software tools like Python, Stata, IBM SPSS, SAS, MATLAB, and the like will calculate a “line of best fit.” According to the definition of a model given previ‐ ously in this section, this line is the model , which is the best approximation of the truth with the data points that you have. Starting with an initial line, the software will calculate the residual : the y-axis distance between a data point and the line, as illus‐ trated in Figure 3-2 . Colloquially, the residual is also referred to as the residual error . Figure 3-2. Fitting the line of best fit in linear regression; the line is iterated on until the residuals are as small as possible. All the residuals are squared so that predictions above and the line don’t cancel each other out due to having opposite signs (positive, negative). The goal is that the sum of the residuals is as small as possible since if you have a line that is drastically far away from the data points, that means the line isn’t fitting well to as many data points as possible and as correctly as possible. Mathematically, a common technique to tell how well the line is fitting is the process called least squares . Achieving least squares means finding the line that results in the smallest sum of squared residuals, which in turn means you have the “line of best fit”: the line is fitting the data points with least distance from the data points overall, as shown in Figure 3-3 . Figure 3-3. Least squares and terminology; y represents observed data points, and ŷ (y hat) represents the predicted/estimated values. The end result is a line that has the smallest sum of least squares to the data points, as illustrated in Figure 3-4 . Figure 3-4. The resulting line of best fit with least squares from the data in Figure 3-1 . Going forward, you can use this “line of best fit” as a model to predict new apple pri‐ ces! You can plug the apple weight into the line (in equation form) to get a numerical value for the predicted price. This is one of the most basic ways of calculating a model from data points, but it has the same pattern as more in-depth ML models and algo‐ rithms that are covered in the next chapter. Namely, you’ll initialize a line (you don’t know if this is the best model yet) and calculate the residuals, or how well it fits. Next, you’ll change the line by tilting it a little—mathematically, this is called updating coef‐ ficients or weights— and calculate the residuals again, as illustrated in Figure 3-2 . This updating process is called training , which is where the commonly used phrase “training/to train an ML model” comes from. If the sum of the squared residuals is getting smaller, then you are on the right track. When you can’t make the squared residuals any smaller, you’ve achieved least squares, and that’s how you can say the line is your best approximation with this dataset (as illustrated in Figure 3-4 ). It’s like that game where there’s an item hidden in the room, and as you walk around the room trying to find it, your friend says “hot” if you’re getting closer and “cold” when you’re walking farther away. You want to walk toward hotter and hotter areas in the room, until you reach the final position. In Chapter 4 , I’ll walk through ways to evaluate models via error terms such as mean squared error (MSE), root mean square error (RMSE), and more, which are very sim‐ ilar concepts to residuals. The main difference here is that residuals are the difference between past observation data and model estimations while errors are the difference between model estimations and actual data previously unseen by the model. In other words, errors are the differences after applying the model to previously unseen data in order to evaluate model performance. To sum up, when using supervised 3 machine learning such as the simple linear regression example in the previous section, you’ll generally start with a dataset and want the ML algorithm to learn a model of how things work. You then will use the model to calculate the values of dependent variables, such as predicting how much apples will sell for before they are actually sold. In other words, you have a dataset of past data points and, of course, no future data points. When the ML model is being trained, it’s learning to “fit” the data that you currently have. There are some issues that could arise with model training when the model is used in the real world. For one, there will always be outliers or changing events in the real world. One example is in financial predictions with ML: the market could swing suddenly to a bear (downturn) market, and the model we’ve trained with financial data in a bull (upswing) market will produce horrible and wildly inaccurate predictions. Another example is that the dataset you have isn’t representative enough of the behaviors of the real world. In the apples example from the previous section, you assume that with the weight and height data of the apples, you can predict the sell price of new apples. But what if the data you have on hand isn’t enough, and apple variants like Fuji or Honeycrisp (one of my favorites) sell for more? You didn’t have each apple’s variant name tracked in your dataset, so then your model may be incorrect once you put it to the test. But for now, you have only the current dataset. To make the most of it, you need to keep some of the data you have for testing purposes. What this means is that you can break out 80% of the apple data points to use for model training and then save 20% of the apple data points to run the trained model predictions on. The 80% the model is trained on is called the training set (sometimes referred to as the train set ), and the 20% of data that is unseen by the model during the training phase is called the test set . This mimics the real-world scenario of running the model to predict new data points; the test set serves that purpose. In many cases, you might even split the data into three chunks: 80% as the training dataset, 10% as the validation (holdout) dataset, and another 10% as the test dataset ( Figure 3-5 ). The validation set allows you to monitor the model’s performance during the train‐ ing process without “formally” evaluating it, and it enables you to diagnose weak spots of the model and tune its parameters. The test set, as previously mentioned, was unseen by the model during the training process and thus is used to formally evaluate the model performance, mimicking a real-world environment as much as possible. Of course, having a test and validation set isn’t infallible, which brings us to more robust techniques and the concepts of model overfitting and underfitting. Figure 3-5. Training, validation, and test set splits. For interview questions on training and test sets, make sure you can name common ways to augment the simpler splits, such as using cross-validation : 4 splitting up data into smaller chunks and rotating through them as training sets. There are many reasons a model may not perform well on real-world data (or even the validation or test set). A common starting point is addressing overfitting or underfitting. Underfitting is when the model isn’t fitting well. This might mean that the model isn’t able to capture the relationship between the dataset’s independent variables (e.g., weight, height, etc.) and the dependent variables (e.g., price). Conse‐ quently, some ways to reduce underfitting are related to helping the model learn more nuances or patterns during the training process. For example, adding more variables, or model features, such as apple variant or age of the apple, could help the model learn more patterns from the training data and potentially reduce underfitting. A second way to reduce underfitting is to increase the Overfitting is when a model fits the training data too closely and very specifically, perhaps finding patterns that happen to be in the training set but not elsewhere. A simplified example is that the training data just so happens to have a lot of apples that are disproportionately expensive despite their weight (e.g., Sekai Ichi apples 6 ). The model learned from that data and overfit to it, therefore making incorrect predictions that are overpriced for cheaper apple variants. Simply put, the model is overmemo‐ rizing the training data and unable to generalize to new data points. There are many techniques to make the model generalize better, such as adding more training data, data augmentation, or regularization. 7 I’ll cover the details of regularization next. Regularization is a technique used to reduce overfitting of ML models. Generally, regularization will create a damper on model weights/coefficient. By this point, you likely know what I’m going to do—which is to bring up the apples again! Apples are my favorite fruit, which is probably why I use the example so often. So let’s say the model has learned to weigh “weight of apple” more heavily (accidental pun, but model “weights” is legitimate terminology); then the weight of the apple is mathe‐ matically increasing the results of the ML model’s prediction of the price by a rela‐ tively high positive value. If you can dampen the amount by which the weight of the apple increases the model’s predictions of the price, via regularization, that can make the model generalize more and take other variables into account more evenly. The variance bias trade-off is a common topic in ML interviews. When applying ML model improvement techniques such as regularization, it is important to consider the trade-off between fixing for bias versus variance. Bias refers to the overall inaccuracy of the model and can often be caused by an oversimplified (underfit) model. Variance comes from overfitting, when the model has learned too specifically from the training set. One way to remember why this is called “variance” is that the term refers to the variability of the model: the model is overfit to specific points or traits, so the model is very sensitive to different data points, causing fluctuation and variability. Regularization might cause a model to reduce its variance but might inadvertently increase bias, so that’s a reason to be cautious and test various model-improvement techniques. Now that I’ve covered various statistical and ML techniques at a higher level, let’s look at some sample questions. Here, I will dive into the details of common interview questions that stem from the concepts covered in this section. These details may not have been previously addressed, so my hope is that these sample questions also serve to explain the new concepts. Example answer L1 regularization , also known as lasso regularization , 8 is a type of regularization that shrinks model parameters toward zero. L2 regularization (also known as ridge regularization ) adds a penalty term to the objective function that is propor‐ tional to the square of the coefficients of the model. This penalty term shrinks the coefficients toward zero, but unlike L1 (lasso) regularization, it does not make any of the coefficients exactly equal to zero. L2 regularization can help reduce overfitting and improve the stability of the model by keeping coefficients from becoming too large. Both L1 and L2 regulari‐ zation are commonly used to prevent overfitting and improve the generalization of ML models. Interview questions on model overfitting and underfitting may lead to follow-up questions. For example, if you bring up L1 and L2 regularization, the interviewer might ask, “What other types of reg‐ ularization could work?” In that case, you could bring up elastic net , which is a combination of L1 and L2 techniques. Or for the overfitting case, ensemble techniques can also help (refer to “Inter‐ view question 3-3: Explain boosting and bagging and what they can help with.” on page 76 ). Example answer Imbalanced datasets in ML refer to datasets in which some classes or categories outweigh others. 9 Techniques to deal with imbalanced datasets include data aug‐ mentation, oversampling, undersampling, ensemble methods, and so on: Data augmentation Data augmentation involves generating more examples for the ML model to train on, such as rotating images so that the dataset includes images of humans turned upside down as well as the normal upright image orienta‐ tion. Without data augmentation, the model might not be able to correctly recognize images of humans who are laying sideways or doing headstands since the data is imbalanced toward humans in an upright pose. Oversampling Oversampling is a technique to increase the number of data points of a minority class via synthetic generation. As an example, SMOTE (synthetic minority oversampling technique) 10 uses the feature vectors of the minority classes to generate synthetic data points that are located between real data points and their k-nearest neighbors. This could synthetically increase the size of the minority class(es) and improve the performance of the ML model trained on a dataset with oversampling treatment. Undersampling Undersampling does the opposite: it reduces examples from the majority class to balance the number of data points of the majority class and minority class(es). Oversampling is generally preferred in practice since undersam‐ pling may cause useful data to be discarded, which is exacerbated when the dataset is already small. Ensemble methods Ensemble methods can also be used to increase model performance when dealing with an imbalanced dataset. 11 Each model in the ensemble can be trained on a different subset of the data and can help learn the nuances of each class better. When answering ML interview questions, take a second to confirm the scope of the question. In other words, if the question is asking only for a definition of logistic regression, don’t go on a tangent about various other techniques. If the question is open-ended, you can confirm whether the interviewer is asking for something specific. Example answer Bagging and boosting are ensemble techniques used to improve the performance of ML models: Bagging Bagging trains multiple models on different subsets of the training data and combines their predictions to make a final prediction. Boosting Boosting trains a series of models where each model tries to correct the mis‐ takes made by the previous model. The final prediction is made by all the models. Ensemble techniques can help with a variety of issues encountered during ML training. For example, they can help with imbalanced data 12 and See Chapter 4 for more in-depth questions concerning model evaluation.',\n",
       "   'text_vector': [0.036523137241601944,\n",
       "    -0.0007432920974679291,\n",
       "    -0.0432945154607296,\n",
       "    -0.0033760142978280783,\n",
       "    -0.034039177000522614,\n",
       "    0.020350027829408646,\n",
       "    0.05255116894841194,\n",
       "    -0.005388661753386259,\n",
       "    -0.012733852490782738,\n",
       "    -0.0362168587744236,\n",
       "    0.060072142630815506,\n",
       "    0.02028055489063263,\n",
       "    0.04295819252729416,\n",
       "    0.020313583314418793,\n",
       "    0.04417930915951729,\n",
       "    -0.09074056148529053,\n",
       "    -0.011303425766527653,\n",
       "    -0.005822582636028528,\n",
       "    -0.006609124131500721,\n",
       "    0.0014482270926237106,\n",
       "    0.012197606265544891,\n",
       "    -0.0038030275609344244,\n",
       "    0.02707531489431858,\n",
       "    -0.007084347307682037,\n",
       "    -0.0828428715467453,\n",
       "    0.00489136204123497,\n",
       "    -0.017467757686972618,\n",
       "    0.019567061215639114,\n",
       "    -0.01860339380800724,\n",
       "    -0.02672656439244747,\n",
       "    0.02430013380944729,\n",
       "    -0.02965652197599411,\n",
       "    -0.028670847415924072,\n",
       "    -0.019623422995209694,\n",
       "    2.100783831338049e-06,\n",
       "    -0.033111900091171265,\n",
       "    -0.03307729586958885,\n",
       "    0.02178502455353737,\n",
       "    0.0020244757179170847,\n",
       "    -0.07057476043701172,\n",
       "    0.043849024921655655,\n",
       "    -0.0259234681725502,\n",
       "    0.049552787095308304,\n",
       "    0.04065810516476631,\n",
       "    -0.024271756410598755,\n",
       "    -0.017819207161664963,\n",
       "    0.08321874588727951,\n",
       "    0.04191126301884651,\n",
       "    0.010351017117500305,\n",
       "    0.035522401332855225,\n",
       "    -0.024218164384365082,\n",
       "    -0.06105755269527435,\n",
       "    0.0020202219020575285,\n",
       "    -0.00172926124650985,\n",
       "    -0.002213859697803855,\n",
       "    0.019961217418313026,\n",
       "    0.018572049215435982,\n",
       "    0.037241872400045395,\n",
       "    -0.019317928701639175,\n",
       "    -0.06962689757347107,\n",
       "    -0.014123695902526379,\n",
       "    0.02653263323009014,\n",
       "    0.025636684149503708,\n",
       "    0.039012473076581955,\n",
       "    0.05353250354528427,\n",
       "    0.00045431280159391463,\n",
       "    -0.033853475004434586,\n",
       "    -0.047667648643255234,\n",
       "    -0.03363697603344917,\n",
       "    0.0302193034440279,\n",
       "    -0.0006059636943973601,\n",
       "    0.004212603904306889,\n",
       "    0.004645414650440216,\n",
       "    -0.024126026779413223,\n",
       "    0.04055643826723099,\n",
       "    0.0386638343334198,\n",
       "    -0.05192442238330841,\n",
       "    -0.01303178258240223,\n",
       "    -0.0027255304157733917,\n",
       "    0.0033434468787163496,\n",
       "    -0.04510887712240219,\n",
       "    0.002753729932010174,\n",
       "    0.004314387682825327,\n",
       "    -0.036215223371982574,\n",
       "    0.009464943781495094,\n",
       "    0.04220883548259735,\n",
       "    -0.03910323604941368,\n",
       "    -0.03750601038336754,\n",
       "    0.04307063668966293,\n",
       "    0.024784497916698456,\n",
       "    0.05995957553386688,\n",
       "    0.012582555413246155,\n",
       "    -0.007807245012372732,\n",
       "    0.03980224207043648,\n",
       "    -0.008865228854119778,\n",
       "    0.01757882535457611,\n",
       "    0.0207821074873209,\n",
       "    -0.01814902201294899,\n",
       "    0.022295553237199783,\n",
       "    -0.03189857676625252,\n",
       "    -0.014835912734270096,\n",
       "    -0.008013461716473103,\n",
       "    -0.01237302366644144,\n",
       "    -0.034499797970056534,\n",
       "    0.004047177731990814,\n",
       "    -0.0008237141883000731,\n",
       "    0.047848641872406006,\n",
       "    -0.022965388372540474,\n",
       "    -0.04463614523410797,\n",
       "    -0.004908710718154907,\n",
       "    -0.03371310234069824,\n",
       "    0.014752012677490711,\n",
       "    0.006612858269363642,\n",
       "    0.040917642414569855,\n",
       "    0.010968323796987534,\n",
       "    0.04976126179099083,\n",
       "    -0.005838205572217703,\n",
       "    -0.00340695190243423,\n",
       "    0.03452989086508751,\n",
       "    -0.048853788524866104,\n",
       "    -0.03082912042737007,\n",
       "    0.03601294755935669,\n",
       "    0.035098280757665634,\n",
       "    0.09186170250177383,\n",
       "    -0.017461182549595833,\n",
       "    -0.07163863629102707,\n",
       "    -0.04070524126291275,\n",
       "    0.03825703635811806,\n",
       "    -0.030244776979088783,\n",
       "    -0.05524608865380287,\n",
       "    0.04929491505026817,\n",
       "    0.017085066065192223,\n",
       "    0.02404647320508957,\n",
       "    -0.0015520754968747497,\n",
       "    0.05577943101525307,\n",
       "    0.01873045787215233,\n",
       "    0.028053198009729385,\n",
       "    0.02291409857571125,\n",
       "    -0.09274528175592422,\n",
       "    -0.0212035421282053,\n",
       "    0.024848653003573418,\n",
       "    -0.07113061845302582,\n",
       "    0.0017574595985934138,\n",
       "    -0.04926798492670059,\n",
       "    -0.023708222433924675,\n",
       "    0.010406098328530788,\n",
       "    -0.01646629348397255,\n",
       "    -0.031374696642160416,\n",
       "    0.004565098322927952,\n",
       "    -0.03471960127353668,\n",
       "    -0.07150722295045853,\n",
       "    -0.006568982265889645,\n",
       "    0.03470566123723984,\n",
       "    0.020325850695371628,\n",
       "    0.03409697487950325,\n",
       "    -0.02281690016388893,\n",
       "    0.021977996453642845,\n",
       "    0.04635070264339447,\n",
       "    0.010784306563436985,\n",
       "    0.012090804055333138,\n",
       "    0.006639380473643541,\n",
       "    -0.05897798016667366,\n",
       "    0.048791754990816116,\n",
       "    0.019540075212717056,\n",
       "    0.023658879101276398,\n",
       "    -0.04116778075695038,\n",
       "    -0.03905382379889488,\n",
       "    0.009988395497202873,\n",
       "    -0.0484577938914299,\n",
       "    -0.023476315662264824,\n",
       "    0.01899963617324829,\n",
       "    0.020813094452023506,\n",
       "    0.0050387876108288765,\n",
       "    0.04070603847503662,\n",
       "    0.03995497524738312,\n",
       "    0.0539737232029438,\n",
       "    0.009352164342999458,\n",
       "    0.00287872226908803,\n",
       "    0.0003331065527163446,\n",
       "    -0.020866677165031433,\n",
       "    0.019330037757754326,\n",
       "    0.0321771539747715,\n",
       "    -0.004746125545352697,\n",
       "    0.022074194625020027,\n",
       "    0.02259751595556736,\n",
       "    -0.017941661179065704,\n",
       "    -0.0613631010055542,\n",
       "    0.04856313392519951,\n",
       "    -0.032226480543613434,\n",
       "    -0.04425865039229393,\n",
       "    -0.0047739543952047825,\n",
       "    0.005284262355417013,\n",
       "    -0.02744286134839058,\n",
       "    0.07892142236232758,\n",
       "    0.03734707459807396,\n",
       "    -0.0571587048470974,\n",
       "    -0.040705326944589615,\n",
       "    -0.03076474741101265,\n",
       "    -0.04223315417766571,\n",
       "    0.01950768008828163,\n",
       "    -0.04649965092539787,\n",
       "    -0.07959268987178802,\n",
       "    -0.009798326529562473,\n",
       "    -0.019634883850812912,\n",
       "    -0.024099254980683327,\n",
       "    -0.0148086566478014,\n",
       "    -0.005400160327553749,\n",
       "    -0.057829100638628006,\n",
       "    -0.01413261704146862,\n",
       "    -0.037579067051410675,\n",
       "    0.023634986951947212,\n",
       "    -0.008333719335496426,\n",
       "    -0.001808247179724276,\n",
       "    0.03684408217668533,\n",
       "    -0.02903398871421814,\n",
       "    0.028932521119713783,\n",
       "    0.018027858808636665,\n",
       "    -0.03655659034848213,\n",
       "    -0.004538258537650108,\n",
       "    0.02655034139752388,\n",
       "    0.0023902698885649443,\n",
       "    -0.00847261119633913,\n",
       "    -0.06344844400882721,\n",
       "    0.04394364729523659,\n",
       "    -0.0013727130135521293,\n",
       "    -0.007013477385044098,\n",
       "    0.020547926425933838,\n",
       "    -0.014848718419671059,\n",
       "    0.05061951279640198,\n",
       "    0.017796959728002548,\n",
       "    -0.017474118620157242,\n",
       "    0.0029133628122508526,\n",
       "    -0.0020552834030240774,\n",
       "    0.06448186188936234,\n",
       "    0.009064741432666779,\n",
       "    0.042802754789590836,\n",
       "    0.05494966730475426,\n",
       "    -0.012089810334146023,\n",
       "    0.01260928250849247,\n",
       "    -0.033363983035087585,\n",
       "    -0.009789299219846725,\n",
       "    0.025636455044150352,\n",
       "    -0.023298529908061028,\n",
       "    -0.013590295799076557,\n",
       "    0.004210152663290501,\n",
       "    -0.024192944169044495,\n",
       "    0.023971131071448326,\n",
       "    0.01794952154159546,\n",
       "    0.019941510632634163,\n",
       "    -0.05012931674718857,\n",
       "    0.053982269018888474,\n",
       "    -0.015611598268151283,\n",
       "    -0.024895042181015015,\n",
       "    0.014172745868563652,\n",
       "    -0.04308832436800003,\n",
       "    -0.029866961762309074,\n",
       "    -0.07729396969079971,\n",
       "    0.06444510072469711,\n",
       "    -0.05598203465342522,\n",
       "    0.019882729277014732,\n",
       "    -0.017082588747143745,\n",
       "    -0.04856284335255623,\n",
       "    0.0402895025908947,\n",
       "    0.05245945230126381,\n",
       "    -0.015597084537148476,\n",
       "    -0.028517892584204674,\n",
       "    0.04628562182188034,\n",
       "    -0.04226870834827423,\n",
       "    -0.051681917160749435,\n",
       "    0.009798443876206875,\n",
       "    0.020686998963356018,\n",
       "    -0.03493507206439972,\n",
       "    -0.02357720397412777,\n",
       "    -0.03515750542283058,\n",
       "    -0.011370116844773293,\n",
       "    -0.02122955210506916,\n",
       "    -0.02620919793844223,\n",
       "    0.018916774541139603,\n",
       "    -0.022632839158177376,\n",
       "    0.016698170453310013,\n",
       "    0.008371404372155666,\n",
       "    -0.017923371866345406,\n",
       "    -0.022603247314691544,\n",
       "    -0.03060407005250454,\n",
       "    -0.030249347910284996,\n",
       "    0.049879878759384155,\n",
       "    0.026675119996070862,\n",
       "    -0.04996521398425102,\n",
       "    0.03452010080218315,\n",
       "    0.04009155556559563,\n",
       "    -0.008268361911177635,\n",
       "    0.004845915827900171,\n",
       "    0.015609807334840298,\n",
       "    -0.03657381981611252,\n",
       "    -0.03021540306508541,\n",
       "    0.0302544254809618,\n",
       "    0.0043389624916017056,\n",
       "    0.07188685983419418,\n",
       "    -0.03142336755990982,\n",
       "    -0.0006203280645422637,\n",
       "    0.004607014358043671,\n",
       "    -0.009082073345780373,\n",
       "    -0.002992464927956462,\n",
       "    0.017054514959454536,\n",
       "    0.007074362598359585,\n",
       "    0.03323652595281601,\n",
       "    -0.013753260485827923,\n",
       "    -0.04071330651640892,\n",
       "    -0.022993650287389755,\n",
       "    -0.008187214843928814,\n",
       "    0.07186315208673477,\n",
       "    -0.04311871901154518,\n",
       "    0.04904330521821976,\n",
       "    0.08635549247264862,\n",
       "    0.02626853808760643,\n",
       "    -0.0796511247754097,\n",
       "    -0.04572908580303192,\n",
       "    0.039594996720552444,\n",
       "    0.005481055937707424,\n",
       "    0.05847568064928055,\n",
       "    -0.0012375687947496772,\n",
       "    -0.07025717198848724,\n",
       "    0.00736632477492094,\n",
       "    0.02295425906777382,\n",
       "    0.018902722746133804,\n",
       "    -0.04538080841302872,\n",
       "    -0.02345798723399639,\n",
       "    0.022726068273186684,\n",
       "    0.015088234096765518,\n",
       "    -7.952674422995187e-06,\n",
       "    -0.0008040404063649476,\n",
       "    -0.02290807105600834,\n",
       "    0.0300035048276186,\n",
       "    -0.010364477522671223,\n",
       "    -0.030911428853869438,\n",
       "    0.008581935428082943,\n",
       "    -0.015181084163486958,\n",
       "    -0.05015239492058754,\n",
       "    -0.012426754459738731,\n",
       "    -0.0050795357674360275,\n",
       "    0.005392144899815321,\n",
       "    0.014479756355285645,\n",
       "    -0.06757500767707825,\n",
       "    -0.07544386386871338,\n",
       "    0.015011251904070377,\n",
       "    -0.026010844856500626,\n",
       "    -0.04803434759378433,\n",
       "    0.011020510457456112,\n",
       "    -0.03159291669726372,\n",
       "    -1.4978345461713616e-06,\n",
       "    0.05167064070701599,\n",
       "    -0.027015365660190582,\n",
       "    -0.007939858362078667,\n",
       "    -0.004214507061988115,\n",
       "    0.06863216310739517,\n",
       "    -0.02415107563138008,\n",
       "    0.025002406910061836,\n",
       "    -0.024279680103063583,\n",
       "    0.05935220792889595,\n",
       "    0.02238640934228897,\n",
       "    0.05052677541971207,\n",
       "    -0.03164755553007126,\n",
       "    0.015941934660077095,\n",
       "    0.021370800212025642,\n",
       "    -0.09116561710834503,\n",
       "    0.026284562423825264,\n",
       "    -0.017275555059313774,\n",
       "    -0.021034061908721924,\n",
       "    0.013541484251618385,\n",
       "    0.051554616540670395,\n",
       "    -0.02600589022040367,\n",
       "    -0.0316668264567852,\n",
       "    0.06590934097766876,\n",
       "    0.07302077114582062,\n",
       "    -0.00018832281057257205,\n",
       "    0.04420003667473793,\n",
       "    -0.013746307231485844,\n",
       "    -0.019049707800149918,\n",
       "    -0.018561065196990967,\n",
       "    -0.02713731862604618,\n",
       "    0.056265559047460556,\n",
       "    -0.02862958237528801,\n",
       "    0.021563809365034103,\n",
       "    -0.02247808687388897,\n",
       "    0.0051451679319143295,\n",
       "    -0.09757675230503082,\n",
       "    -0.02506050281226635,\n",
       "    0.02779846265912056,\n",
       "    -0.009687189012765884,\n",
       "    0.037709254771471024,\n",
       "    -0.010662616230547428,\n",
       "    -0.07664190977811813,\n",
       "    -0.05375158041715622,\n",
       "    -0.043101225048303604,\n",
       "    -0.010198119096457958,\n",
       "    -0.039026301354169846,\n",
       "    0.024810612201690674,\n",
       "    -0.025994187220931053,\n",
       "    0.0005799017962999642,\n",
       "    0.021318256855010986,\n",
       "    0.05682651326060295,\n",
       "    -0.12498573213815689,\n",
       "    -0.03433620557188988,\n",
       "    -0.012117802165448666,\n",
       "    -0.031183630228042603,\n",
       "    -0.039130035787820816,\n",
       "    0.0805756077170372,\n",
       "    0.050922542810440063,\n",
       "    -0.06782805174589157,\n",
       "    0.04181859642267227,\n",
       "    0.016941865906119347,\n",
       "    0.0697193518280983,\n",
       "    0.05734032392501831,\n",
       "    0.011460741050541401,\n",
       "    0.0796310305595398,\n",
       "    0.005391092970967293,\n",
       "    0.02560981921851635,\n",
       "    -0.03779125213623047,\n",
       "    0.06821620464324951,\n",
       "    -0.0008785868994891644,\n",
       "    0.0054435632191598415,\n",
       "    0.04192770645022392,\n",
       "    0.06535118073225021,\n",
       "    -0.027544058859348297,\n",
       "    -0.05329585447907448,\n",
       "    -0.005204528104513884,\n",
       "    0.017901187762618065,\n",
       "    0.03392971307039261,\n",
       "    0.024046586826443672,\n",
       "    -0.01413942500948906,\n",
       "    -0.012698640115559101,\n",
       "    -0.040295444428920746,\n",
       "    0.01628299430012703,\n",
       "    0.01014635618776083,\n",
       "    0.005880649667233229,\n",
       "    0.015682145953178406,\n",
       "    0.010439534671604633,\n",
       "    -0.0041690487414598465,\n",
       "    0.009202095679938793,\n",
       "    0.015596497803926468,\n",
       "    -0.0530460849404335,\n",
       "    0.06010688841342926,\n",
       "    0.004741130396723747,\n",
       "    0.03997192159295082,\n",
       "    0.05987829715013504,\n",
       "    -0.026152106001973152,\n",
       "    -0.013342569582164288,\n",
       "    -0.03995945304632187,\n",
       "    0.0134560726583004,\n",
       "    0.03594321012496948,\n",
       "    0.02462034486234188,\n",
       "    0.04707102105021477,\n",
       "    -0.014405380003154278,\n",
       "    0.0369429774582386,\n",
       "    -0.04781769588589668,\n",
       "    0.003910190425813198,\n",
       "    -0.08764549344778061,\n",
       "    -0.03587689250707626,\n",
       "    -0.02670390158891678,\n",
       "    -0.007295926101505756,\n",
       "    0.07252317667007446,\n",
       "    0.00910146627575159,\n",
       "    0.00845267716795206,\n",
       "    -0.003459046594798565,\n",
       "    -0.06469310820102692,\n",
       "    0.013545672409236431,\n",
       "    0.00966564379632473,\n",
       "    -0.018970590084791183,\n",
       "    -0.0036542972084134817,\n",
       "    0.04566759988665581,\n",
       "    -0.01280929148197174,\n",
       "    0.005412218160927296,\n",
       "    -0.05538897588849068,\n",
       "    0.04199243709445,\n",
       "    0.028635846450924873,\n",
       "    -0.013836679980158806,\n",
       "    -0.07188721001148224,\n",
       "    -0.0365544892847538,\n",
       "    -0.026219792664051056,\n",
       "    -0.024923231452703476,\n",
       "    0.03383069112896919,\n",
       "    0.06315983086824417,\n",
       "    -0.023246990516781807,\n",
       "    -0.050979502499103546,\n",
       "    -0.0038422434590756893,\n",
       "    -0.011691350489854813,\n",
       "    0.07211718708276749,\n",
       "    0.0011100223055109382,\n",
       "    -0.018548181280493736,\n",
       "    -0.023065609857439995,\n",
       "    0.04869747534394264,\n",
       "    0.0777503103017807,\n",
       "    -0.03319019824266434,\n",
       "    -0.07338888198137283,\n",
       "    -0.05023299530148506,\n",
       "    -0.039534974843263626,\n",
       "    -0.026219382882118225,\n",
       "    0.09284378588199615,\n",
       "    0.0321725457906723,\n",
       "    -0.00911789946258068,\n",
       "    -0.0052711437456309795,\n",
       "    0.023324456065893173,\n",
       "    -0.009662849828600883,\n",
       "    -0.00870165042579174,\n",
       "    0.03987937048077583,\n",
       "    -0.01313027273863554,\n",
       "    0.06363864988088608,\n",
       "    0.01266563031822443,\n",
       "    -0.00911864172667265,\n",
       "    0.05104406177997589,\n",
       "    -0.009922846220433712,\n",
       "    -0.006362599320709705,\n",
       "    0.0057648406364023685,\n",
       "    -0.03484948351979256,\n",
       "    -0.0032184962183237076,\n",
       "    -0.05094584822654724,\n",
       "    0.04392722249031067,\n",
       "    -0.04660162702202797,\n",
       "    -0.03263597562909126,\n",
       "    0.00948328897356987,\n",
       "    0.02031722478568554,\n",
       "    -0.022979261353611946,\n",
       "    0.006743982899934053,\n",
       "    -0.005397664848715067,\n",
       "    0.08122143894433975,\n",
       "    -0.006449740380048752,\n",
       "    -0.0233546681702137,\n",
       "    -0.070151686668396,\n",
       "    -0.009988682344555855,\n",
       "    0.02183588035404682,\n",
       "    -0.00047578749945387244,\n",
       "    0.013671123422682285,\n",
       "    0.011637314222753048,\n",
       "    -0.017824925482273102,\n",
       "    0.01357120368629694,\n",
       "    0.05561745539307594,\n",
       "    0.08742599189281464,\n",
       "    0.008799877017736435,\n",
       "    0.006877847481518984,\n",
       "    -0.008604791015386581,\n",
       "    -0.03516962379217148,\n",
       "    -0.04831170290708542,\n",
       "    0.022092105820775032,\n",
       "    -0.04061242565512657,\n",
       "    0.034801434725522995,\n",
       "    -0.030106810852885246,\n",
       "    0.004412548150867224,\n",
       "    0.046932071447372437,\n",
       "    -0.02305869571864605,\n",
       "    0.08602577447891235,\n",
       "    -0.05137799680233002,\n",
       "    -0.057735055685043335,\n",
       "    -0.017031613737344742,\n",
       "    -0.039930518716573715,\n",
       "    0.027200479060411453,\n",
       "    -5.6179185606880455e-33,\n",
       "    0.060003962367773056,\n",
       "    -0.07018348574638367,\n",
       "    0.03426384553313255,\n",
       "    0.06646239012479782,\n",
       "    -0.006295596715062857,\n",
       "    -0.017198534682393074,\n",
       "    -0.025232603773474693,\n",
       "    0.04921244457364082,\n",
       "    -0.0398479588329792,\n",
       "    -0.009288524277508259,\n",
       "    -0.012060563080012798,\n",
       "    0.022754870355129242,\n",
       "    0.0038370632100850344,\n",
       "    0.0022318768315017223,\n",
       "    -0.013165656477212906,\n",
       "    -0.03507212921977043,\n",
       "    -0.018438003957271576,\n",
       "    -0.002450457541272044,\n",
       "    0.021565282717347145,\n",
       "    -0.08119048178195953,\n",
       "    -0.07423408329486847,\n",
       "    0.032853756099939346,\n",
       "    0.0038617178797721863,\n",
       "    -0.05869396775960922,\n",
       "    0.03303302451968193,\n",
       "    0.035638682544231415,\n",
       "    0.014838764443993568,\n",
       "    -0.03369583934545517,\n",
       "    0.006180229131132364,\n",
       "    0.004653407260775566,\n",
       "    0.03176305815577507,\n",
       "    0.0009315602947026491,\n",
       "    0.02639714814722538,\n",
       "    -0.025116661563515663,\n",
       "    -0.021693794056773186,\n",
       "    0.08230579644441605,\n",
       "    -0.020206209272146225,\n",
       "    0.008325675502419472,\n",
       "    0.024737032130360603,\n",
       "    0.054107893258333206,\n",
       "    0.020826589316129684,\n",
       "    -0.012079989537596703,\n",
       "    0.012211107648909092,\n",
       "    0.0037422452587634325,\n",
       "    -0.021670423448085785,\n",
       "    -0.00245551741681993,\n",
       "    0.06535849720239639,\n",
       "    -0.032669272273778915,\n",
       "    0.020419709384441376,\n",
       "    -0.019589316099882126,\n",
       "    0.014356718398630619,\n",
       "    0.025222048163414,\n",
       "    -0.05632797256112099,\n",
       "    0.02787345089018345,\n",
       "    0.030664199963212013,\n",
       "    0.0861092135310173,\n",
       "    -0.023670261725783348,\n",
       "    -0.010860497131943703,\n",
       "    0.039253298193216324,\n",
       "    0.021682728081941605,\n",
       "    -0.005390592385083437,\n",
       "    -0.0360676646232605,\n",
       "    0.04944359138607979,\n",
       "    0.04714436084032059,\n",
       "    0.052185606211423874,\n",
       "    -0.01125520933419466,\n",
       "    0.03295044228434563,\n",
       "    -0.07306060940027237,\n",
       "    0.007280468475073576,\n",
       "    0.0015147136291489005,\n",
       "    0.03143317624926567,\n",
       "    0.0414789617061615,\n",
       "    0.022633442655205727,\n",
       "    0.03986106440424919,\n",
       "    0.004179369192570448,\n",
       "    -0.010775432921946049,\n",
       "    -0.09657904505729675,\n",
       "    0.005734473932534456,\n",
       "    0.07747834920883179,\n",
       "    -0.047730252146720886,\n",
       "    -0.023753708228468895,\n",
       "    -0.013946753926575184,\n",
       "    0.0205468088388443,\n",
       "    -0.02594764344394207,\n",
       "    -0.04007091745734215,\n",
       "    -0.04849539324641228,\n",
       "    0.009682039730250835,\n",
       "    -0.007807636633515358,\n",
       "    0.006197373848408461,\n",
       "    -0.032278772443532944,\n",
       "    -0.03521755710244179,\n",
       "    0.0634220689535141,\n",
       "    0.06396190822124481,\n",
       "    -0.0007208309252746403,\n",
       "    0.0005205576308071613,\n",
       "    -0.023755040019750595,\n",
       "    0.0009311099420301616,\n",
       "    0.009270060807466507,\n",
       "    0.028142698109149933,\n",
       "    0.033171385526657104,\n",
       "    -0.038683757185935974,\n",
       "    -4.958247700415086e-06,\n",
       "    -0.02278917282819748,\n",
       "    -0.000536112580448389,\n",
       "    0.055870264768600464,\n",
       "    -0.015047461725771427,\n",
       "    -0.040450453758239746,\n",
       "    0.04462364688515663,\n",
       "    0.004165883641690016,\n",
       "    -0.006083915010094643,\n",
       "    0.037934593856334686,\n",
       "    0.036177776753902435,\n",
       "    -0.027824878692626953,\n",
       "    0.005931659135967493,\n",
       "    -0.04001924395561218,\n",
       "    0.015831658616662025,\n",
       "    -0.01831350475549698,\n",
       "    0.036606840789318085,\n",
       "    -0.05697938799858093,\n",
       "    -0.05217789486050606,\n",
       "    -0.01843121089041233,\n",
       "    -0.03513339161872864,\n",
       "    -0.01859796978533268,\n",
       "    -0.063231460750103,\n",
       "    -0.015719572082161903,\n",
       "    -0.031277816742658615,\n",
       "    0.009847285225987434,\n",
       "    -0.04689276963472366,\n",
       "    0.03387955576181412,\n",
       "    -0.005764287896454334,\n",
       "    -0.00893369410187006,\n",
       "    -0.005525819957256317,\n",
       "    2.6454799240127613e-07,\n",
       "    0.02674459107220173,\n",
       "    0.03304973244667053,\n",
       "    -0.0027186351362615824,\n",
       "    0.004944884683936834,\n",
       "    0.021683242172002792,\n",
       "    -0.009071430191397667,\n",
       "    -0.003795315744355321,\n",
       "    -0.03485168144106865,\n",
       "    -0.0055013177916407585,\n",
       "    -0.025743385776877403,\n",
       "    0.040535520762205124,\n",
       "    0.023685017600655556,\n",
       "    0.03869763761758804,\n",
       "    -0.004096765071153641,\n",
       "    -0.04646331071853638,\n",
       "    -0.12914304435253143,\n",
       "    -0.00530387694016099,\n",
       "    -0.027295267209410667,\n",
       "    -0.004948721267282963,\n",
       "    0.05135786533355713,\n",
       "    -0.005525260232388973,\n",
       "    0.03796916827559471,\n",
       "    0.014531390741467476,\n",
       "    -0.010143283754587173,\n",
       "    -0.009512348100543022,\n",
       "    0.031208237633109093,\n",
       "    -0.04764265567064285,\n",
       "    -0.00661342591047287,\n",
       "    0.053484126925468445,\n",
       "    0.01847987249493599,\n",
       "    0.030841808766126633,\n",
       "    -0.006576251238584518,\n",
       "    0.06080634891986847,\n",
       "    0.04468175396323204,\n",
       "    -0.007604672573506832,\n",
       "    -0.0403311662375927,\n",
       "    0.0024315412156283855,\n",
       "    0.02273777313530445,\n",
       "    0.009461461566388607,\n",
       "    -0.043872684240341187,\n",
       "    0.03613031655550003,\n",
       "    0.028147215023636818,\n",
       "    0.018233099952340126,\n",
       "    -0.041730064898729324,\n",
       "    0.06812526285648346,\n",
       "    0.0032304669730365276,\n",
       "    -0.04977729544043541,\n",
       "    -0.035548996180295944,\n",
       "    0.017497794702649117,\n",
       "    0.00135122612118721,\n",
       "    0.05635526031255722,\n",
       "    -0.02774309366941452,\n",
       "    0.04953973740339279,\n",
       "    0.016103480011224747,\n",
       "    0.002199676586315036,\n",
       "    -0.0394558310508728,\n",
       "    -0.017118239775300026,\n",
       "    0.02750077098608017,\n",
       "    -0.007899406366050243,\n",
       "    0.010119176469743252,\n",
       "    -0.007878738455474377,\n",
       "    -0.05360225588083267,\n",
       "    0.06121016666293144,\n",
       "    -0.004513206891715527,\n",
       "    0.04067198187112808,\n",
       "    0.033526480197906494,\n",
       "    8.93038886715658e-05,\n",
       "    2.304326167691738e-34,\n",
       "    0.002129961736500263,\n",
       "    0.026877233758568764,\n",
       "    -0.030736606568098068,\n",
       "    0.05065155029296875,\n",
       "    -0.014355966821312904,\n",
       "    -0.03415072336792946,\n",
       "    -0.059387873858213425,\n",
       "    -0.007071103435009718,\n",
       "    0.018682895228266716,\n",
       "    0.027728809043765068,\n",
       "    0.018173668533563614],\n",
       "   'chapter': 'CHAPTER 3',\n",
       "   'title': 'Technical Interview: Machine Learning Algorithms'}}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = es_client.search(\n",
    "    index=index_name,\n",
    "    body={\n",
    "        \"size\": 5,\n",
    "        \"knn\": {\n",
    "            \"field\": \"text_vector\",\n",
    "            \"query_vector\": vector_search_term,\n",
    "            \"k\": 5,\n",
    "            \"num_candidates\": 10000\n",
    "        },\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": [\n",
    "                    {\n",
    "                        \"match\": {\"chapter\": \"CHAPTER 3\"}\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "response['hits']['hits']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score == 2.81. When semanticsearch with elasticsearch can be more than 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_shard': '[ml-interview-questions][0]',\n",
       "  '_node': 'EScPTzvSSEeYjH0xRE7sig',\n",
       "  '_index': 'ml-interview-questions',\n",
       "  '_id': '3Lez-ZEBwj93mx26upki',\n",
       "  '_score': 2.817597,\n",
       "  '_source': {'text': 'In Chapter 1 , you learned about the various steps you will go through as part of your ML interviews. In Chapter 2 , you looked at how to tie your experiences to roles of interest as well as how to craft a relevant resume. The goal of the previous chapters was to get you invited to interviews. In this chapter, I’ll focus on ML algorithms. As you recall, the interview process is illustrated in Figure 1-9 , and the ML algorithms interview is only one portion of the technical interviews; the rest, such as ML training and evaluation, coding, and so on, will be covered in subsequent chapters.',\n",
       "   'text_vector': [0.0555436834692955,\n",
       "    -0.057862937450408936,\n",
       "    -0.06579995900392532,\n",
       "    -0.018207866698503494,\n",
       "    0.008833812549710274,\n",
       "    0.02567276917397976,\n",
       "    -0.01176581159234047,\n",
       "    -0.03683503344655037,\n",
       "    0.007987787015736103,\n",
       "    -0.03394889086484909,\n",
       "    0.06602350622415543,\n",
       "    0.05767723545432091,\n",
       "    0.033435020595788956,\n",
       "    0.03225232660770416,\n",
       "    0.04612836241722107,\n",
       "    -0.05221432074904442,\n",
       "    0.026441171765327454,\n",
       "    -0.0014632202219218016,\n",
       "    -0.02367429994046688,\n",
       "    -0.02824978344142437,\n",
       "    -0.01720539666712284,\n",
       "    0.011182794347405434,\n",
       "    -0.02285507321357727,\n",
       "    0.01757640205323696,\n",
       "    -0.0022415067069232464,\n",
       "    -0.04095837473869324,\n",
       "    -0.02818242833018303,\n",
       "    0.02927485667169094,\n",
       "    -0.024174313992261887,\n",
       "    0.01460934802889824,\n",
       "    -0.033344026654958725,\n",
       "    -0.06602809578180313,\n",
       "    0.01739729754626751,\n",
       "    -0.00852904748171568,\n",
       "    2.1192950043769088e-06,\n",
       "    -0.04168311879038811,\n",
       "    0.008932244032621384,\n",
       "    0.0050083850510418415,\n",
       "    -0.038519881665706635,\n",
       "    -0.07101845741271973,\n",
       "    0.014262642711400986,\n",
       "    0.023514946922659874,\n",
       "    0.03278544545173645,\n",
       "    0.006159830838441849,\n",
       "    0.004875018261373043,\n",
       "    0.02702215686440468,\n",
       "    0.06059115752577782,\n",
       "    0.03233695030212402,\n",
       "    0.02308649942278862,\n",
       "    0.05835425853729248,\n",
       "    0.0018108042422682047,\n",
       "    -0.054530736058950424,\n",
       "    0.017236042767763138,\n",
       "    0.05804227665066719,\n",
       "    0.009896809235215187,\n",
       "    0.0024993084371089935,\n",
       "    -0.007473664823919535,\n",
       "    -0.0018575682770460844,\n",
       "    0.028342463076114655,\n",
       "    -0.08948442339897156,\n",
       "    0.006773480214178562,\n",
       "    0.027347367256879807,\n",
       "    0.013087188825011253,\n",
       "    0.011941199190914631,\n",
       "    0.07401155680418015,\n",
       "    0.03871525451540947,\n",
       "    -0.015987496823072433,\n",
       "    -0.04244381561875343,\n",
       "    -0.034510288387537,\n",
       "    -0.022553689777851105,\n",
       "    0.05420053377747536,\n",
       "    -0.02955695241689682,\n",
       "    -0.010906168259680271,\n",
       "    0.0023433237802237272,\n",
       "    0.04894989728927612,\n",
       "    -0.026037678122520447,\n",
       "    -0.0288933627307415,\n",
       "    -0.0016128385905176401,\n",
       "    -0.0076993368566036224,\n",
       "    -0.026812460273504257,\n",
       "    -0.04198426008224487,\n",
       "    0.024094486609101295,\n",
       "    -0.005777856335043907,\n",
       "    -0.010915958322584629,\n",
       "    0.0020754162687808275,\n",
       "    0.08892589807510376,\n",
       "    -0.011866339482367039,\n",
       "    -0.004000314977020025,\n",
       "    0.0317656509578228,\n",
       "    -0.003056977177038789,\n",
       "    0.10577969253063202,\n",
       "    0.009065842255949974,\n",
       "    0.04349280893802643,\n",
       "    0.05554656311869621,\n",
       "    -0.02227569930255413,\n",
       "    -0.009005554020404816,\n",
       "    0.010012597776949406,\n",
       "    0.04513302445411682,\n",
       "    -0.03996916115283966,\n",
       "    -0.05106068775057793,\n",
       "    -0.03335968777537346,\n",
       "    -0.03976384177803993,\n",
       "    0.0300985649228096,\n",
       "    -0.009320302866399288,\n",
       "    0.007749408017843962,\n",
       "    0.0040068961679935455,\n",
       "    0.0126752695068717,\n",
       "    -0.007286017760634422,\n",
       "    -0.06379349529743195,\n",
       "    -0.02335672825574875,\n",
       "    -0.024715466424822807,\n",
       "    -0.015609756112098694,\n",
       "    -0.04227031394839287,\n",
       "    0.06827794760465622,\n",
       "    0.016676589846611023,\n",
       "    0.047177065163850784,\n",
       "    -0.0035761441104114056,\n",
       "    0.02046915702521801,\n",
       "    -0.0030867676250636578,\n",
       "    -0.01624840497970581,\n",
       "    -0.05002843216061592,\n",
       "    -0.0049226293340325356,\n",
       "    0.025981562212109566,\n",
       "    0.014169106259942055,\n",
       "    -0.06140174716711044,\n",
       "    -0.11447747051715851,\n",
       "    -0.01888675056397915,\n",
       "    0.011734779924154282,\n",
       "    -0.02456224150955677,\n",
       "    -0.03797826170921326,\n",
       "    0.04024959355592728,\n",
       "    0.010643023997545242,\n",
       "    -0.03059883788228035,\n",
       "    -0.038278792053461075,\n",
       "    0.021988922730088234,\n",
       "    0.037600889801979065,\n",
       "    0.005798473488539457,\n",
       "    0.059164609760046005,\n",
       "    -0.05642351880669594,\n",
       "    -0.00628235936164856,\n",
       "    0.05029341205954552,\n",
       "    -0.009426411241292953,\n",
       "    0.007925529032945633,\n",
       "    -0.014529744163155556,\n",
       "    0.02758382260799408,\n",
       "    0.0011672679102048278,\n",
       "    0.008453315123915672,\n",
       "    0.02347537688910961,\n",
       "    -0.006724216043949127,\n",
       "    0.012913526967167854,\n",
       "    -0.05142621323466301,\n",
       "    0.040453679859638214,\n",
       "    0.014146007597446442,\n",
       "    0.02165142260491848,\n",
       "    0.06602109968662262,\n",
       "    -0.024761414155364037,\n",
       "    0.004136813338845968,\n",
       "    0.03475387021899223,\n",
       "    0.004976203199476004,\n",
       "    0.04137200489640236,\n",
       "    0.030602457001805305,\n",
       "    -0.0865999311208725,\n",
       "    0.06299915164709091,\n",
       "    0.04042217135429382,\n",
       "    -0.019092941656708717,\n",
       "    -0.06676444411277771,\n",
       "    -0.010988704860210419,\n",
       "    0.06927398592233658,\n",
       "    -0.03410648927092552,\n",
       "    0.05176632106304169,\n",
       "    0.00696374149993062,\n",
       "    0.000979853211902082,\n",
       "    -0.03008519671857357,\n",
       "    -0.002183991251513362,\n",
       "    0.020917464047670364,\n",
       "    0.05570581182837486,\n",
       "    -0.0036588606890290976,\n",
       "    0.011243538931012154,\n",
       "    0.012576507404446602,\n",
       "    0.05081085488200188,\n",
       "    -0.003095656866207719,\n",
       "    0.012759125791490078,\n",
       "    -0.022207455709576607,\n",
       "    0.02410558983683586,\n",
       "    -0.03610306605696678,\n",
       "    -0.04237627610564232,\n",
       "    -0.09075170010328293,\n",
       "    0.04033561795949936,\n",
       "    -0.02238745242357254,\n",
       "    0.00826395396143198,\n",
       "    0.0049531483091413975,\n",
       "    0.024598030373454094,\n",
       "    -0.04518873989582062,\n",
       "    0.04118770733475685,\n",
       "    -0.030843881890177727,\n",
       "    -0.017632674425840378,\n",
       "    -0.05936505272984505,\n",
       "    -0.03088628686964512,\n",
       "    -0.04211476817727089,\n",
       "    -0.004396734293550253,\n",
       "    -0.05650852620601654,\n",
       "    -0.04228554666042328,\n",
       "    -0.026843419298529625,\n",
       "    0.010540333576500416,\n",
       "    0.045067984610795975,\n",
       "    0.003109560813754797,\n",
       "    0.011042953468859196,\n",
       "    -0.07115644216537476,\n",
       "    -0.028637006878852844,\n",
       "    -0.015206752344965935,\n",
       "    0.050512779504060745,\n",
       "    -0.020359938964247704,\n",
       "    -0.00747396145015955,\n",
       "    -0.007474882062524557,\n",
       "    -0.005225108936429024,\n",
       "    -0.012213820591568947,\n",
       "    -0.016705961897969246,\n",
       "    -0.04215404391288757,\n",
       "    -0.017395291477441788,\n",
       "    0.011769152246415615,\n",
       "    -0.0037994615267962217,\n",
       "    -0.025702424347400665,\n",
       "    -0.06845331192016602,\n",
       "    0.020313778892159462,\n",
       "    0.00036658684257417917,\n",
       "    -0.02452375553548336,\n",
       "    0.029715580865740776,\n",
       "    0.006583073176443577,\n",
       "    0.0611429326236248,\n",
       "    0.051164012402296066,\n",
       "    -0.0035399645566940308,\n",
       "    0.012590887024998665,\n",
       "    0.014657041057944298,\n",
       "    0.015573440119624138,\n",
       "    0.014325344935059547,\n",
       "    0.02745431661605835,\n",
       "    0.02197115682065487,\n",
       "    0.03292655944824219,\n",
       "    0.024111110717058182,\n",
       "    -0.06751292198896408,\n",
       "    -0.05045504868030548,\n",
       "    0.06353924423456192,\n",
       "    0.00643673911690712,\n",
       "    0.010098842903971672,\n",
       "    -0.03668275475502014,\n",
       "    -0.018185745924711227,\n",
       "    0.0002117723342962563,\n",
       "    0.009329434484243393,\n",
       "    -0.008158360607922077,\n",
       "    -0.021570837125182152,\n",
       "    0.019329393282532692,\n",
       "    0.008125909604132175,\n",
       "    0.015164189040660858,\n",
       "    0.05726585164666176,\n",
       "    -0.009541784413158894,\n",
       "    0.0061468868516385555,\n",
       "    -0.004457440227270126,\n",
       "    0.023716401308774948,\n",
       "    -0.06971192359924316,\n",
       "    0.028796060010790825,\n",
       "    0.01041675079613924,\n",
       "    -0.04553953930735588,\n",
       "    0.020738620311021805,\n",
       "    0.004832092206925154,\n",
       "    -0.03150299936532974,\n",
       "    -0.04429600015282631,\n",
       "    0.013790628872811794,\n",
       "    -0.053115058690309525,\n",
       "    -0.02560560405254364,\n",
       "    0.036688730120658875,\n",
       "    0.014488043263554573,\n",
       "    -0.009228067472577095,\n",
       "    -0.06419932097196579,\n",
       "    -0.012880503199994564,\n",
       "    -0.02107907459139824,\n",
       "    1.444513054593699e-05,\n",
       "    -0.0033198660239577293,\n",
       "    -0.024905933067202568,\n",
       "    -0.03108673356473446,\n",
       "    0.033358171582221985,\n",
       "    0.0009742958354763687,\n",
       "    -0.03336518630385399,\n",
       "    0.027100173756480217,\n",
       "    -0.009219962172210217,\n",
       "    -0.03113715350627899,\n",
       "    0.04466729238629341,\n",
       "    0.008119719102978706,\n",
       "    -0.0565766766667366,\n",
       "    0.0037011869717389345,\n",
       "    -0.012338224798440933,\n",
       "    -0.03370840102434158,\n",
       "    -0.0005041065742261708,\n",
       "    0.011166509240865707,\n",
       "    0.009698199108242989,\n",
       "    -0.0012518489966169,\n",
       "    0.029880018904805183,\n",
       "    -0.050972066819667816,\n",
       "    0.07752571254968643,\n",
       "    0.0021770054008811712,\n",
       "    0.10417456179857254,\n",
       "    -0.0009557668818160892,\n",
       "    0.0067534553818404675,\n",
       "    0.00782690104097128,\n",
       "    0.02586180530488491,\n",
       "    0.045426320284605026,\n",
       "    0.06360748410224915,\n",
       "    -0.03204941377043724,\n",
       "    0.002880049403756857,\n",
       "    0.02095639519393444,\n",
       "    -0.03027138113975525,\n",
       "    0.001833139336667955,\n",
       "    -0.02031288668513298,\n",
       "    0.03506040945649147,\n",
       "    0.05600018426775932,\n",
       "    0.03160380944609642,\n",
       "    -0.05123694986104965,\n",
       "    -0.0626993402838707,\n",
       "    0.01658344827592373,\n",
       "    -0.03829720988869667,\n",
       "    0.058923933655023575,\n",
       "    -0.02719443291425705,\n",
       "    -0.037641558796167374,\n",
       "    0.02953055500984192,\n",
       "    0.0038646492175757885,\n",
       "    -0.004005240276455879,\n",
       "    -0.09462728351354599,\n",
       "    -0.0652087852358818,\n",
       "    0.025428099557757378,\n",
       "    0.051540397107601166,\n",
       "    -0.008449424058198929,\n",
       "    0.02641494944691658,\n",
       "    -0.01028856448829174,\n",
       "    -0.0008263083873316646,\n",
       "    -0.010583778843283653,\n",
       "    0.006414654664695263,\n",
       "    -0.005061961244791746,\n",
       "    -0.04029203578829765,\n",
       "    -0.061678167432546616,\n",
       "    -0.005267309490591288,\n",
       "    -0.011394286528229713,\n",
       "    -0.02568669244647026,\n",
       "    -0.006238511763513088,\n",
       "    -0.030004238709807396,\n",
       "    -0.011689789593219757,\n",
       "    -0.010035756044089794,\n",
       "    -0.012819045223295689,\n",
       "    -0.027572203427553177,\n",
       "    0.047848884016275406,\n",
       "    -0.10166989266872406,\n",
       "    0.011310838162899017,\n",
       "    0.05754116177558899,\n",
       "    -0.02797839604318142,\n",
       "    -0.007163635920733213,\n",
       "    -0.0045363702811300755,\n",
       "    0.001722582383081317,\n",
       "    -0.026574403047561646,\n",
       "    0.03393394127488136,\n",
       "    0.030201716348528862,\n",
       "    0.05449223145842552,\n",
       "    -0.0014090642798691988,\n",
       "    0.0484597384929657,\n",
       "    -0.014405087567865849,\n",
       "    0.0635632649064064,\n",
       "    0.02740427292883396,\n",
       "    -0.032986439764499664,\n",
       "    -0.00026623960002325475,\n",
       "    0.06325620412826538,\n",
       "    -0.0749872699379921,\n",
       "    0.011908404529094696,\n",
       "    0.05931319668889046,\n",
       "    -0.009721901267766953,\n",
       "    -0.0029848364647477865,\n",
       "    0.04216653108596802,\n",
       "    0.03902079537510872,\n",
       "    -0.0011361106298863888,\n",
       "    -0.047263335436582565,\n",
       "    0.0013711013598367572,\n",
       "    -0.002253495389595628,\n",
       "    -0.06365787982940674,\n",
       "    0.03780275955796242,\n",
       "    0.09864509105682373,\n",
       "    -0.0921313688158989,\n",
       "    0.0361601896584034,\n",
       "    0.0005696952575817704,\n",
       "    0.035040974617004395,\n",
       "    -0.07377096265554428,\n",
       "    -0.0032924015540629625,\n",
       "    -0.016228212043642998,\n",
       "    -0.05882954224944115,\n",
       "    0.07859773933887482,\n",
       "    0.007806798908859491,\n",
       "    -0.08102353662252426,\n",
       "    -0.06597550958395004,\n",
       "    -0.05070755258202553,\n",
       "    -0.04055945947766304,\n",
       "    -0.02871568500995636,\n",
       "    0.012567104771733284,\n",
       "    -0.007313437294214964,\n",
       "    -0.007488269824534655,\n",
       "    0.023991720750927925,\n",
       "    0.008935095742344856,\n",
       "    -0.01046465802937746,\n",
       "    0.0021425914019346237,\n",
       "    0.007641823962330818,\n",
       "    0.009950722567737103,\n",
       "    -0.010274703614413738,\n",
       "    0.017408626154065132,\n",
       "    0.014523457735776901,\n",
       "    -0.06436450779438019,\n",
       "    0.1004868596792221,\n",
       "    -0.000903633888810873,\n",
       "    0.04568679630756378,\n",
       "    0.04622214660048485,\n",
       "    0.007708179764449596,\n",
       "    -0.008373342454433441,\n",
       "    -0.03050040826201439,\n",
       "    -0.020414335653185844,\n",
       "    -0.044397175312042236,\n",
       "    0.02221738174557686,\n",
       "    -0.032325614243745804,\n",
       "    0.014936499297618866,\n",
       "    0.08872969448566437,\n",
       "    0.04268105700612068,\n",
       "    -0.046038590371608734,\n",
       "    -0.03106744959950447,\n",
       "    -0.04355143383145332,\n",
       "    0.03068115934729576,\n",
       "    -0.02723471075296402,\n",
       "    0.030294595286250114,\n",
       "    -0.0006456805276684463,\n",
       "    -0.025102095678448677,\n",
       "    0.014653416350483894,\n",
       "    -0.055196356028318405,\n",
       "    0.030849432572722435,\n",
       "    -0.004917859565466642,\n",
       "    -0.005791716277599335,\n",
       "    0.015591038390994072,\n",
       "    -0.0031345393508672714,\n",
       "    0.05132317915558815,\n",
       "    0.04705221578478813,\n",
       "    -0.04342886805534363,\n",
       "    0.04300316795706749,\n",
       "    0.046058446168899536,\n",
       "    0.048040661960840225,\n",
       "    0.012944714166224003,\n",
       "    -0.011117047630250454,\n",
       "    -0.009213698096573353,\n",
       "    -0.05809669569134712,\n",
       "    0.021689631044864655,\n",
       "    0.006982988212257624,\n",
       "    0.06197545304894447,\n",
       "    0.02837219648063183,\n",
       "    -0.018559865653514862,\n",
       "    0.018082227557897568,\n",
       "    -0.08950725197792053,\n",
       "    -0.03263150900602341,\n",
       "    0.008956968784332275,\n",
       "    -0.014526150189340115,\n",
       "    -0.005484144669026136,\n",
       "    -0.015242813155055046,\n",
       "    0.06690023094415665,\n",
       "    0.018335523083806038,\n",
       "    0.02857264317572117,\n",
       "    -0.019645391032099724,\n",
       "    -0.04296684265136719,\n",
       "    -0.021879279986023903,\n",
       "    -0.00605639535933733,\n",
       "    -0.02949342504143715,\n",
       "    0.0328107587993145,\n",
       "    -0.017099875956773758,\n",
       "    -0.014169501140713692,\n",
       "    -0.016771145164966583,\n",
       "    -0.02702675573527813,\n",
       "    -0.007903637364506721,\n",
       "    -0.0012131552211940289,\n",
       "    -0.01654183119535446,\n",
       "    -0.07500816881656647,\n",
       "    0.012747488915920258,\n",
       "    -0.07625360041856766,\n",
       "    -0.02109275385737419,\n",
       "    0.059736207127571106,\n",
       "    0.017681820318102837,\n",
       "    0.023135708644986153,\n",
       "    -0.0527171790599823,\n",
       "    -0.0079732621088624,\n",
       "    0.04027407988905907,\n",
       "    0.06080561876296997,\n",
       "    -0.00024582273908890784,\n",
       "    -0.015182348899543285,\n",
       "    -0.03382405638694763,\n",
       "    0.01959913596510887,\n",
       "    0.06458045542240143,\n",
       "    0.031074633821845055,\n",
       "    -0.027559731155633926,\n",
       "    -0.01902710273861885,\n",
       "    0.02676854096353054,\n",
       "    -0.06168573349714279,\n",
       "    0.06287561357021332,\n",
       "    0.03229837864637375,\n",
       "    0.0267706960439682,\n",
       "    -0.014071136713027954,\n",
       "    -0.04835142567753792,\n",
       "    -0.0002685262297745794,\n",
       "    0.014674643985927105,\n",
       "    0.0429278202354908,\n",
       "    0.009557894431054592,\n",
       "    0.01411944068968296,\n",
       "    0.040344785898923874,\n",
       "    -0.017512237653136253,\n",
       "    0.0697597861289978,\n",
       "    -0.01871246099472046,\n",
       "    -0.01187979243695736,\n",
       "    -0.0334937609732151,\n",
       "    -0.06489583849906921,\n",
       "    -0.005144050344824791,\n",
       "    -0.0031802502926439047,\n",
       "    0.015644492581486702,\n",
       "    -0.05590880662202835,\n",
       "    -0.0012232032604515553,\n",
       "    0.019098447635769844,\n",
       "    0.027245230972766876,\n",
       "    -0.01354304701089859,\n",
       "    0.008252816274762154,\n",
       "    -0.005761743523180485,\n",
       "    0.034632325172424316,\n",
       "    0.004386072047054768,\n",
       "    0.03024035505950451,\n",
       "    -0.03132651746273041,\n",
       "    -0.0013931026915088296,\n",
       "    0.025102458894252777,\n",
       "    -0.044591765850782394,\n",
       "    0.022047078236937523,\n",
       "    0.01993469148874283,\n",
       "    -0.027287648990750313,\n",
       "    0.01605903171002865,\n",
       "    -0.027557380497455597,\n",
       "    0.07241769134998322,\n",
       "    0.07506473362445831,\n",
       "    0.021350864320993423,\n",
       "    -0.022647012025117874,\n",
       "    -0.0016555121401324868,\n",
       "    -0.017964234575629234,\n",
       "    0.0019138501957058907,\n",
       "    -0.04209174960851669,\n",
       "    0.03402373939752579,\n",
       "    -0.04082201421260834,\n",
       "    0.022594287991523743,\n",
       "    0.05462169647216797,\n",
       "    0.030491342768073082,\n",
       "    0.02596099115908146,\n",
       "    -0.04354478418827057,\n",
       "    -0.010473662056028843,\n",
       "    -0.005429483018815517,\n",
       "    0.003918552305549383,\n",
       "    0.052850812673568726,\n",
       "    -5.303304844530153e-33,\n",
       "    0.01702840067446232,\n",
       "    -0.056560955941677094,\n",
       "    0.04573400691151619,\n",
       "    0.06508587300777435,\n",
       "    -0.04285672307014465,\n",
       "    -0.05109775438904762,\n",
       "    0.050866223871707916,\n",
       "    0.04227638989686966,\n",
       "    -0.025974459946155548,\n",
       "    -0.01610616222023964,\n",
       "    -0.009718967601656914,\n",
       "    0.032903462648391724,\n",
       "    0.025739530101418495,\n",
       "    0.004229131154716015,\n",
       "    -0.011001212522387505,\n",
       "    0.05255405977368355,\n",
       "    -0.006634087767452002,\n",
       "    -0.005283951293677092,\n",
       "    0.018961679190397263,\n",
       "    -0.03759443014860153,\n",
       "    -0.12264592200517654,\n",
       "    0.03509067744016647,\n",
       "    -0.008380809798836708,\n",
       "    -0.05518479645252228,\n",
       "    0.010505560785531998,\n",
       "    -0.009315045550465584,\n",
       "    -0.00565839558839798,\n",
       "    -0.03348743915557861,\n",
       "    -0.03098953887820244,\n",
       "    0.020658018067479134,\n",
       "    0.01880752481520176,\n",
       "    0.00940119381994009,\n",
       "    0.008798932656645775,\n",
       "    -0.06605726480484009,\n",
       "    -0.004690034314990044,\n",
       "    -0.017345082014799118,\n",
       "    -0.04276911914348602,\n",
       "    0.012706020846962929,\n",
       "    0.06526324898004532,\n",
       "    0.056300584226846695,\n",
       "    0.01834196411073208,\n",
       "    -0.05255022272467613,\n",
       "    0.010545743629336357,\n",
       "    -0.018409432843327522,\n",
       "    -0.020507916808128357,\n",
       "    -0.0015346669824793935,\n",
       "    0.050874363631010056,\n",
       "    -0.03360476344823837,\n",
       "    0.004133853130042553,\n",
       "    0.011227321811020374,\n",
       "    0.021602688357234,\n",
       "    0.017476549372076988,\n",
       "    -0.08376026898622513,\n",
       "    0.03431857377290726,\n",
       "    0.05031701177358627,\n",
       "    0.04750784859061241,\n",
       "    -0.024411320686340332,\n",
       "    -0.004706140607595444,\n",
       "    0.01233531255275011,\n",
       "    0.0298208799213171,\n",
       "    -0.035119667649269104,\n",
       "    -0.01426283735781908,\n",
       "    0.031402356922626495,\n",
       "    0.000615061610005796,\n",
       "    0.03349127620458603,\n",
       "    0.013317774049937725,\n",
       "    0.0928422063589096,\n",
       "    -0.01126711443066597,\n",
       "    0.02454427443444729,\n",
       "    0.01920291595160961,\n",
       "    -0.04626567289233208,\n",
       "    0.018365828320384026,\n",
       "    -0.02355961687862873,\n",
       "    0.0051731448620557785,\n",
       "    -0.039166178554296494,\n",
       "    -0.014305955730378628,\n",
       "    0.0003413552767597139,\n",
       "    0.013759766705334187,\n",
       "    0.05689927563071251,\n",
       "    -0.05660371482372284,\n",
       "    -0.03517749905586243,\n",
       "    0.01338745467364788,\n",
       "    0.014829626306891441,\n",
       "    -0.023855460807681084,\n",
       "    -0.03497660532593727,\n",
       "    -0.05942613631486893,\n",
       "    -0.00938485935330391,\n",
       "    0.021183079108595848,\n",
       "    -0.0039022641722112894,\n",
       "    -0.07242384552955627,\n",
       "    -0.004741104785352945,\n",
       "    0.07137104868888855,\n",
       "    -0.01994631066918373,\n",
       "    -0.02913278341293335,\n",
       "    0.00778693612664938,\n",
       "    -0.07083778083324432,\n",
       "    0.008682136423885822,\n",
       "    -0.0014370795106515288,\n",
       "    0.018057657405734062,\n",
       "    0.04102323576807976,\n",
       "    -0.010502127930521965,\n",
       "    -0.01651713252067566,\n",
       "    -0.0453028678894043,\n",
       "    0.0013727961340919137,\n",
       "    0.04811263084411621,\n",
       "    0.03806470334529877,\n",
       "    -0.03737318143248558,\n",
       "    0.007716530002653599,\n",
       "    0.010669378563761711,\n",
       "    -0.008015112951397896,\n",
       "    0.00980397593230009,\n",
       "    -0.005007595289498568,\n",
       "    -0.014020075090229511,\n",
       "    0.06020569056272507,\n",
       "    -0.02610359527170658,\n",
       "    0.012772062793374062,\n",
       "    0.020639780908823013,\n",
       "    0.025241930037736893,\n",
       "    -0.03505292162299156,\n",
       "    -0.028304923325777054,\n",
       "    0.020991824567317963,\n",
       "    -0.07555881887674332,\n",
       "    -0.027660232037305832,\n",
       "    -0.025702092796564102,\n",
       "    -0.017589827999472618,\n",
       "    -0.006780896335840225,\n",
       "    -0.03141704201698303,\n",
       "    0.01874798722565174,\n",
       "    0.009514318779110909,\n",
       "    -0.0060359216295182705,\n",
       "    -0.032264988869428635,\n",
       "    0.04182039201259613,\n",
       "    2.681613580080011e-07,\n",
       "    -0.014299983158707619,\n",
       "    0.002939284546300769,\n",
       "    0.004445292521268129,\n",
       "    0.03197871521115303,\n",
       "    0.04121676832437515,\n",
       "    -0.025570368394255638,\n",
       "    -0.019838375970721245,\n",
       "    -0.010622522793710232,\n",
       "    -0.00897264014929533,\n",
       "    -0.00025921911583282053,\n",
       "    -0.0011481561232358217,\n",
       "    0.020900851115584373,\n",
       "    0.017441032454371452,\n",
       "    -0.024796366691589355,\n",
       "    -0.05471300706267357,\n",
       "    -0.08296633511781693,\n",
       "    -0.00997112225741148,\n",
       "    -0.033534422516822815,\n",
       "    -0.035346031188964844,\n",
       "    0.05051207169890404,\n",
       "    0.010406292043626308,\n",
       "    0.06468521803617477,\n",
       "    0.01503473799675703,\n",
       "    0.020156454294919968,\n",
       "    -0.013537248596549034,\n",
       "    -0.008642966859042645,\n",
       "    -0.050673943012952805,\n",
       "    -0.01924692466855049,\n",
       "    0.030434882268309593,\n",
       "    0.03130599856376648,\n",
       "    0.056672900915145874,\n",
       "    -0.01006452925503254,\n",
       "    0.029682589694857597,\n",
       "    0.016981903463602066,\n",
       "    0.00627155089750886,\n",
       "    -0.012158429250121117,\n",
       "    0.02561526745557785,\n",
       "    0.0695880874991417,\n",
       "    0.0034745077136904,\n",
       "    0.01579027622938156,\n",
       "    0.04564111307263374,\n",
       "    0.03411541134119034,\n",
       "    -0.01547867339104414,\n",
       "    -0.05036322399973869,\n",
       "    0.07243656367063522,\n",
       "    0.041025299578905106,\n",
       "    -0.05858024209737778,\n",
       "    -0.07631809264421463,\n",
       "    -0.012783370912075043,\n",
       "    -0.01840232126414776,\n",
       "    0.008342069573700428,\n",
       "    -0.017893338575959206,\n",
       "    0.055248767137527466,\n",
       "    0.04840861260890961,\n",
       "    0.002504949923604727,\n",
       "    -0.05759318172931671,\n",
       "    -0.02218037098646164,\n",
       "    0.0234922394156456,\n",
       "    0.005701913498342037,\n",
       "    -0.04539813846349716,\n",
       "    -0.022554881870746613,\n",
       "    -0.03350740298628807,\n",
       "    0.02447611652314663,\n",
       "    -0.040239367634058,\n",
       "    0.09340112656354904,\n",
       "    0.011206032708287239,\n",
       "    -0.0007736246334388852,\n",
       "    1.7423631853593927e-34,\n",
       "    0.006638782098889351,\n",
       "    0.0063453661277890205,\n",
       "    -0.01829923689365387,\n",
       "    0.049107443541288376,\n",
       "    0.03721340000629425,\n",
       "    -0.06219286471605301,\n",
       "    -0.03059951402246952,\n",
       "    -0.013291475363075733,\n",
       "    -0.006363479886204004,\n",
       "    0.06046830117702484,\n",
       "    -0.03094950132071972],\n",
       "   'chapter': 'CHAPTER 3',\n",
       "   'title': 'Technical Interview: Machine Learning Algorithms'},\n",
       "  '_explanation': {'value': 2.817597,\n",
       "   'description': 'sum of:',\n",
       "   'details': [{'value': 0.0040404093,\n",
       "     'description': 'weight(chapter:chapter in 5) [PerFieldSimilarity], result of:',\n",
       "     'details': [{'value': 0.0040404093,\n",
       "       'description': 'score(freq=1.0), computed as boost * idf * tf from:',\n",
       "       'details': [{'value': 2.2, 'description': 'boost', 'details': []},\n",
       "        {'value': 0.0040404093,\n",
       "         'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:',\n",
       "         'details': [{'value': 123,\n",
       "           'description': 'n, number of documents containing term',\n",
       "           'details': []},\n",
       "          {'value': 123,\n",
       "           'description': 'N, total number of documents with field',\n",
       "           'details': []}]},\n",
       "        {'value': 0.45454544,\n",
       "         'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:',\n",
       "         'details': [{'value': 1.0,\n",
       "           'description': 'freq, occurrences of term within document',\n",
       "           'details': []},\n",
       "          {'value': 1.2,\n",
       "           'description': 'k1, term saturation parameter',\n",
       "           'details': []},\n",
       "          {'value': 0.75,\n",
       "           'description': 'b, length normalization parameter',\n",
       "           'details': []},\n",
       "          {'value': 2.0, 'description': 'dl, length of field', 'details': []},\n",
       "          {'value': 2.0,\n",
       "           'description': 'avgdl, average length of field',\n",
       "           'details': []}]}]}]},\n",
       "    {'value': 1.9580805,\n",
       "     'description': 'weight(chapter:3 in 5) [PerFieldSimilarity], result of:',\n",
       "     'details': [{'value': 1.9580805,\n",
       "       'description': 'score(freq=1.0), computed as boost * idf * tf from:',\n",
       "       'details': [{'value': 2.2, 'description': 'boost', 'details': []},\n",
       "        {'value': 1.9580806,\n",
       "         'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:',\n",
       "         'details': [{'value': 17,\n",
       "           'description': 'n, number of documents containing term',\n",
       "           'details': []},\n",
       "          {'value': 123,\n",
       "           'description': 'N, total number of documents with field',\n",
       "           'details': []}]},\n",
       "        {'value': 0.45454544,\n",
       "         'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:',\n",
       "         'details': [{'value': 1.0,\n",
       "           'description': 'freq, occurrences of term within document',\n",
       "           'details': []},\n",
       "          {'value': 1.2,\n",
       "           'description': 'k1, term saturation parameter',\n",
       "           'details': []},\n",
       "          {'value': 0.75,\n",
       "           'description': 'b, length normalization parameter',\n",
       "           'details': []},\n",
       "          {'value': 2.0, 'description': 'dl, length of field', 'details': []},\n",
       "          {'value': 2.0,\n",
       "           'description': 'avgdl, average length of field',\n",
       "           'details': []}]}]}]}]}},\n",
       " {'_shard': '[ml-interview-questions][0]',\n",
       "  '_node': 'EScPTzvSSEeYjH0xRE7sig',\n",
       "  '_index': 'ml-interview-questions',\n",
       "  '_id': '3bez-ZEBwj93mx26upk6',\n",
       "  '_score': 1.9621209,\n",
       "  '_source': {'section': 'Overview of the Machine Learning Algorithms Technical Interview',\n",
       "   'chapter': 'CHAPTER 3',\n",
       "   'title': 'Technical Interview: Machine Learning Algorithms'},\n",
       "  '_explanation': {'value': 1.9621209,\n",
       "   'description': 'sum of:',\n",
       "   'details': [{'value': 0.0040404093,\n",
       "     'description': 'weight(chapter:chapter in 6) [PerFieldSimilarity], result of:',\n",
       "     'details': [{'value': 0.0040404093,\n",
       "       'description': 'score(freq=1.0), computed as boost * idf * tf from:',\n",
       "       'details': [{'value': 2.2, 'description': 'boost', 'details': []},\n",
       "        {'value': 0.0040404093,\n",
       "         'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:',\n",
       "         'details': [{'value': 123,\n",
       "           'description': 'n, number of documents containing term',\n",
       "           'details': []},\n",
       "          {'value': 123,\n",
       "           'description': 'N, total number of documents with field',\n",
       "           'details': []}]},\n",
       "        {'value': 0.45454544,\n",
       "         'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:',\n",
       "         'details': [{'value': 1.0,\n",
       "           'description': 'freq, occurrences of term within document',\n",
       "           'details': []},\n",
       "          {'value': 1.2,\n",
       "           'description': 'k1, term saturation parameter',\n",
       "           'details': []},\n",
       "          {'value': 0.75,\n",
       "           'description': 'b, length normalization parameter',\n",
       "           'details': []},\n",
       "          {'value': 2.0, 'description': 'dl, length of field', 'details': []},\n",
       "          {'value': 2.0,\n",
       "           'description': 'avgdl, average length of field',\n",
       "           'details': []}]}]}]},\n",
       "    {'value': 1.9580805,\n",
       "     'description': 'weight(chapter:3 in 6) [PerFieldSimilarity], result of:',\n",
       "     'details': [{'value': 1.9580805,\n",
       "       'description': 'score(freq=1.0), computed as boost * idf * tf from:',\n",
       "       'details': [{'value': 2.2, 'description': 'boost', 'details': []},\n",
       "        {'value': 1.9580806,\n",
       "         'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:',\n",
       "         'details': [{'value': 17,\n",
       "           'description': 'n, number of documents containing term',\n",
       "           'details': []},\n",
       "          {'value': 123,\n",
       "           'description': 'N, total number of documents with field',\n",
       "           'details': []}]},\n",
       "        {'value': 0.45454544,\n",
       "         'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:',\n",
       "         'details': [{'value': 1.0,\n",
       "           'description': 'freq, occurrences of term within document',\n",
       "           'details': []},\n",
       "          {'value': 1.2,\n",
       "           'description': 'k1, term saturation parameter',\n",
       "           'details': []},\n",
       "          {'value': 0.75,\n",
       "           'description': 'b, length normalization parameter',\n",
       "           'details': []},\n",
       "          {'value': 2.0, 'description': 'dl, length of field', 'details': []},\n",
       "          {'value': 2.0,\n",
       "           'description': 'avgdl, average length of field',\n",
       "           'details': []}]}]}]}]}},\n",
       " {'_shard': '[ml-interview-questions][0]',\n",
       "  '_node': 'EScPTzvSSEeYjH0xRE7sig',\n",
       "  '_index': 'ml-interview-questions',\n",
       "  '_id': '3rez-ZEBwj93mx26uplV',\n",
       "  '_score': 1.9621209,\n",
       "  '_source': {'text': 'You’re likely to be asked ML algorithm technical questions in an interview if you’re applying for any of the following jobs: • Data scientist who builds ML models • Machine learning engineer • Applied scientist • And similar roles Recall that within the common ML job titles ( Figure 1-8 ), there are some jobs that have the responsibility of training ML models in the ML lifecycle. This chapter focu‐ ses on assessing candidates for those skills; if the job you’re aiming for focuses less on training ML models, you might get a simplified version of this type of interview, or it might be skipped completely. This interview is meant to assess your understanding of ML algorithms, especially on the theoretical side. As to how you implement the algorithms with code, I cover that in the model deployment questions in Chapter 6 and the coding/programming tech‐ nical interview in Chapter 5 . The goal for you as an interviewee is for the interviewers to confirm that you understand the underlying concepts behind ML algorithms. Roles do exist where all you have to know is how to import the library with Python, but for more advanced projects, an underlying understanding can help you custom‐ ize various ML approaches and better debug and troubleshoot models. As covered in Chapter 1 , in the three pillars of ML roles, this is the pillar of ML algorithm and data intuition, which showcases your ability to adapt (refer to Figure 1-6 ). This skill is especially important in companies that have complex ML use cases and custom-made solutions, where you might modify or combine various off-the-shelf methods or cre‐ ate something from scratch. I try to mention as many common algorithms as space allows, but there are many more techniques under the sun. Be sure to check out the linked resources to extend your learning and interview preparation! It is also important to note that, in addition to understanding the ML algorithms’ inner workings and underlying statistical methods, you need to successfully commu‐ nicate that understanding to the interviewer. Yes, I know that communication skills have been brought up many times in this book, but they are what help set you apart as a successful candidate. As a rule of thumb, it’s important to be able to explain algorithms and ML concepts at two levels: on a simple “explain like I’m five years old” level and at a deeper, tech‐ nical level, one more appropriate for a college course. A second rule of thumb is to be prepared to answer follow-up questions to these ML algorithm interview questions. This is so the interviewer knows that you didn’t just memorize and then regurgitate the answer, but that you can apply it to various real-life scenarios on the job. In this chapter, I break down technical questions on the following topics so you can easily refer to a specific question if your interview focuses on that topic: • Statistical techniques • Supervised, unsupervised, and reinforcement learning • Natural language processing (NLP) • Recommender systems • Reinforcement learning • Computer vision In technical interviews that are very structured, such as the Ama‐ zon data science initial phone screen, they will ask you clearly scoped questions, such as asking for a definition of a particular algorithm. After you answer, they will generally move on without additional follow-up questions. There are companies that mix structured questions with a free-form discussion, where the inter‐ viewer might dig deeper into your answer, and the conversation might branch out from there into your past experiences.',\n",
       "   'text_vector': [0.05993577092885971,\n",
       "    -0.05391945317387581,\n",
       "    -0.04208781570196152,\n",
       "    0.017544545233249664,\n",
       "    -0.002861324232071638,\n",
       "    0.041564736515283585,\n",
       "    -0.04502599686384201,\n",
       "    -0.03485346958041191,\n",
       "    0.02959667146205902,\n",
       "    -0.04890763759613037,\n",
       "    0.04734601825475693,\n",
       "    0.04816044867038727,\n",
       "    0.020772868767380714,\n",
       "    0.059839390218257904,\n",
       "    0.02621373161673546,\n",
       "    -0.03506791219115257,\n",
       "    0.0030805773567408323,\n",
       "    -0.008104348555207253,\n",
       "    -0.02095099724829197,\n",
       "    -0.005705469287931919,\n",
       "    -0.018839184194803238,\n",
       "    0.0153201250359416,\n",
       "    0.010476820170879364,\n",
       "    -0.007886525243520737,\n",
       "    -0.0054871258325874805,\n",
       "    -0.03927190974354744,\n",
       "    -0.05143311992287636,\n",
       "    0.028323808684945107,\n",
       "    -0.027788985520601273,\n",
       "    0.020367588847875595,\n",
       "    -0.030829526484012604,\n",
       "    -0.032792653888463974,\n",
       "    0.0036351655144244432,\n",
       "    -0.014580332674086094,\n",
       "    2.306577471244964e-06,\n",
       "    -0.018277574330568314,\n",
       "    -0.0208209827542305,\n",
       "    0.03600199148058891,\n",
       "    0.007936419919133186,\n",
       "    -0.10433556139469147,\n",
       "    0.039740514010190964,\n",
       "    -0.01752186007797718,\n",
       "    0.05168933421373367,\n",
       "    0.015967221930623055,\n",
       "    -0.04609043896198273,\n",
       "    0.043641407042741776,\n",
       "    0.07432569563388824,\n",
       "    -0.017735924571752548,\n",
       "    0.05891294777393341,\n",
       "    0.041267551481723785,\n",
       "    0.016758892685174942,\n",
       "    -0.0710165724158287,\n",
       "    0.010138644836843014,\n",
       "    0.10137741267681122,\n",
       "    -0.03521575778722763,\n",
       "    0.016337063163518906,\n",
       "    0.02317001298069954,\n",
       "    0.05400419235229492,\n",
       "    0.010251354426145554,\n",
       "    -0.07366803288459778,\n",
       "    0.022489724680781364,\n",
       "    0.009480347856879234,\n",
       "    0.04316667094826698,\n",
       "    0.036297887563705444,\n",
       "    0.07358944416046143,\n",
       "    -0.0030124688055366278,\n",
       "    -0.01601889170706272,\n",
       "    -0.0353853739798069,\n",
       "    -0.04866044595837593,\n",
       "    -3.4659540233406005e-06,\n",
       "    0.030104078352451324,\n",
       "    -0.07502267509698868,\n",
       "    -0.010263746604323387,\n",
       "    0.0007292333757504821,\n",
       "    0.045224033296108246,\n",
       "    -0.02872268110513687,\n",
       "    -0.025965502485632896,\n",
       "    -0.0164458230137825,\n",
       "    0.027926500886678696,\n",
       "    -0.030829666182398796,\n",
       "    -0.01192169077694416,\n",
       "    -0.007750106509774923,\n",
       "    -0.005981812719255686,\n",
       "    -0.0013405110221356153,\n",
       "    -0.015932733193039894,\n",
       "    0.07468276470899582,\n",
       "    -0.0268633421510458,\n",
       "    -0.02454020082950592,\n",
       "    0.017150944098830223,\n",
       "    -0.017291303724050522,\n",
       "    0.08549156785011292,\n",
       "    0.009127943776547909,\n",
       "    0.053818706423044205,\n",
       "    0.0978497713804245,\n",
       "    -0.030410291627049446,\n",
       "    0.016753489151597023,\n",
       "    0.008483332581818104,\n",
       "    0.007069938350468874,\n",
       "    -0.04653380811214447,\n",
       "    -0.04721872881054878,\n",
       "    -0.019372476264834404,\n",
       "    -0.012400536797940731,\n",
       "    0.034717459231615067,\n",
       "    -0.0043710023164749146,\n",
       "    0.014921296387910843,\n",
       "    0.0026266395580023527,\n",
       "    -0.011007481254637241,\n",
       "    0.012630293145775795,\n",
       "    -0.03945392370223999,\n",
       "    -0.053298793733119965,\n",
       "    -0.02690868265926838,\n",
       "    -0.02548239938914776,\n",
       "    -0.011483878828585148,\n",
       "    0.041013166308403015,\n",
       "    0.009593886323273182,\n",
       "    0.05729550123214722,\n",
       "    0.020113874226808548,\n",
       "    0.04357387498021126,\n",
       "    -0.002971044508740306,\n",
       "    -0.01951770856976509,\n",
       "    -0.04815621301531792,\n",
       "    -0.008002993650734425,\n",
       "    0.06028402969241142,\n",
       "    0.00944421999156475,\n",
       "    -0.02250523306429386,\n",
       "    -0.1133100688457489,\n",
       "    -0.0030280649662017822,\n",
       "    -0.016901895403862,\n",
       "    -0.004208337981253862,\n",
       "    -0.0201493501663208,\n",
       "    0.05163604021072388,\n",
       "    0.0307147316634655,\n",
       "    -0.01903790980577469,\n",
       "    -0.003076596651226282,\n",
       "    0.056388143450021744,\n",
       "    0.018235811963677406,\n",
       "    0.009810620918869972,\n",
       "    0.05104219168424606,\n",
       "    -0.06376949697732925,\n",
       "    -0.02144075185060501,\n",
       "    0.028959348797798157,\n",
       "    -0.0544670969247818,\n",
       "    0.023991452530026436,\n",
       "    -0.03641391918063164,\n",
       "    0.015026185661554337,\n",
       "    0.010897352360188961,\n",
       "    -0.03244491294026375,\n",
       "    -0.01687229797244072,\n",
       "    -0.0002178361901314929,\n",
       "    0.011289150454103947,\n",
       "    -0.06343404203653336,\n",
       "    0.016163624823093414,\n",
       "    0.01032592635601759,\n",
       "    0.029909729957580566,\n",
       "    0.03334677219390869,\n",
       "    -0.0011833147145807743,\n",
       "    -0.015152734704315662,\n",
       "    0.0714680552482605,\n",
       "    0.017896126955747604,\n",
       "    0.015749327838420868,\n",
       "    0.0026956640649586916,\n",
       "    -0.08691760897636414,\n",
       "    0.028267212212085724,\n",
       "    0.019728438928723335,\n",
       "    -0.008056264370679855,\n",
       "    -0.052956853061914444,\n",
       "    -0.013408202677965164,\n",
       "    0.07618243247270584,\n",
       "    -0.029021495953202248,\n",
       "    0.029180333018302917,\n",
       "    0.00249526952393353,\n",
       "    0.00178820441942662,\n",
       "    -0.008166523650288582,\n",
       "    -0.0206541009247303,\n",
       "    -0.021456651389598846,\n",
       "    0.02576206438243389,\n",
       "    0.01771801896393299,\n",
       "    0.011412512511014938,\n",
       "    0.01657455414533615,\n",
       "    0.03267432749271393,\n",
       "    0.010272804647684097,\n",
       "    0.0319068543612957,\n",
       "    -0.008611908182501793,\n",
       "    0.04399814084172249,\n",
       "    -0.029559556394815445,\n",
       "    -0.030444510281085968,\n",
       "    -0.07995172590017319,\n",
       "    0.05623342841863632,\n",
       "    -0.003955999854952097,\n",
       "    -0.02993065118789673,\n",
       "    0.007692529819905758,\n",
       "    0.006042165216058493,\n",
       "    0.029032841324806213,\n",
       "    0.03565121442079544,\n",
       "    -0.016098299995064735,\n",
       "    -0.022455161437392235,\n",
       "    -0.04448382183909416,\n",
       "    -0.01865617372095585,\n",
       "    -0.06115516275167465,\n",
       "    0.0010106650879606605,\n",
       "    -0.06868947297334671,\n",
       "    -0.0458696112036705,\n",
       "    -0.05172445625066757,\n",
       "    0.015783187001943588,\n",
       "    0.05050439387559891,\n",
       "    -0.009645971469581127,\n",
       "    0.001623976626433432,\n",
       "    -0.0382707417011261,\n",
       "    -0.033755600452423096,\n",
       "    -0.04780231788754463,\n",
       "    0.02130255103111267,\n",
       "    -0.03061540797352791,\n",
       "    -0.06319183111190796,\n",
       "    0.0010918228654190898,\n",
       "    -0.0012680012732744217,\n",
       "    -0.007369408383965492,\n",
       "    -0.012422468513250351,\n",
       "    -0.06171559914946556,\n",
       "    0.02558796852827072,\n",
       "    -0.007650704123079777,\n",
       "    -0.011241782456636429,\n",
       "    -0.00856319535523653,\n",
       "    -0.05983719229698181,\n",
       "    0.02328561805188656,\n",
       "    -0.017166275531053543,\n",
       "    0.008013581857085228,\n",
       "    0.00467833923175931,\n",
       "    -0.005270530469715595,\n",
       "    0.04493091627955437,\n",
       "    0.05974894389510155,\n",
       "    0.02762911096215248,\n",
       "    0.021947922185063362,\n",
       "    0.009479965083301067,\n",
       "    0.04670926183462143,\n",
       "    -0.005805241875350475,\n",
       "    -0.00601569889113307,\n",
       "    0.0005478634848259389,\n",
       "    0.03909299150109291,\n",
       "    0.0354221947491169,\n",
       "    -0.031512949615716934,\n",
       "    -0.05738512799143791,\n",
       "    0.052931029349565506,\n",
       "    -0.009274779818952084,\n",
       "    0.013796319253742695,\n",
       "    -0.034763891249895096,\n",
       "    -0.024308128282427788,\n",
       "    -0.015620075166225433,\n",
       "    0.003542307298630476,\n",
       "    -0.0044745649211108685,\n",
       "    -0.049325548112392426,\n",
       "    0.009378290735185146,\n",
       "    0.012563003227114677,\n",
       "    -0.01178928092122078,\n",
       "    0.049020953476428986,\n",
       "    -0.03744195029139519,\n",
       "    0.026297423988580704,\n",
       "    0.021261204034090042,\n",
       "    0.025703465566039085,\n",
       "    -0.027882926166057587,\n",
       "    -0.009657630696892738,\n",
       "    0.026188263669610023,\n",
       "    -0.030565543100237846,\n",
       "    0.03216014802455902,\n",
       "    0.018007267266511917,\n",
       "    -0.014501531608402729,\n",
       "    -0.04228790104389191,\n",
       "    0.03544946759939194,\n",
       "    -0.03698614239692688,\n",
       "    -0.03924060985445976,\n",
       "    0.012816576287150383,\n",
       "    0.014563743025064468,\n",
       "    0.023837367072701454,\n",
       "    -0.05846910551190376,\n",
       "    -0.012787357904016972,\n",
       "    -0.02741323783993721,\n",
       "    -0.020585916936397552,\n",
       "    -0.001209129812195897,\n",
       "    -0.05252275615930557,\n",
       "    -0.01353229396045208,\n",
       "    0.02691379189491272,\n",
       "    0.0011114790104329586,\n",
       "    -0.050979144871234894,\n",
       "    0.04675965756177902,\n",
       "    -0.0029980253893882036,\n",
       "    -0.061511848121881485,\n",
       "    0.035548027604818344,\n",
       "    0.002458383096382022,\n",
       "    -0.018027696758508682,\n",
       "    -0.026821397244930267,\n",
       "    4.712527879746631e-05,\n",
       "    -0.01787043921649456,\n",
       "    -0.005689395125955343,\n",
       "    0.017084216699004173,\n",
       "    0.013610954396426678,\n",
       "    -0.0001286293991142884,\n",
       "    0.033829305320978165,\n",
       "    -0.03355007246136665,\n",
       "    0.0557778924703598,\n",
       "    -0.029652204364538193,\n",
       "    0.02846684120595455,\n",
       "    0.027290843427181244,\n",
       "    -0.005436788313090801,\n",
       "    0.019736172631382942,\n",
       "    0.032048944383859634,\n",
       "    0.04577649012207985,\n",
       "    0.05548853799700737,\n",
       "    -0.018594011664390564,\n",
       "    0.008174204267561436,\n",
       "    0.007912821136415005,\n",
       "    -0.008682255633175373,\n",
       "    -0.006157692521810532,\n",
       "    -0.03717422857880592,\n",
       "    0.024356331676244736,\n",
       "    0.06748518347740173,\n",
       "    0.04238046333193779,\n",
       "    -0.054608304053545,\n",
       "    -0.03292936831712723,\n",
       "    0.01873568817973137,\n",
       "    -0.014274615794420242,\n",
       "    0.07546140998601913,\n",
       "    0.004782046657055616,\n",
       "    -0.04863451421260834,\n",
       "    0.047969575971364975,\n",
       "    0.00390136637724936,\n",
       "    -0.03183310851454735,\n",
       "    -0.08540407568216324,\n",
       "    -0.053729984909296036,\n",
       "    0.003996640909463167,\n",
       "    0.05064919590950012,\n",
       "    0.001986056799069047,\n",
       "    0.02963203564286232,\n",
       "    0.012075965292751789,\n",
       "    0.008922475390136242,\n",
       "    -0.025153566151857376,\n",
       "    0.008491539396345615,\n",
       "    -0.02616213448345661,\n",
       "    -0.037500910460948944,\n",
       "    -0.03089454397559166,\n",
       "    0.0030680010095238686,\n",
       "    -0.0204415712505579,\n",
       "    -0.00427921861410141,\n",
       "    0.009458516724407673,\n",
       "    -0.04305867850780487,\n",
       "    0.004741364624351263,\n",
       "    -0.006667343899607658,\n",
       "    0.03224755451083183,\n",
       "    -0.04373624175786972,\n",
       "    0.026834916323423386,\n",
       "    -0.014308436773717403,\n",
       "    0.005499574821442366,\n",
       "    0.04331716522574425,\n",
       "    -0.05446040257811546,\n",
       "    0.013915828429162502,\n",
       "    -0.0008997956174425781,\n",
       "    0.020016750320792198,\n",
       "    -0.02286701090633869,\n",
       "    0.025071611627936363,\n",
       "    0.026579203084111214,\n",
       "    0.041532211005687714,\n",
       "    0.09337524324655533,\n",
       "    0.05580047518014908,\n",
       "    -0.012136666104197502,\n",
       "    0.025789368897676468,\n",
       "    -0.005617617163807154,\n",
       "    -0.09415501356124878,\n",
       "    0.007408094592392445,\n",
       "    0.034514810889959335,\n",
       "    -0.08612942695617676,\n",
       "    0.04577364772558212,\n",
       "    0.064839668571949,\n",
       "    -0.015150727704167366,\n",
       "    -0.02587071992456913,\n",
       "    0.042271438986063004,\n",
       "    -0.009953559376299381,\n",
       "    0.02992803044617176,\n",
       "    -0.0036049233749508858,\n",
       "    -0.015037626028060913,\n",
       "    -0.001692193211056292,\n",
       "    -0.04068995267152786,\n",
       "    0.03311828896403313,\n",
       "    0.11041078716516495,\n",
       "    -0.09891027957201004,\n",
       "    -0.0038298945873975754,\n",
       "    -0.003567442763596773,\n",
       "    0.0008514173678122461,\n",
       "    -0.06327812373638153,\n",
       "    -0.0019446437945589423,\n",
       "    -0.013977233320474625,\n",
       "    -0.031722359359264374,\n",
       "    0.05255626514554024,\n",
       "    0.011341225355863571,\n",
       "    -0.018325481563806534,\n",
       "    -0.09305328130722046,\n",
       "    -0.04385515674948692,\n",
       "    -0.021065441891551018,\n",
       "    -0.03270413354039192,\n",
       "    0.028510747477412224,\n",
       "    0.03319522738456726,\n",
       "    0.033509768545627594,\n",
       "    0.010342761874198914,\n",
       "    -0.01136382482945919,\n",
       "    -0.04447039216756821,\n",
       "    -0.0010092954616993666,\n",
       "    0.01844889298081398,\n",
       "    0.0675230324268341,\n",
       "    -0.027409326285123825,\n",
       "    0.04122979938983917,\n",
       "    0.008118131197988987,\n",
       "    -0.036129795014858246,\n",
       "    0.05665532499551773,\n",
       "    -0.00499502569437027,\n",
       "    0.06812010705471039,\n",
       "    0.05219460651278496,\n",
       "    -0.010073339566588402,\n",
       "    -0.014728322625160217,\n",
       "    -0.0167916938662529,\n",
       "    -0.004374856129288673,\n",
       "    -0.03333187848329544,\n",
       "    0.03307511284947395,\n",
       "    -0.03677910566329956,\n",
       "    -0.005631446838378906,\n",
       "    0.052848074585199356,\n",
       "    0.05438699945807457,\n",
       "    -0.035300251096487045,\n",
       "    -0.03587113693356514,\n",
       "    -0.03251694515347481,\n",
       "    0.005911991931498051,\n",
       "    -0.005745507311075926,\n",
       "    0.0320640429854393,\n",
       "    0.01940460316836834,\n",
       "    0.0030746629927307367,\n",
       "    0.01862635277211666,\n",
       "    -0.02828310616314411,\n",
       "    0.015341688878834248,\n",
       "    -0.0021174391731619835,\n",
       "    -0.02391892857849598,\n",
       "    0.0062637547962367535,\n",
       "    0.019581161439418793,\n",
       "    0.05508286878466606,\n",
       "    0.03221283107995987,\n",
       "    -0.023399323225021362,\n",
       "    0.0410117469727993,\n",
       "    -0.04265822097659111,\n",
       "    0.029823128134012222,\n",
       "    -0.008005606010556221,\n",
       "    0.0273310374468565,\n",
       "    -0.036690112203359604,\n",
       "    -0.054626401513814926,\n",
       "    0.01336683239787817,\n",
       "    0.0017563584260642529,\n",
       "    0.044320836663246155,\n",
       "    0.06189727038145065,\n",
       "    -0.0518164187669754,\n",
       "    0.04654927924275398,\n",
       "    -0.06478781998157501,\n",
       "    -0.023027317598462105,\n",
       "    -0.02212967723608017,\n",
       "    -0.003679978894069791,\n",
       "    -0.003016661386936903,\n",
       "    -0.01085482258349657,\n",
       "    0.03977006673812866,\n",
       "    0.002247579861432314,\n",
       "    0.04523169621825218,\n",
       "    -0.043138716369867325,\n",
       "    -0.025353819131851196,\n",
       "    -0.02551446668803692,\n",
       "    -0.012033957056701183,\n",
       "    -0.03725782036781311,\n",
       "    -0.007928349077701569,\n",
       "    0.0004430134140420705,\n",
       "    -0.0026307604275643826,\n",
       "    -0.01326088048517704,\n",
       "    -0.03453647717833519,\n",
       "    0.0013453749706968665,\n",
       "    0.006465521175414324,\n",
       "    -0.018370239064097404,\n",
       "    -0.029575781896710396,\n",
       "    0.003943406045436859,\n",
       "    -0.026192499324679375,\n",
       "    0.009102510288357735,\n",
       "    0.07205945998430252,\n",
       "    -0.0023073353804647923,\n",
       "    -0.0059700473211705685,\n",
       "    -0.0841856449842453,\n",
       "    -0.017623718827962875,\n",
       "    0.022123027592897415,\n",
       "    0.08713827282190323,\n",
       "    -0.012247614562511444,\n",
       "    -0.03296934440732002,\n",
       "    -0.021746641024947166,\n",
       "    -0.007418270222842693,\n",
       "    0.04136619344353676,\n",
       "    0.06717617064714432,\n",
       "    -0.08875633776187897,\n",
       "    -0.05366329848766327,\n",
       "    -0.0558367520570755,\n",
       "    -0.003470067633315921,\n",
       "    0.0767635777592659,\n",
       "    0.014793012291193008,\n",
       "    0.027478082105517387,\n",
       "    -0.006460086442530155,\n",
       "    -0.041930969804525375,\n",
       "    -0.002409784821793437,\n",
       "    0.0004270104691386223,\n",
       "    0.03365336358547211,\n",
       "    -0.010113412514328957,\n",
       "    0.026774859055876732,\n",
       "    0.05500324070453644,\n",
       "    -0.01112450659275055,\n",
       "    0.04466130584478378,\n",
       "    -0.02262239158153534,\n",
       "    0.017057405784726143,\n",
       "    0.006867766845971346,\n",
       "    -0.03996741399168968,\n",
       "    -0.011655066162347794,\n",
       "    -0.018929189071059227,\n",
       "    0.01034036185592413,\n",
       "    -0.0038934543263167143,\n",
       "    -0.05759424716234207,\n",
       "    -0.011592062190175056,\n",
       "    0.024933474138379097,\n",
       "    0.010807954706251621,\n",
       "    -0.0069776782765984535,\n",
       "    -0.005839897785335779,\n",
       "    0.06343044340610504,\n",
       "    0.002327922498807311,\n",
       "    0.019114816561341286,\n",
       "    -0.049885787069797516,\n",
       "    -0.02309996262192726,\n",
       "    0.01848038285970688,\n",
       "    -0.019487766548991203,\n",
       "    0.009689359925687313,\n",
       "    0.00975723285228014,\n",
       "    -0.04209696128964424,\n",
       "    0.03587847203016281,\n",
       "    -0.005190533585846424,\n",
       "    0.08579230308532715,\n",
       "    0.04956401139497757,\n",
       "    -0.020616646856069565,\n",
       "    -0.020491894334554672,\n",
       "    -9.416192915523425e-05,\n",
       "    -0.05641939118504524,\n",
       "    -0.018611639738082886,\n",
       "    -0.036973364651203156,\n",
       "    0.03063562884926796,\n",
       "    -0.024205969646573067,\n",
       "    0.03661487251520157,\n",
       "    0.05673282593488693,\n",
       "    -0.004076502285897732,\n",
       "    0.0500049814581871,\n",
       "    -0.03081602044403553,\n",
       "    -0.018356995657086372,\n",
       "    0.014984437264502048,\n",
       "    -0.026441847905516624,\n",
       "    0.02377171255648136,\n",
       "    -5.549448953488524e-33,\n",
       "    0.01239333488047123,\n",
       "    -0.057521503418684006,\n",
       "    0.06717737019062042,\n",
       "    0.0680851861834526,\n",
       "    0.02260686829686165,\n",
       "    -0.05317200720310211,\n",
       "    0.042564067989587784,\n",
       "    0.042860809713602066,\n",
       "    -0.06277427077293396,\n",
       "    -0.01589544489979744,\n",
       "    0.02844943106174469,\n",
       "    0.01964864879846573,\n",
       "    0.0002799423527903855,\n",
       "    -0.01957213319838047,\n",
       "    -0.022903716191649437,\n",
       "    0.010073741897940636,\n",
       "    -0.047704312950372696,\n",
       "    -0.004744244273751974,\n",
       "    0.014411794021725655,\n",
       "    -0.03411859646439552,\n",
       "    -0.10801413655281067,\n",
       "    0.015253081917762756,\n",
       "    -0.020603014156222343,\n",
       "    -0.057520508766174316,\n",
       "    0.043438851833343506,\n",
       "    0.0043800584971904755,\n",
       "    0.007122566923499107,\n",
       "    0.0018238702323287725,\n",
       "    -0.05229131877422333,\n",
       "    0.03859343007206917,\n",
       "    0.020271189510822296,\n",
       "    0.004761361517012119,\n",
       "    0.0030847780872136354,\n",
       "    -0.06521423906087875,\n",
       "    -0.015419901348650455,\n",
       "    0.06322918832302094,\n",
       "    -0.01671646535396576,\n",
       "    0.011693081818521023,\n",
       "    0.047780122607946396,\n",
       "    0.059144970029592514,\n",
       "    0.006453026086091995,\n",
       "    -0.031581539660692215,\n",
       "    0.05880031734704971,\n",
       "    -0.007230551913380623,\n",
       "    -0.024975255131721497,\n",
       "    -0.014322281815111637,\n",
       "    0.03447920083999634,\n",
       "    -0.025666305795311928,\n",
       "    0.03816276416182518,\n",
       "    0.010934993624687195,\n",
       "    0.024891266599297523,\n",
       "    0.028975753113627434,\n",
       "    -0.08036989718675613,\n",
       "    0.013232114724814892,\n",
       "    0.06195138767361641,\n",
       "    0.04292212426662445,\n",
       "    -0.03426282852888107,\n",
       "    0.03269559517502785,\n",
       "    0.006005424540489912,\n",
       "    0.0008267576340585947,\n",
       "    -0.01433753501623869,\n",
       "    -0.05257308483123779,\n",
       "    0.06654360890388489,\n",
       "    -0.019356420263648033,\n",
       "    0.03146854043006897,\n",
       "    -0.024668611586093903,\n",
       "    0.06244141235947609,\n",
       "    -0.043590568006038666,\n",
       "    0.028514323756098747,\n",
       "    0.02715054526925087,\n",
       "    -0.031581442803144455,\n",
       "    0.021475771442055702,\n",
       "    -0.012741453014314175,\n",
       "    -0.0030736918561160564,\n",
       "    -0.012239676900207996,\n",
       "    0.0486503504216671,\n",
       "    -0.014228986576199532,\n",
       "    -0.007760099601000547,\n",
       "    -0.009477583691477776,\n",
       "    -0.05859416350722313,\n",
       "    -0.017644720152020454,\n",
       "    0.014053200371563435,\n",
       "    0.01808910071849823,\n",
       "    -0.025319695472717285,\n",
       "    -0.01022283174097538,\n",
       "    -0.04215439409017563,\n",
       "    -0.014142959378659725,\n",
       "    1.746752423059661e-05,\n",
       "    -0.04155829921364784,\n",
       "    -0.053963955491781235,\n",
       "    -0.026281701400876045,\n",
       "    0.0662294402718544,\n",
       "    -0.025692131370306015,\n",
       "    -0.03932614251971245,\n",
       "    0.026369275525212288,\n",
       "    -0.06941965967416763,\n",
       "    -0.01149339135736227,\n",
       "    -0.024656690657138824,\n",
       "    0.034839875996112823,\n",
       "    0.09246819466352463,\n",
       "    0.007840657606720924,\n",
       "    -0.005290364846587181,\n",
       "    -0.028822703287005424,\n",
       "    -0.036222007125616074,\n",
       "    0.04496601223945618,\n",
       "    0.03117789514362812,\n",
       "    -0.018668867647647858,\n",
       "    0.0005299983313307166,\n",
       "    0.0021931210067123175,\n",
       "    -0.023787183687090874,\n",
       "    -0.005189722869545221,\n",
       "    0.008987859822809696,\n",
       "    -0.012216147035360336,\n",
       "    0.0546068511903286,\n",
       "    -0.03400413691997528,\n",
       "    0.02338911034166813,\n",
       "    -0.004806441254913807,\n",
       "    0.01140426006168127,\n",
       "    -0.0261690691113472,\n",
       "    -0.009725328534841537,\n",
       "    -0.009852258488535881,\n",
       "    -0.05725707486271858,\n",
       "    -0.03211885318160057,\n",
       "    -0.04102523997426033,\n",
       "    -0.01062001008540392,\n",
       "    -0.018177682533860207,\n",
       "    -0.010140843689441681,\n",
       "    0.013285171240568161,\n",
       "    0.009264613501727581,\n",
       "    0.054535239934921265,\n",
       "    -0.030733125284314156,\n",
       "    0.0691450908780098,\n",
       "    2.852781051387865e-07,\n",
       "    0.03187370300292969,\n",
       "    0.002965539461001754,\n",
       "    -0.036401938647031784,\n",
       "    0.024400630965828896,\n",
       "    0.03300417214632034,\n",
       "    0.0004970004083588719,\n",
       "    -0.044018372893333435,\n",
       "    -0.04952297732234001,\n",
       "    0.00491013377904892,\n",
       "    -0.043458662927150726,\n",
       "    -0.008403669111430645,\n",
       "    0.05733603611588478,\n",
       "    0.023775087669491768,\n",
       "    -0.030646173283457756,\n",
       "    -0.0407634899020195,\n",
       "    -0.06806742399930954,\n",
       "    -0.0043092467822134495,\n",
       "    -0.021598175168037415,\n",
       "    -0.0010229316540062428,\n",
       "    0.05443334951996803,\n",
       "    -0.01235464122146368,\n",
       "    0.03557674214243889,\n",
       "    0.02124745212495327,\n",
       "    0.04711419343948364,\n",
       "    -0.018970131874084473,\n",
       "    -0.00847373716533184,\n",
       "    -0.04721027612686157,\n",
       "    -0.0025783327873796225,\n",
       "    0.05383775755763054,\n",
       "    0.03721201792359352,\n",
       "    -0.01994265429675579,\n",
       "    -0.011151931248605251,\n",
       "    0.04433703050017357,\n",
       "    0.0362195260822773,\n",
       "    0.0038219457492232323,\n",
       "    -0.024197371676564217,\n",
       "    -0.0013786935014650226,\n",
       "    0.06318159401416779,\n",
       "    -0.0016053984872996807,\n",
       "    0.021409476175904274,\n",
       "    0.05994655191898346,\n",
       "    0.013800276443362236,\n",
       "    0.01648109219968319,\n",
       "    -0.07152589410543442,\n",
       "    0.02832082286477089,\n",
       "    0.03596406802535057,\n",
       "    -0.06912487745285034,\n",
       "    -0.09572704136371613,\n",
       "    0.03602512925863266,\n",
       "    -0.04952844977378845,\n",
       "    -0.019229931756854057,\n",
       "    -0.005574565380811691,\n",
       "    0.04854048788547516,\n",
       "    0.02885768935084343,\n",
       "    -0.0009208315168507397,\n",
       "    -0.03850596770644188,\n",
       "    -0.021094683557748795,\n",
       "    0.018557477742433548,\n",
       "    -0.019414110109210014,\n",
       "    -0.0554368756711483,\n",
       "    0.011423497460782528,\n",
       "    -0.04004163295030594,\n",
       "    0.005466346628963947,\n",
       "    -0.05517859384417534,\n",
       "    0.057868655771017075,\n",
       "    0.017602544277906418,\n",
       "    0.03824613243341446,\n",
       "    2.6181650218507977e-34,\n",
       "    0.0028443161863833666,\n",
       "    0.039099451154470444,\n",
       "    -0.05114372819662094,\n",
       "    0.0901920422911644,\n",
       "    0.037334851920604706,\n",
       "    -0.06057137995958328,\n",
       "    -0.030114270746707916,\n",
       "    -0.009225860238075256,\n",
       "    -0.011277182959020138,\n",
       "    0.03147172927856445,\n",
       "    0.01292212400585413],\n",
       "   'chapter': 'CHAPTER 3',\n",
       "   'title': 'Technical Interview: Machine Learning Algorithms'},\n",
       "  '_explanation': {'value': 1.9621209,\n",
       "   'description': 'sum of:',\n",
       "   'details': [{'value': 0.0040404093,\n",
       "     'description': 'weight(chapter:chapter in 7) [PerFieldSimilarity], result of:',\n",
       "     'details': [{'value': 0.0040404093,\n",
       "       'description': 'score(freq=1.0), computed as boost * idf * tf from:',\n",
       "       'details': [{'value': 2.2, 'description': 'boost', 'details': []},\n",
       "        {'value': 0.0040404093,\n",
       "         'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:',\n",
       "         'details': [{'value': 123,\n",
       "           'description': 'n, number of documents containing term',\n",
       "           'details': []},\n",
       "          {'value': 123,\n",
       "           'description': 'N, total number of documents with field',\n",
       "           'details': []}]},\n",
       "        {'value': 0.45454544,\n",
       "         'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:',\n",
       "         'details': [{'value': 1.0,\n",
       "           'description': 'freq, occurrences of term within document',\n",
       "           'details': []},\n",
       "          {'value': 1.2,\n",
       "           'description': 'k1, term saturation parameter',\n",
       "           'details': []},\n",
       "          {'value': 0.75,\n",
       "           'description': 'b, length normalization parameter',\n",
       "           'details': []},\n",
       "          {'value': 2.0, 'description': 'dl, length of field', 'details': []},\n",
       "          {'value': 2.0,\n",
       "           'description': 'avgdl, average length of field',\n",
       "           'details': []}]}]}]},\n",
       "    {'value': 1.9580805,\n",
       "     'description': 'weight(chapter:3 in 7) [PerFieldSimilarity], result of:',\n",
       "     'details': [{'value': 1.9580805,\n",
       "       'description': 'score(freq=1.0), computed as boost * idf * tf from:',\n",
       "       'details': [{'value': 2.2, 'description': 'boost', 'details': []},\n",
       "        {'value': 1.9580806,\n",
       "         'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:',\n",
       "         'details': [{'value': 17,\n",
       "           'description': 'n, number of documents containing term',\n",
       "           'details': []},\n",
       "          {'value': 123,\n",
       "           'description': 'N, total number of documents with field',\n",
       "           'details': []}]},\n",
       "        {'value': 0.45454544,\n",
       "         'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:',\n",
       "         'details': [{'value': 1.0,\n",
       "           'description': 'freq, occurrences of term within document',\n",
       "           'details': []},\n",
       "          {'value': 1.2,\n",
       "           'description': 'k1, term saturation parameter',\n",
       "           'details': []},\n",
       "          {'value': 0.75,\n",
       "           'description': 'b, length normalization parameter',\n",
       "           'details': []},\n",
       "          {'value': 2.0, 'description': 'dl, length of field', 'details': []},\n",
       "          {'value': 2.0,\n",
       "           'description': 'avgdl, average length of field',\n",
       "           'details': []}]}]}]}]}},\n",
       " {'_shard': '[ml-interview-questions][0]',\n",
       "  '_node': 'EScPTzvSSEeYjH0xRE7sig',\n",
       "  '_index': 'ml-interview-questions',\n",
       "  '_id': '37ez-ZEBwj93mx26uplr',\n",
       "  '_score': 1.9621209,\n",
       "  '_source': {'section': 'Statistical and Foundational Techniques',\n",
       "   'chapter': 'CHAPTER 3',\n",
       "   'title': 'Technical Interview: Machine Learning Algorithms'},\n",
       "  '_explanation': {'value': 1.9621209,\n",
       "   'description': 'sum of:',\n",
       "   'details': [{'value': 0.0040404093,\n",
       "     'description': 'weight(chapter:chapter in 8) [PerFieldSimilarity], result of:',\n",
       "     'details': [{'value': 0.0040404093,\n",
       "       'description': 'score(freq=1.0), computed as boost * idf * tf from:',\n",
       "       'details': [{'value': 2.2, 'description': 'boost', 'details': []},\n",
       "        {'value': 0.0040404093,\n",
       "         'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:',\n",
       "         'details': [{'value': 123,\n",
       "           'description': 'n, number of documents containing term',\n",
       "           'details': []},\n",
       "          {'value': 123,\n",
       "           'description': 'N, total number of documents with field',\n",
       "           'details': []}]},\n",
       "        {'value': 0.45454544,\n",
       "         'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:',\n",
       "         'details': [{'value': 1.0,\n",
       "           'description': 'freq, occurrences of term within document',\n",
       "           'details': []},\n",
       "          {'value': 1.2,\n",
       "           'description': 'k1, term saturation parameter',\n",
       "           'details': []},\n",
       "          {'value': 0.75,\n",
       "           'description': 'b, length normalization parameter',\n",
       "           'details': []},\n",
       "          {'value': 2.0, 'description': 'dl, length of field', 'details': []},\n",
       "          {'value': 2.0,\n",
       "           'description': 'avgdl, average length of field',\n",
       "           'details': []}]}]}]},\n",
       "    {'value': 1.9580805,\n",
       "     'description': 'weight(chapter:3 in 8) [PerFieldSimilarity], result of:',\n",
       "     'details': [{'value': 1.9580805,\n",
       "       'description': 'score(freq=1.0), computed as boost * idf * tf from:',\n",
       "       'details': [{'value': 2.2, 'description': 'boost', 'details': []},\n",
       "        {'value': 1.9580806,\n",
       "         'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:',\n",
       "         'details': [{'value': 17,\n",
       "           'description': 'n, number of documents containing term',\n",
       "           'details': []},\n",
       "          {'value': 123,\n",
       "           'description': 'N, total number of documents with field',\n",
       "           'details': []}]},\n",
       "        {'value': 0.45454544,\n",
       "         'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:',\n",
       "         'details': [{'value': 1.0,\n",
       "           'description': 'freq, occurrences of term within document',\n",
       "           'details': []},\n",
       "          {'value': 1.2,\n",
       "           'description': 'k1, term saturation parameter',\n",
       "           'details': []},\n",
       "          {'value': 0.75,\n",
       "           'description': 'b, length normalization parameter',\n",
       "           'details': []},\n",
       "          {'value': 2.0, 'description': 'dl, length of field', 'details': []},\n",
       "          {'value': 2.0,\n",
       "           'description': 'avgdl, average length of field',\n",
       "           'details': []}]}]}]}]}},\n",
       " {'_shard': '[ml-interview-questions][0]',\n",
       "  '_node': 'EScPTzvSSEeYjH0xRE7sig',\n",
       "  '_index': 'ml-interview-questions',\n",
       "  '_id': '4Lez-ZEBwj93mx26u5lH',\n",
       "  '_score': 1.9621209,\n",
       "  '_source': {'text': 'Statistical techniques are used in every data role, and these techniques are the foun‐ dations for ML projects. Hence, in ML interviews, you will most likely have questions that cover this topic. 1 Statistical techniques help build baseline models to compare more costly models and algorithms against or help discover if there is enough mean‐ ingful data in the first place to build ML models. For the purposes of this book, I will be placing the foundational regression tech‐ niques in this section, as well as various techniques for training and improving ML models. In short, these are (1) foundational techniques and (2) methods used during model training, such as training splits, regularization, and so on. These concepts are foundational knowledge for any type of ML algorithms that will be mentioned later as well as foundations for ML interview questions. This section covers the basics of statistical techniques for those who are unsure whether they have sufficient background knowledge in this area. Feel free to skip the subsections if you already have expertise in any of these areas. Regardless of your expertise, I’ve highlighted specific advice for ML interviews in the tip boxes to help you apply your knowledge of each ML area and excel in your interviews. To further supplement your knowledge on statistical and foundational techniques beyond the summaries I’ve provided in this book, I recommend the following resources: • The Elements of Statistical Learning by Trevor Hastie et al. • An Introduction to Statistical Learning with Applications in Python by Gareth James et al. • Courses on Coursera by DeepLearning.AI and Andrew Ng; this resource is also useful for all following subtopics of ML (I won’t repeatedly link their courses— since they sometimes change and update—in the following sections). • Introduction to Machine Learning Interviews by Chip Huyen has additional questions for overall ML interviews which can be referenced for most sections in this chapter. Refer back to this section for reference material when preparing for interviews. Now, let’s jump in. Here’s an overview of one of the foundations of ML algorithms—variables—and a simple example of fitting a model. Let’s say that you have a dataset about apples, with the weight and height of each apple. You also have a list of past sales prices of each apple. With the list of apple weights, heights, and past sales prices, you want to guess the sales price of new apples, before they are sold. For the sake of this example, ignore big grocery chains automati‐ cally calculating a price, but let’s say you’re selling as a hobby to friends and family, or maybe you’re running a farm that a grandparent left you. So you are making use of the weight and height of each single new apple to predict its price. Weight and height are fixed observations at that point in time (an apple can’t be both 100 grams and 150 grams at the same time). Now, to connect all these concepts, let’s add in some terminology. Variables refer to everything that is being taken into account in your model of how apple prices are cal‐ culated. So the variables in this case include weight, height, and price. Within these variables, you know the weight and height of each new apple, and they are fixed at that point in time. So the weight and height are independent variables. Then, you have another variable, price, that you’d like to predict for new apples prior to know‐ ing the correct answer before selling them. The predicted price depends on the height and weight of the new apple. For example, heavier and taller apples sell for more money. Thus, price is a dependent variable (shown in Table 3-1 ). Table 3-1. Examples of independent and dependent variables The concepts of independent and dependent variables are tried and true, but the ter‐ minology might not be. In different fields, you may have come across the terms listed in Table 3-2 , although they may differ from industry to industry or from textbook to textbook. During your interview, make sure you and the interviewer aren’t crossing wires due to terminology, and double-check with your interviewer if you sense that you’re using a different term to refer to the same thing. Knowing the most common terms can help you use them appropriately in your interviews in various fields. Table 3-2. Synonyms of independent and dependent variables Models are a way of using past data points to describe “the way the world works,” or, in other words, a way of finding patterns and connections with past information. The apples example from the previous section uses a model that describes the way pricing works. The model is something that knows the “truth”—even if it’s not the full truth but rather our best attempt to approximate the truth. Thus, the model can be used to predict our best approximation of future data points. This applies for all “models” in ML models. Recommender-system models seek to predict what a user will like or click on when visiting a website. Convolutional neural networks (CNNs) for image recognition “learn” a model of what various pixels represent: is this cluster and layout of pixels a cat or a dog? Just as for independent and dependent variables, it is important to have a shared defi‐ nition for “model” to prevent miscommunication during an interview, such as con‐ fusing algorithms with models. 2 The model is the outcome of having run and fit an ML algorithm. I wanted to make sure I included regression models. I’m glad that I learned the detailed ins and outs of linear and logistic regression, even calculating them by hand (a requirement for the second-year statistics course I was taking as part of an eco‐ nomics major at university). This knowledge has compounded and helped me under‐ stand the new ML algorithms I’ve encountered as well as how to apply them in practice. All of my learning stemmed from understanding these entry-level concepts, so I highly recommend not shying away from learning the mathematics of regression models. Again, feel free to skip this section if you already have expertise in this area. Let’s use the apple example from an earlier section in a graph. For simplicity and to squeeze it onto a two-dimensional graph, let’s use just one independent variable, weight , to predict the dependent variable, price . Each dot on the graph in Figure 3-1 represents a data point from past sales, so you already know the sale prices for them. For example, the dot with a callout on the graph weighs 80 grams (its intersect on the x-axis) and sold for $1 (its intersect on the y-axis). Note that this is a simple example; most usage of linear regression will have multiple independent variables (“multivaria‐ ble”) and if visualized, will be a line in an N -dimensional space, where N = number of variables + 1 (when there is one output variable). In addition, this example has one dependent variable; when there are multiple dependent/output variables, the regres‐ sion task is referred to as being multivariate . Note that multivariate is separate from the “multivariable” concept mentioned earlier. Figure 3-1. Data points to be used for linear regression. The next step in linear regression is fitting the titular “line” to the data points. Behind the scenes, software tools like Python, Stata, IBM SPSS, SAS, MATLAB, and the like will calculate a “line of best fit.” According to the definition of a model given previ‐ ously in this section, this line is the model , which is the best approximation of the truth with the data points that you have. Starting with an initial line, the software will calculate the residual : the y-axis distance between a data point and the line, as illus‐ trated in Figure 3-2 . Colloquially, the residual is also referred to as the residual error . Figure 3-2. Fitting the line of best fit in linear regression; the line is iterated on until the residuals are as small as possible. All the residuals are squared so that predictions above and the line don’t cancel each other out due to having opposite signs (positive, negative). The goal is that the sum of the residuals is as small as possible since if you have a line that is drastically far away from the data points, that means the line isn’t fitting well to as many data points as possible and as correctly as possible. Mathematically, a common technique to tell how well the line is fitting is the process called least squares . Achieving least squares means finding the line that results in the smallest sum of squared residuals, which in turn means you have the “line of best fit”: the line is fitting the data points with least distance from the data points overall, as shown in Figure 3-3 . Figure 3-3. Least squares and terminology; y represents observed data points, and ŷ (y hat) represents the predicted/estimated values. The end result is a line that has the smallest sum of least squares to the data points, as illustrated in Figure 3-4 . Figure 3-4. The resulting line of best fit with least squares from the data in Figure 3-1 . Going forward, you can use this “line of best fit” as a model to predict new apple pri‐ ces! You can plug the apple weight into the line (in equation form) to get a numerical value for the predicted price. This is one of the most basic ways of calculating a model from data points, but it has the same pattern as more in-depth ML models and algo‐ rithms that are covered in the next chapter. Namely, you’ll initialize a line (you don’t know if this is the best model yet) and calculate the residuals, or how well it fits. Next, you’ll change the line by tilting it a little—mathematically, this is called updating coef‐ ficients or weights— and calculate the residuals again, as illustrated in Figure 3-2 . This updating process is called training , which is where the commonly used phrase “training/to train an ML model” comes from. If the sum of the squared residuals is getting smaller, then you are on the right track. When you can’t make the squared residuals any smaller, you’ve achieved least squares, and that’s how you can say the line is your best approximation with this dataset (as illustrated in Figure 3-4 ). It’s like that game where there’s an item hidden in the room, and as you walk around the room trying to find it, your friend says “hot” if you’re getting closer and “cold” when you’re walking farther away. You want to walk toward hotter and hotter areas in the room, until you reach the final position. In Chapter 4 , I’ll walk through ways to evaluate models via error terms such as mean squared error (MSE), root mean square error (RMSE), and more, which are very sim‐ ilar concepts to residuals. The main difference here is that residuals are the difference between past observation data and model estimations while errors are the difference between model estimations and actual data previously unseen by the model. In other words, errors are the differences after applying the model to previously unseen data in order to evaluate model performance. To sum up, when using supervised 3 machine learning such as the simple linear regression example in the previous section, you’ll generally start with a dataset and want the ML algorithm to learn a model of how things work. You then will use the model to calculate the values of dependent variables, such as predicting how much apples will sell for before they are actually sold. In other words, you have a dataset of past data points and, of course, no future data points. When the ML model is being trained, it’s learning to “fit” the data that you currently have. There are some issues that could arise with model training when the model is used in the real world. For one, there will always be outliers or changing events in the real world. One example is in financial predictions with ML: the market could swing suddenly to a bear (downturn) market, and the model we’ve trained with financial data in a bull (upswing) market will produce horrible and wildly inaccurate predictions. Another example is that the dataset you have isn’t representative enough of the behaviors of the real world. In the apples example from the previous section, you assume that with the weight and height data of the apples, you can predict the sell price of new apples. But what if the data you have on hand isn’t enough, and apple variants like Fuji or Honeycrisp (one of my favorites) sell for more? You didn’t have each apple’s variant name tracked in your dataset, so then your model may be incorrect once you put it to the test. But for now, you have only the current dataset. To make the most of it, you need to keep some of the data you have for testing purposes. What this means is that you can break out 80% of the apple data points to use for model training and then save 20% of the apple data points to run the trained model predictions on. The 80% the model is trained on is called the training set (sometimes referred to as the train set ), and the 20% of data that is unseen by the model during the training phase is called the test set . This mimics the real-world scenario of running the model to predict new data points; the test set serves that purpose. In many cases, you might even split the data into three chunks: 80% as the training dataset, 10% as the validation (holdout) dataset, and another 10% as the test dataset ( Figure 3-5 ). The validation set allows you to monitor the model’s performance during the train‐ ing process without “formally” evaluating it, and it enables you to diagnose weak spots of the model and tune its parameters. The test set, as previously mentioned, was unseen by the model during the training process and thus is used to formally evaluate the model performance, mimicking a real-world environment as much as possible. Of course, having a test and validation set isn’t infallible, which brings us to more robust techniques and the concepts of model overfitting and underfitting. Figure 3-5. Training, validation, and test set splits. For interview questions on training and test sets, make sure you can name common ways to augment the simpler splits, such as using cross-validation : 4 splitting up data into smaller chunks and rotating through them as training sets. There are many reasons a model may not perform well on real-world data (or even the validation or test set). A common starting point is addressing overfitting or underfitting. Underfitting is when the model isn’t fitting well. This might mean that the model isn’t able to capture the relationship between the dataset’s independent variables (e.g., weight, height, etc.) and the dependent variables (e.g., price). Conse‐ quently, some ways to reduce underfitting are related to helping the model learn more nuances or patterns during the training process. For example, adding more variables, or model features, such as apple variant or age of the apple, could help the model learn more patterns from the training data and potentially reduce underfitting. A second way to reduce underfitting is to increase the Overfitting is when a model fits the training data too closely and very specifically, perhaps finding patterns that happen to be in the training set but not elsewhere. A simplified example is that the training data just so happens to have a lot of apples that are disproportionately expensive despite their weight (e.g., Sekai Ichi apples 6 ). The model learned from that data and overfit to it, therefore making incorrect predictions that are overpriced for cheaper apple variants. Simply put, the model is overmemo‐ rizing the training data and unable to generalize to new data points. There are many techniques to make the model generalize better, such as adding more training data, data augmentation, or regularization. 7 I’ll cover the details of regularization next. Regularization is a technique used to reduce overfitting of ML models. Generally, regularization will create a damper on model weights/coefficient. By this point, you likely know what I’m going to do—which is to bring up the apples again! Apples are my favorite fruit, which is probably why I use the example so often. So let’s say the model has learned to weigh “weight of apple” more heavily (accidental pun, but model “weights” is legitimate terminology); then the weight of the apple is mathe‐ matically increasing the results of the ML model’s prediction of the price by a rela‐ tively high positive value. If you can dampen the amount by which the weight of the apple increases the model’s predictions of the price, via regularization, that can make the model generalize more and take other variables into account more evenly. The variance bias trade-off is a common topic in ML interviews. When applying ML model improvement techniques such as regularization, it is important to consider the trade-off between fixing for bias versus variance. Bias refers to the overall inaccuracy of the model and can often be caused by an oversimplified (underfit) model. Variance comes from overfitting, when the model has learned too specifically from the training set. One way to remember why this is called “variance” is that the term refers to the variability of the model: the model is overfit to specific points or traits, so the model is very sensitive to different data points, causing fluctuation and variability. Regularization might cause a model to reduce its variance but might inadvertently increase bias, so that’s a reason to be cautious and test various model-improvement techniques. Now that I’ve covered various statistical and ML techniques at a higher level, let’s look at some sample questions. Here, I will dive into the details of common interview questions that stem from the concepts covered in this section. These details may not have been previously addressed, so my hope is that these sample questions also serve to explain the new concepts. Example answer L1 regularization , also known as lasso regularization , 8 is a type of regularization that shrinks model parameters toward zero. L2 regularization (also known as ridge regularization ) adds a penalty term to the objective function that is propor‐ tional to the square of the coefficients of the model. This penalty term shrinks the coefficients toward zero, but unlike L1 (lasso) regularization, it does not make any of the coefficients exactly equal to zero. L2 regularization can help reduce overfitting and improve the stability of the model by keeping coefficients from becoming too large. Both L1 and L2 regulari‐ zation are commonly used to prevent overfitting and improve the generalization of ML models. Interview questions on model overfitting and underfitting may lead to follow-up questions. For example, if you bring up L1 and L2 regularization, the interviewer might ask, “What other types of reg‐ ularization could work?” In that case, you could bring up elastic net , which is a combination of L1 and L2 techniques. Or for the overfitting case, ensemble techniques can also help (refer to “Inter‐ view question 3-3: Explain boosting and bagging and what they can help with.” on page 76 ). Example answer Imbalanced datasets in ML refer to datasets in which some classes or categories outweigh others. 9 Techniques to deal with imbalanced datasets include data aug‐ mentation, oversampling, undersampling, ensemble methods, and so on: Data augmentation Data augmentation involves generating more examples for the ML model to train on, such as rotating images so that the dataset includes images of humans turned upside down as well as the normal upright image orienta‐ tion. Without data augmentation, the model might not be able to correctly recognize images of humans who are laying sideways or doing headstands since the data is imbalanced toward humans in an upright pose. Oversampling Oversampling is a technique to increase the number of data points of a minority class via synthetic generation. As an example, SMOTE (synthetic minority oversampling technique) 10 uses the feature vectors of the minority classes to generate synthetic data points that are located between real data points and their k-nearest neighbors. This could synthetically increase the size of the minority class(es) and improve the performance of the ML model trained on a dataset with oversampling treatment. Undersampling Undersampling does the opposite: it reduces examples from the majority class to balance the number of data points of the majority class and minority class(es). Oversampling is generally preferred in practice since undersam‐ pling may cause useful data to be discarded, which is exacerbated when the dataset is already small. Ensemble methods Ensemble methods can also be used to increase model performance when dealing with an imbalanced dataset. 11 Each model in the ensemble can be trained on a different subset of the data and can help learn the nuances of each class better. When answering ML interview questions, take a second to confirm the scope of the question. In other words, if the question is asking only for a definition of logistic regression, don’t go on a tangent about various other techniques. If the question is open-ended, you can confirm whether the interviewer is asking for something specific. Example answer Bagging and boosting are ensemble techniques used to improve the performance of ML models: Bagging Bagging trains multiple models on different subsets of the training data and combines their predictions to make a final prediction. Boosting Boosting trains a series of models where each model tries to correct the mis‐ takes made by the previous model. The final prediction is made by all the models. Ensemble techniques can help with a variety of issues encountered during ML training. For example, they can help with imbalanced data 12 and See Chapter 4 for more in-depth questions concerning model evaluation.',\n",
       "   'text_vector': [0.036523137241601944,\n",
       "    -0.0007432920974679291,\n",
       "    -0.0432945154607296,\n",
       "    -0.0033760142978280783,\n",
       "    -0.034039177000522614,\n",
       "    0.020350027829408646,\n",
       "    0.05255116894841194,\n",
       "    -0.005388661753386259,\n",
       "    -0.012733852490782738,\n",
       "    -0.0362168587744236,\n",
       "    0.060072142630815506,\n",
       "    0.02028055489063263,\n",
       "    0.04295819252729416,\n",
       "    0.020313583314418793,\n",
       "    0.04417930915951729,\n",
       "    -0.09074056148529053,\n",
       "    -0.011303425766527653,\n",
       "    -0.005822582636028528,\n",
       "    -0.006609124131500721,\n",
       "    0.0014482270926237106,\n",
       "    0.012197606265544891,\n",
       "    -0.0038030275609344244,\n",
       "    0.02707531489431858,\n",
       "    -0.007084347307682037,\n",
       "    -0.0828428715467453,\n",
       "    0.00489136204123497,\n",
       "    -0.017467757686972618,\n",
       "    0.019567061215639114,\n",
       "    -0.01860339380800724,\n",
       "    -0.02672656439244747,\n",
       "    0.02430013380944729,\n",
       "    -0.02965652197599411,\n",
       "    -0.028670847415924072,\n",
       "    -0.019623422995209694,\n",
       "    2.100783831338049e-06,\n",
       "    -0.033111900091171265,\n",
       "    -0.03307729586958885,\n",
       "    0.02178502455353737,\n",
       "    0.0020244757179170847,\n",
       "    -0.07057476043701172,\n",
       "    0.043849024921655655,\n",
       "    -0.0259234681725502,\n",
       "    0.049552787095308304,\n",
       "    0.04065810516476631,\n",
       "    -0.024271756410598755,\n",
       "    -0.017819207161664963,\n",
       "    0.08321874588727951,\n",
       "    0.04191126301884651,\n",
       "    0.010351017117500305,\n",
       "    0.035522401332855225,\n",
       "    -0.024218164384365082,\n",
       "    -0.06105755269527435,\n",
       "    0.0020202219020575285,\n",
       "    -0.00172926124650985,\n",
       "    -0.002213859697803855,\n",
       "    0.019961217418313026,\n",
       "    0.018572049215435982,\n",
       "    0.037241872400045395,\n",
       "    -0.019317928701639175,\n",
       "    -0.06962689757347107,\n",
       "    -0.014123695902526379,\n",
       "    0.02653263323009014,\n",
       "    0.025636684149503708,\n",
       "    0.039012473076581955,\n",
       "    0.05353250354528427,\n",
       "    0.00045431280159391463,\n",
       "    -0.033853475004434586,\n",
       "    -0.047667648643255234,\n",
       "    -0.03363697603344917,\n",
       "    0.0302193034440279,\n",
       "    -0.0006059636943973601,\n",
       "    0.004212603904306889,\n",
       "    0.004645414650440216,\n",
       "    -0.024126026779413223,\n",
       "    0.04055643826723099,\n",
       "    0.0386638343334198,\n",
       "    -0.05192442238330841,\n",
       "    -0.01303178258240223,\n",
       "    -0.0027255304157733917,\n",
       "    0.0033434468787163496,\n",
       "    -0.04510887712240219,\n",
       "    0.002753729932010174,\n",
       "    0.004314387682825327,\n",
       "    -0.036215223371982574,\n",
       "    0.009464943781495094,\n",
       "    0.04220883548259735,\n",
       "    -0.03910323604941368,\n",
       "    -0.03750601038336754,\n",
       "    0.04307063668966293,\n",
       "    0.024784497916698456,\n",
       "    0.05995957553386688,\n",
       "    0.012582555413246155,\n",
       "    -0.007807245012372732,\n",
       "    0.03980224207043648,\n",
       "    -0.008865228854119778,\n",
       "    0.01757882535457611,\n",
       "    0.0207821074873209,\n",
       "    -0.01814902201294899,\n",
       "    0.022295553237199783,\n",
       "    -0.03189857676625252,\n",
       "    -0.014835912734270096,\n",
       "    -0.008013461716473103,\n",
       "    -0.01237302366644144,\n",
       "    -0.034499797970056534,\n",
       "    0.004047177731990814,\n",
       "    -0.0008237141883000731,\n",
       "    0.047848641872406006,\n",
       "    -0.022965388372540474,\n",
       "    -0.04463614523410797,\n",
       "    -0.004908710718154907,\n",
       "    -0.03371310234069824,\n",
       "    0.014752012677490711,\n",
       "    0.006612858269363642,\n",
       "    0.040917642414569855,\n",
       "    0.010968323796987534,\n",
       "    0.04976126179099083,\n",
       "    -0.005838205572217703,\n",
       "    -0.00340695190243423,\n",
       "    0.03452989086508751,\n",
       "    -0.048853788524866104,\n",
       "    -0.03082912042737007,\n",
       "    0.03601294755935669,\n",
       "    0.035098280757665634,\n",
       "    0.09186170250177383,\n",
       "    -0.017461182549595833,\n",
       "    -0.07163863629102707,\n",
       "    -0.04070524126291275,\n",
       "    0.03825703635811806,\n",
       "    -0.030244776979088783,\n",
       "    -0.05524608865380287,\n",
       "    0.04929491505026817,\n",
       "    0.017085066065192223,\n",
       "    0.02404647320508957,\n",
       "    -0.0015520754968747497,\n",
       "    0.05577943101525307,\n",
       "    0.01873045787215233,\n",
       "    0.028053198009729385,\n",
       "    0.02291409857571125,\n",
       "    -0.09274528175592422,\n",
       "    -0.0212035421282053,\n",
       "    0.024848653003573418,\n",
       "    -0.07113061845302582,\n",
       "    0.0017574595985934138,\n",
       "    -0.04926798492670059,\n",
       "    -0.023708222433924675,\n",
       "    0.010406098328530788,\n",
       "    -0.01646629348397255,\n",
       "    -0.031374696642160416,\n",
       "    0.004565098322927952,\n",
       "    -0.03471960127353668,\n",
       "    -0.07150722295045853,\n",
       "    -0.006568982265889645,\n",
       "    0.03470566123723984,\n",
       "    0.020325850695371628,\n",
       "    0.03409697487950325,\n",
       "    -0.02281690016388893,\n",
       "    0.021977996453642845,\n",
       "    0.04635070264339447,\n",
       "    0.010784306563436985,\n",
       "    0.012090804055333138,\n",
       "    0.006639380473643541,\n",
       "    -0.05897798016667366,\n",
       "    0.048791754990816116,\n",
       "    0.019540075212717056,\n",
       "    0.023658879101276398,\n",
       "    -0.04116778075695038,\n",
       "    -0.03905382379889488,\n",
       "    0.009988395497202873,\n",
       "    -0.0484577938914299,\n",
       "    -0.023476315662264824,\n",
       "    0.01899963617324829,\n",
       "    0.020813094452023506,\n",
       "    0.0050387876108288765,\n",
       "    0.04070603847503662,\n",
       "    0.03995497524738312,\n",
       "    0.0539737232029438,\n",
       "    0.009352164342999458,\n",
       "    0.00287872226908803,\n",
       "    0.0003331065527163446,\n",
       "    -0.020866677165031433,\n",
       "    0.019330037757754326,\n",
       "    0.0321771539747715,\n",
       "    -0.004746125545352697,\n",
       "    0.022074194625020027,\n",
       "    0.02259751595556736,\n",
       "    -0.017941661179065704,\n",
       "    -0.0613631010055542,\n",
       "    0.04856313392519951,\n",
       "    -0.032226480543613434,\n",
       "    -0.04425865039229393,\n",
       "    -0.0047739543952047825,\n",
       "    0.005284262355417013,\n",
       "    -0.02744286134839058,\n",
       "    0.07892142236232758,\n",
       "    0.03734707459807396,\n",
       "    -0.0571587048470974,\n",
       "    -0.040705326944589615,\n",
       "    -0.03076474741101265,\n",
       "    -0.04223315417766571,\n",
       "    0.01950768008828163,\n",
       "    -0.04649965092539787,\n",
       "    -0.07959268987178802,\n",
       "    -0.009798326529562473,\n",
       "    -0.019634883850812912,\n",
       "    -0.024099254980683327,\n",
       "    -0.0148086566478014,\n",
       "    -0.005400160327553749,\n",
       "    -0.057829100638628006,\n",
       "    -0.01413261704146862,\n",
       "    -0.037579067051410675,\n",
       "    0.023634986951947212,\n",
       "    -0.008333719335496426,\n",
       "    -0.001808247179724276,\n",
       "    0.03684408217668533,\n",
       "    -0.02903398871421814,\n",
       "    0.028932521119713783,\n",
       "    0.018027858808636665,\n",
       "    -0.03655659034848213,\n",
       "    -0.004538258537650108,\n",
       "    0.02655034139752388,\n",
       "    0.0023902698885649443,\n",
       "    -0.00847261119633913,\n",
       "    -0.06344844400882721,\n",
       "    0.04394364729523659,\n",
       "    -0.0013727130135521293,\n",
       "    -0.007013477385044098,\n",
       "    0.020547926425933838,\n",
       "    -0.014848718419671059,\n",
       "    0.05061951279640198,\n",
       "    0.017796959728002548,\n",
       "    -0.017474118620157242,\n",
       "    0.0029133628122508526,\n",
       "    -0.0020552834030240774,\n",
       "    0.06448186188936234,\n",
       "    0.009064741432666779,\n",
       "    0.042802754789590836,\n",
       "    0.05494966730475426,\n",
       "    -0.012089810334146023,\n",
       "    0.01260928250849247,\n",
       "    -0.033363983035087585,\n",
       "    -0.009789299219846725,\n",
       "    0.025636455044150352,\n",
       "    -0.023298529908061028,\n",
       "    -0.013590295799076557,\n",
       "    0.004210152663290501,\n",
       "    -0.024192944169044495,\n",
       "    0.023971131071448326,\n",
       "    0.01794952154159546,\n",
       "    0.019941510632634163,\n",
       "    -0.05012931674718857,\n",
       "    0.053982269018888474,\n",
       "    -0.015611598268151283,\n",
       "    -0.024895042181015015,\n",
       "    0.014172745868563652,\n",
       "    -0.04308832436800003,\n",
       "    -0.029866961762309074,\n",
       "    -0.07729396969079971,\n",
       "    0.06444510072469711,\n",
       "    -0.05598203465342522,\n",
       "    0.019882729277014732,\n",
       "    -0.017082588747143745,\n",
       "    -0.04856284335255623,\n",
       "    0.0402895025908947,\n",
       "    0.05245945230126381,\n",
       "    -0.015597084537148476,\n",
       "    -0.028517892584204674,\n",
       "    0.04628562182188034,\n",
       "    -0.04226870834827423,\n",
       "    -0.051681917160749435,\n",
       "    0.009798443876206875,\n",
       "    0.020686998963356018,\n",
       "    -0.03493507206439972,\n",
       "    -0.02357720397412777,\n",
       "    -0.03515750542283058,\n",
       "    -0.011370116844773293,\n",
       "    -0.02122955210506916,\n",
       "    -0.02620919793844223,\n",
       "    0.018916774541139603,\n",
       "    -0.022632839158177376,\n",
       "    0.016698170453310013,\n",
       "    0.008371404372155666,\n",
       "    -0.017923371866345406,\n",
       "    -0.022603247314691544,\n",
       "    -0.03060407005250454,\n",
       "    -0.030249347910284996,\n",
       "    0.049879878759384155,\n",
       "    0.026675119996070862,\n",
       "    -0.04996521398425102,\n",
       "    0.03452010080218315,\n",
       "    0.04009155556559563,\n",
       "    -0.008268361911177635,\n",
       "    0.004845915827900171,\n",
       "    0.015609807334840298,\n",
       "    -0.03657381981611252,\n",
       "    -0.03021540306508541,\n",
       "    0.0302544254809618,\n",
       "    0.0043389624916017056,\n",
       "    0.07188685983419418,\n",
       "    -0.03142336755990982,\n",
       "    -0.0006203280645422637,\n",
       "    0.004607014358043671,\n",
       "    -0.009082073345780373,\n",
       "    -0.002992464927956462,\n",
       "    0.017054514959454536,\n",
       "    0.007074362598359585,\n",
       "    0.03323652595281601,\n",
       "    -0.013753260485827923,\n",
       "    -0.04071330651640892,\n",
       "    -0.022993650287389755,\n",
       "    -0.008187214843928814,\n",
       "    0.07186315208673477,\n",
       "    -0.04311871901154518,\n",
       "    0.04904330521821976,\n",
       "    0.08635549247264862,\n",
       "    0.02626853808760643,\n",
       "    -0.0796511247754097,\n",
       "    -0.04572908580303192,\n",
       "    0.039594996720552444,\n",
       "    0.005481055937707424,\n",
       "    0.05847568064928055,\n",
       "    -0.0012375687947496772,\n",
       "    -0.07025717198848724,\n",
       "    0.00736632477492094,\n",
       "    0.02295425906777382,\n",
       "    0.018902722746133804,\n",
       "    -0.04538080841302872,\n",
       "    -0.02345798723399639,\n",
       "    0.022726068273186684,\n",
       "    0.015088234096765518,\n",
       "    -7.952674422995187e-06,\n",
       "    -0.0008040404063649476,\n",
       "    -0.02290807105600834,\n",
       "    0.0300035048276186,\n",
       "    -0.010364477522671223,\n",
       "    -0.030911428853869438,\n",
       "    0.008581935428082943,\n",
       "    -0.015181084163486958,\n",
       "    -0.05015239492058754,\n",
       "    -0.012426754459738731,\n",
       "    -0.0050795357674360275,\n",
       "    0.005392144899815321,\n",
       "    0.014479756355285645,\n",
       "    -0.06757500767707825,\n",
       "    -0.07544386386871338,\n",
       "    0.015011251904070377,\n",
       "    -0.026010844856500626,\n",
       "    -0.04803434759378433,\n",
       "    0.011020510457456112,\n",
       "    -0.03159291669726372,\n",
       "    -1.4978345461713616e-06,\n",
       "    0.05167064070701599,\n",
       "    -0.027015365660190582,\n",
       "    -0.007939858362078667,\n",
       "    -0.004214507061988115,\n",
       "    0.06863216310739517,\n",
       "    -0.02415107563138008,\n",
       "    0.025002406910061836,\n",
       "    -0.024279680103063583,\n",
       "    0.05935220792889595,\n",
       "    0.02238640934228897,\n",
       "    0.05052677541971207,\n",
       "    -0.03164755553007126,\n",
       "    0.015941934660077095,\n",
       "    0.021370800212025642,\n",
       "    -0.09116561710834503,\n",
       "    0.026284562423825264,\n",
       "    -0.017275555059313774,\n",
       "    -0.021034061908721924,\n",
       "    0.013541484251618385,\n",
       "    0.051554616540670395,\n",
       "    -0.02600589022040367,\n",
       "    -0.0316668264567852,\n",
       "    0.06590934097766876,\n",
       "    0.07302077114582062,\n",
       "    -0.00018832281057257205,\n",
       "    0.04420003667473793,\n",
       "    -0.013746307231485844,\n",
       "    -0.019049707800149918,\n",
       "    -0.018561065196990967,\n",
       "    -0.02713731862604618,\n",
       "    0.056265559047460556,\n",
       "    -0.02862958237528801,\n",
       "    0.021563809365034103,\n",
       "    -0.02247808687388897,\n",
       "    0.0051451679319143295,\n",
       "    -0.09757675230503082,\n",
       "    -0.02506050281226635,\n",
       "    0.02779846265912056,\n",
       "    -0.009687189012765884,\n",
       "    0.037709254771471024,\n",
       "    -0.010662616230547428,\n",
       "    -0.07664190977811813,\n",
       "    -0.05375158041715622,\n",
       "    -0.043101225048303604,\n",
       "    -0.010198119096457958,\n",
       "    -0.039026301354169846,\n",
       "    0.024810612201690674,\n",
       "    -0.025994187220931053,\n",
       "    0.0005799017962999642,\n",
       "    0.021318256855010986,\n",
       "    0.05682651326060295,\n",
       "    -0.12498573213815689,\n",
       "    -0.03433620557188988,\n",
       "    -0.012117802165448666,\n",
       "    -0.031183630228042603,\n",
       "    -0.039130035787820816,\n",
       "    0.0805756077170372,\n",
       "    0.050922542810440063,\n",
       "    -0.06782805174589157,\n",
       "    0.04181859642267227,\n",
       "    0.016941865906119347,\n",
       "    0.0697193518280983,\n",
       "    0.05734032392501831,\n",
       "    0.011460741050541401,\n",
       "    0.0796310305595398,\n",
       "    0.005391092970967293,\n",
       "    0.02560981921851635,\n",
       "    -0.03779125213623047,\n",
       "    0.06821620464324951,\n",
       "    -0.0008785868994891644,\n",
       "    0.0054435632191598415,\n",
       "    0.04192770645022392,\n",
       "    0.06535118073225021,\n",
       "    -0.027544058859348297,\n",
       "    -0.05329585447907448,\n",
       "    -0.005204528104513884,\n",
       "    0.017901187762618065,\n",
       "    0.03392971307039261,\n",
       "    0.024046586826443672,\n",
       "    -0.01413942500948906,\n",
       "    -0.012698640115559101,\n",
       "    -0.040295444428920746,\n",
       "    0.01628299430012703,\n",
       "    0.01014635618776083,\n",
       "    0.005880649667233229,\n",
       "    0.015682145953178406,\n",
       "    0.010439534671604633,\n",
       "    -0.0041690487414598465,\n",
       "    0.009202095679938793,\n",
       "    0.015596497803926468,\n",
       "    -0.0530460849404335,\n",
       "    0.06010688841342926,\n",
       "    0.004741130396723747,\n",
       "    0.03997192159295082,\n",
       "    0.05987829715013504,\n",
       "    -0.026152106001973152,\n",
       "    -0.013342569582164288,\n",
       "    -0.03995945304632187,\n",
       "    0.0134560726583004,\n",
       "    0.03594321012496948,\n",
       "    0.02462034486234188,\n",
       "    0.04707102105021477,\n",
       "    -0.014405380003154278,\n",
       "    0.0369429774582386,\n",
       "    -0.04781769588589668,\n",
       "    0.003910190425813198,\n",
       "    -0.08764549344778061,\n",
       "    -0.03587689250707626,\n",
       "    -0.02670390158891678,\n",
       "    -0.007295926101505756,\n",
       "    0.07252317667007446,\n",
       "    0.00910146627575159,\n",
       "    0.00845267716795206,\n",
       "    -0.003459046594798565,\n",
       "    -0.06469310820102692,\n",
       "    0.013545672409236431,\n",
       "    0.00966564379632473,\n",
       "    -0.018970590084791183,\n",
       "    -0.0036542972084134817,\n",
       "    0.04566759988665581,\n",
       "    -0.01280929148197174,\n",
       "    0.005412218160927296,\n",
       "    -0.05538897588849068,\n",
       "    0.04199243709445,\n",
       "    0.028635846450924873,\n",
       "    -0.013836679980158806,\n",
       "    -0.07188721001148224,\n",
       "    -0.0365544892847538,\n",
       "    -0.026219792664051056,\n",
       "    -0.024923231452703476,\n",
       "    0.03383069112896919,\n",
       "    0.06315983086824417,\n",
       "    -0.023246990516781807,\n",
       "    -0.050979502499103546,\n",
       "    -0.0038422434590756893,\n",
       "    -0.011691350489854813,\n",
       "    0.07211718708276749,\n",
       "    0.0011100223055109382,\n",
       "    -0.018548181280493736,\n",
       "    -0.023065609857439995,\n",
       "    0.04869747534394264,\n",
       "    0.0777503103017807,\n",
       "    -0.03319019824266434,\n",
       "    -0.07338888198137283,\n",
       "    -0.05023299530148506,\n",
       "    -0.039534974843263626,\n",
       "    -0.026219382882118225,\n",
       "    0.09284378588199615,\n",
       "    0.0321725457906723,\n",
       "    -0.00911789946258068,\n",
       "    -0.0052711437456309795,\n",
       "    0.023324456065893173,\n",
       "    -0.009662849828600883,\n",
       "    -0.00870165042579174,\n",
       "    0.03987937048077583,\n",
       "    -0.01313027273863554,\n",
       "    0.06363864988088608,\n",
       "    0.01266563031822443,\n",
       "    -0.00911864172667265,\n",
       "    0.05104406177997589,\n",
       "    -0.009922846220433712,\n",
       "    -0.006362599320709705,\n",
       "    0.0057648406364023685,\n",
       "    -0.03484948351979256,\n",
       "    -0.0032184962183237076,\n",
       "    -0.05094584822654724,\n",
       "    0.04392722249031067,\n",
       "    -0.04660162702202797,\n",
       "    -0.03263597562909126,\n",
       "    0.00948328897356987,\n",
       "    0.02031722478568554,\n",
       "    -0.022979261353611946,\n",
       "    0.006743982899934053,\n",
       "    -0.005397664848715067,\n",
       "    0.08122143894433975,\n",
       "    -0.006449740380048752,\n",
       "    -0.0233546681702137,\n",
       "    -0.070151686668396,\n",
       "    -0.009988682344555855,\n",
       "    0.02183588035404682,\n",
       "    -0.00047578749945387244,\n",
       "    0.013671123422682285,\n",
       "    0.011637314222753048,\n",
       "    -0.017824925482273102,\n",
       "    0.01357120368629694,\n",
       "    0.05561745539307594,\n",
       "    0.08742599189281464,\n",
       "    0.008799877017736435,\n",
       "    0.006877847481518984,\n",
       "    -0.008604791015386581,\n",
       "    -0.03516962379217148,\n",
       "    -0.04831170290708542,\n",
       "    0.022092105820775032,\n",
       "    -0.04061242565512657,\n",
       "    0.034801434725522995,\n",
       "    -0.030106810852885246,\n",
       "    0.004412548150867224,\n",
       "    0.046932071447372437,\n",
       "    -0.02305869571864605,\n",
       "    0.08602577447891235,\n",
       "    -0.05137799680233002,\n",
       "    -0.057735055685043335,\n",
       "    -0.017031613737344742,\n",
       "    -0.039930518716573715,\n",
       "    0.027200479060411453,\n",
       "    -5.6179185606880455e-33,\n",
       "    0.060003962367773056,\n",
       "    -0.07018348574638367,\n",
       "    0.03426384553313255,\n",
       "    0.06646239012479782,\n",
       "    -0.006295596715062857,\n",
       "    -0.017198534682393074,\n",
       "    -0.025232603773474693,\n",
       "    0.04921244457364082,\n",
       "    -0.0398479588329792,\n",
       "    -0.009288524277508259,\n",
       "    -0.012060563080012798,\n",
       "    0.022754870355129242,\n",
       "    0.0038370632100850344,\n",
       "    0.0022318768315017223,\n",
       "    -0.013165656477212906,\n",
       "    -0.03507212921977043,\n",
       "    -0.018438003957271576,\n",
       "    -0.002450457541272044,\n",
       "    0.021565282717347145,\n",
       "    -0.08119048178195953,\n",
       "    -0.07423408329486847,\n",
       "    0.032853756099939346,\n",
       "    0.0038617178797721863,\n",
       "    -0.05869396775960922,\n",
       "    0.03303302451968193,\n",
       "    0.035638682544231415,\n",
       "    0.014838764443993568,\n",
       "    -0.03369583934545517,\n",
       "    0.006180229131132364,\n",
       "    0.004653407260775566,\n",
       "    0.03176305815577507,\n",
       "    0.0009315602947026491,\n",
       "    0.02639714814722538,\n",
       "    -0.025116661563515663,\n",
       "    -0.021693794056773186,\n",
       "    0.08230579644441605,\n",
       "    -0.020206209272146225,\n",
       "    0.008325675502419472,\n",
       "    0.024737032130360603,\n",
       "    0.054107893258333206,\n",
       "    0.020826589316129684,\n",
       "    -0.012079989537596703,\n",
       "    0.012211107648909092,\n",
       "    0.0037422452587634325,\n",
       "    -0.021670423448085785,\n",
       "    -0.00245551741681993,\n",
       "    0.06535849720239639,\n",
       "    -0.032669272273778915,\n",
       "    0.020419709384441376,\n",
       "    -0.019589316099882126,\n",
       "    0.014356718398630619,\n",
       "    0.025222048163414,\n",
       "    -0.05632797256112099,\n",
       "    0.02787345089018345,\n",
       "    0.030664199963212013,\n",
       "    0.0861092135310173,\n",
       "    -0.023670261725783348,\n",
       "    -0.010860497131943703,\n",
       "    0.039253298193216324,\n",
       "    0.021682728081941605,\n",
       "    -0.005390592385083437,\n",
       "    -0.0360676646232605,\n",
       "    0.04944359138607979,\n",
       "    0.04714436084032059,\n",
       "    0.052185606211423874,\n",
       "    -0.01125520933419466,\n",
       "    0.03295044228434563,\n",
       "    -0.07306060940027237,\n",
       "    0.007280468475073576,\n",
       "    0.0015147136291489005,\n",
       "    0.03143317624926567,\n",
       "    0.0414789617061615,\n",
       "    0.022633442655205727,\n",
       "    0.03986106440424919,\n",
       "    0.004179369192570448,\n",
       "    -0.010775432921946049,\n",
       "    -0.09657904505729675,\n",
       "    0.005734473932534456,\n",
       "    0.07747834920883179,\n",
       "    -0.047730252146720886,\n",
       "    -0.023753708228468895,\n",
       "    -0.013946753926575184,\n",
       "    0.0205468088388443,\n",
       "    -0.02594764344394207,\n",
       "    -0.04007091745734215,\n",
       "    -0.04849539324641228,\n",
       "    0.009682039730250835,\n",
       "    -0.007807636633515358,\n",
       "    0.006197373848408461,\n",
       "    -0.032278772443532944,\n",
       "    -0.03521755710244179,\n",
       "    0.0634220689535141,\n",
       "    0.06396190822124481,\n",
       "    -0.0007208309252746403,\n",
       "    0.0005205576308071613,\n",
       "    -0.023755040019750595,\n",
       "    0.0009311099420301616,\n",
       "    0.009270060807466507,\n",
       "    0.028142698109149933,\n",
       "    0.033171385526657104,\n",
       "    -0.038683757185935974,\n",
       "    -4.958247700415086e-06,\n",
       "    -0.02278917282819748,\n",
       "    -0.000536112580448389,\n",
       "    0.055870264768600464,\n",
       "    -0.015047461725771427,\n",
       "    -0.040450453758239746,\n",
       "    0.04462364688515663,\n",
       "    0.004165883641690016,\n",
       "    -0.006083915010094643,\n",
       "    0.037934593856334686,\n",
       "    0.036177776753902435,\n",
       "    -0.027824878692626953,\n",
       "    0.005931659135967493,\n",
       "    -0.04001924395561218,\n",
       "    0.015831658616662025,\n",
       "    -0.01831350475549698,\n",
       "    0.036606840789318085,\n",
       "    -0.05697938799858093,\n",
       "    -0.05217789486050606,\n",
       "    -0.01843121089041233,\n",
       "    -0.03513339161872864,\n",
       "    -0.01859796978533268,\n",
       "    -0.063231460750103,\n",
       "    -0.015719572082161903,\n",
       "    -0.031277816742658615,\n",
       "    0.009847285225987434,\n",
       "    -0.04689276963472366,\n",
       "    0.03387955576181412,\n",
       "    -0.005764287896454334,\n",
       "    -0.00893369410187006,\n",
       "    -0.005525819957256317,\n",
       "    2.6454799240127613e-07,\n",
       "    0.02674459107220173,\n",
       "    0.03304973244667053,\n",
       "    -0.0027186351362615824,\n",
       "    0.004944884683936834,\n",
       "    0.021683242172002792,\n",
       "    -0.009071430191397667,\n",
       "    -0.003795315744355321,\n",
       "    -0.03485168144106865,\n",
       "    -0.0055013177916407585,\n",
       "    -0.025743385776877403,\n",
       "    0.040535520762205124,\n",
       "    0.023685017600655556,\n",
       "    0.03869763761758804,\n",
       "    -0.004096765071153641,\n",
       "    -0.04646331071853638,\n",
       "    -0.12914304435253143,\n",
       "    -0.00530387694016099,\n",
       "    -0.027295267209410667,\n",
       "    -0.004948721267282963,\n",
       "    0.05135786533355713,\n",
       "    -0.005525260232388973,\n",
       "    0.03796916827559471,\n",
       "    0.014531390741467476,\n",
       "    -0.010143283754587173,\n",
       "    -0.009512348100543022,\n",
       "    0.031208237633109093,\n",
       "    -0.04764265567064285,\n",
       "    -0.00661342591047287,\n",
       "    0.053484126925468445,\n",
       "    0.01847987249493599,\n",
       "    0.030841808766126633,\n",
       "    -0.006576251238584518,\n",
       "    0.06080634891986847,\n",
       "    0.04468175396323204,\n",
       "    -0.007604672573506832,\n",
       "    -0.0403311662375927,\n",
       "    0.0024315412156283855,\n",
       "    0.02273777313530445,\n",
       "    0.009461461566388607,\n",
       "    -0.043872684240341187,\n",
       "    0.03613031655550003,\n",
       "    0.028147215023636818,\n",
       "    0.018233099952340126,\n",
       "    -0.041730064898729324,\n",
       "    0.06812526285648346,\n",
       "    0.0032304669730365276,\n",
       "    -0.04977729544043541,\n",
       "    -0.035548996180295944,\n",
       "    0.017497794702649117,\n",
       "    0.00135122612118721,\n",
       "    0.05635526031255722,\n",
       "    -0.02774309366941452,\n",
       "    0.04953973740339279,\n",
       "    0.016103480011224747,\n",
       "    0.002199676586315036,\n",
       "    -0.0394558310508728,\n",
       "    -0.017118239775300026,\n",
       "    0.02750077098608017,\n",
       "    -0.007899406366050243,\n",
       "    0.010119176469743252,\n",
       "    -0.007878738455474377,\n",
       "    -0.05360225588083267,\n",
       "    0.06121016666293144,\n",
       "    -0.004513206891715527,\n",
       "    0.04067198187112808,\n",
       "    0.033526480197906494,\n",
       "    8.93038886715658e-05,\n",
       "    2.304326167691738e-34,\n",
       "    0.002129961736500263,\n",
       "    0.026877233758568764,\n",
       "    -0.030736606568098068,\n",
       "    0.05065155029296875,\n",
       "    -0.014355966821312904,\n",
       "    -0.03415072336792946,\n",
       "    -0.059387873858213425,\n",
       "    -0.007071103435009718,\n",
       "    0.018682895228266716,\n",
       "    0.027728809043765068,\n",
       "    0.018173668533563614],\n",
       "   'chapter': 'CHAPTER 3',\n",
       "   'title': 'Technical Interview: Machine Learning Algorithms'},\n",
       "  '_explanation': {'value': 1.9621209,\n",
       "   'description': 'sum of:',\n",
       "   'details': [{'value': 0.0040404093,\n",
       "     'description': 'weight(chapter:chapter in 9) [PerFieldSimilarity], result of:',\n",
       "     'details': [{'value': 0.0040404093,\n",
       "       'description': 'score(freq=1.0), computed as boost * idf * tf from:',\n",
       "       'details': [{'value': 2.2, 'description': 'boost', 'details': []},\n",
       "        {'value': 0.0040404093,\n",
       "         'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:',\n",
       "         'details': [{'value': 123,\n",
       "           'description': 'n, number of documents containing term',\n",
       "           'details': []},\n",
       "          {'value': 123,\n",
       "           'description': 'N, total number of documents with field',\n",
       "           'details': []}]},\n",
       "        {'value': 0.45454544,\n",
       "         'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:',\n",
       "         'details': [{'value': 1.0,\n",
       "           'description': 'freq, occurrences of term within document',\n",
       "           'details': []},\n",
       "          {'value': 1.2,\n",
       "           'description': 'k1, term saturation parameter',\n",
       "           'details': []},\n",
       "          {'value': 0.75,\n",
       "           'description': 'b, length normalization parameter',\n",
       "           'details': []},\n",
       "          {'value': 2.0, 'description': 'dl, length of field', 'details': []},\n",
       "          {'value': 2.0,\n",
       "           'description': 'avgdl, average length of field',\n",
       "           'details': []}]}]}]},\n",
       "    {'value': 1.9580805,\n",
       "     'description': 'weight(chapter:3 in 9) [PerFieldSimilarity], result of:',\n",
       "     'details': [{'value': 1.9580805,\n",
       "       'description': 'score(freq=1.0), computed as boost * idf * tf from:',\n",
       "       'details': [{'value': 2.2, 'description': 'boost', 'details': []},\n",
       "        {'value': 1.9580806,\n",
       "         'description': 'idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:',\n",
       "         'details': [{'value': 17,\n",
       "           'description': 'n, number of documents containing term',\n",
       "           'details': []},\n",
       "          {'value': 123,\n",
       "           'description': 'N, total number of documents with field',\n",
       "           'details': []}]},\n",
       "        {'value': 0.45454544,\n",
       "         'description': 'tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:',\n",
       "         'details': [{'value': 1.0,\n",
       "           'description': 'freq, occurrences of term within document',\n",
       "           'details': []},\n",
       "          {'value': 1.2,\n",
       "           'description': 'k1, term saturation parameter',\n",
       "           'details': []},\n",
       "          {'value': 0.75,\n",
       "           'description': 'b, length normalization parameter',\n",
       "           'details': []},\n",
       "          {'value': 2.0, 'description': 'dl, length of field', 'details': []},\n",
       "          {'value': 2.0,\n",
       "           'description': 'avgdl, average length of field',\n",
       "           'details': []}]}]}]}]}}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = es_client.search(\n",
    "    index=index_name,\n",
    "    body={\n",
    "        \"size\": 5,\n",
    "        \"knn\": {\n",
    "            \"field\": \"text_vector\",\n",
    "            \"query_vector\": vector_search_term,\n",
    "            \"k\": 5,\n",
    "            \"num_candidates\": 10000\n",
    "        },\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": [\n",
    "                    {\n",
    "                        \"match\": {\"chapter\": \"CHAPTER 3\"}\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"explain\": True\n",
    "    }\n",
    ")\n",
    "\n",
    "response['hits']['hits']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
